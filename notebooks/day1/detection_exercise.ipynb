{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNLyRyMaRenV"
      },
      "source": [
        "# Image Detection with Deep Learning\n",
        "\n",
        "Welcome to the **BILD 2025 Summer School** hands-on session on object detection! This notebook will guide you through the complete pipeline of building a model to locate signs of pneumonia in chest X-rays.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/albarqounilab/BILD-Summer-School/blob/main/notebooks/day1/detection_exercise.ipynb)\n",
        "\n",
        "![BILD Banner](https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/helpers/notebook-banner.png)\n",
        "\n",
        "---\n",
        "\n",
        "### Today's Goals\n",
        "\n",
        "This session is a practical journey into the world of medical object detection. By the end of this notebook, you will be able to:\n",
        "\n",
        "1.  **Prepare Medical Imaging Data**: Load and process complex medical images in the DICOM format and parse their corresponding bounding box annotations.\n",
        "2.  **Understand Detection Data Pipelines**: Create a custom PyTorch `Dataset` and `DataLoader` specifically for object detection.\n",
        "3.  **Train a Detector Model**: Fine-tune a state-of-the-art **Faster R-CNN** model to draw precise boxes around infected lung areas.\n",
        "4.  **Master Detection Metrics**: Use and interpret key evaluation metrics like **Intersection over Union (IoU)** and **Average Precision (AP)**.\n",
        "5.  **Perform Advanced Quality Control**: Go beyond a single number to systematically analyze your model's failure modes.\n",
        "\n",
        "*   **Objectives**: You'll see how AI can be trained to identify and localize pathological findings, a crucial step in computer-assisted diagnosis. You'll also apply your object detection skills to a challenging real-world problem using medical imaging data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hQomaASgRenZ"
      },
      "source": [
        "## 1. Environment Setup\n",
        "\n",
        "First, we'll prepare our environment. This involves installing necessary packages for handling large files and medical images, cloning the dataset, and importing all the Python libraries we'll need.\n",
        "\n",
        "> **Note**: This first code block will install several packages and download a large dataset. This may take a few minutes and requires an internet connection."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1xI7L919gM_",
        "outputId": "5a282ec3-10c7-4407-a47c-951f2dba1567"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/2.4 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.1/2.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/2.4 MB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m26.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Library Installations\n",
        "!pip install pydicom albumentations -q\n",
        "\n",
        "# Core Library Import\n",
        "import os\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import sys, math, random, time, warnings\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Data Handling and Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "\n",
        "# Deep Learning with PyTorch & Torchvision\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "# Data Augmentation & Visualization\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Utilities\n",
        "from tqdm.notebook import tqdm\n",
        "# Download official torchvision helper scripts for training and evaluation\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n",
        "import utils\n",
        "\n",
        "# Environment Configuration\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Ensure reproducibility\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "seed_everything(RANDOM_SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Setup complete. Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yI-5t8cZ9gNA"
      },
      "source": [
        "## 2. The Dataset: RSNA Pneumonia Detection Challenge\n",
        "\n",
        "Our task is to detect signs of pneumonia in chest X-rays. We are using a dataset from the **RSNA Pneumonia Detection Challenge**, which contains nearly 30,000 chest X-ray images in the DICOM format. A subset of these images has been annotated by expert radiologists with bounding boxes indicating areas of lung opacity, which are a sign of pneumonia."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMLOJUGLXAfP"
      },
      "source": [
        "### Downloading the Data (2 minutes)\n",
        "> ~1.8GB; may take a few minutes. If you're have the local data, skip."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mQLjBtlxWqgj"
      },
      "outputs": [],
      "source": [
        "!pip -q install -U \"huggingface_hub[cli]\" -q\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Detection/rsna-pneumonia-detection-challenge.zip\" --local-dir ./\n",
        "!unzip -q ./Detection/rsna-pneumonia-detection-challenge.zip -d ./"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j3dKx-Jq9gNB"
      },
      "source": [
        "### 2.1. Data Exploration & Quality Control\n",
        "\n",
        "Before we build any model, we must first understand our data. If we train our model on noisy, inconsistent, or poorly understood data, we cannot trust its predictions.\n",
        "\n",
        "This process, known as **Exploratory Data Analysis (EDA)**, is our first quality control step. We will:\n",
        "1.  Analyze the class balance: How many patients have pneumonia versus how many do not?\n",
        "2.  Inspect the annotations: How many bounding boxes are there per image?\n",
        "3.  Visualize a sample: We will perform a \"sanity check\" to ensure our data is loading correctly and the labels align with the images.\n",
        "\n",
        "We'll start by loading the labels file, which contains the patient IDs and bounding box information for each positive case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aI76Nq099gNC"
      },
      "outputs": [],
      "source": [
        "# Define paths to our data files\n",
        "DATA_PATH = './rsna-pneumonia-detection-challenge'\n",
        "LABELS_PATH = os.path.join(DATA_PATH, 'labels.csv')\n",
        "IMAGE_DIR = os.path.join(DATA_PATH, 'images')\n",
        "\n",
        "# Load the labels into a pandas DataFrame\n",
        "df = pd.read_csv(LABELS_PATH)\n",
        "\n",
        "# Initial Analysis\n",
        "# Calculate the number of unique patients with and without pneumonia\n",
        "positive_cases = df[df['Target'] == 1]['patientId'].nunique()\n",
        "total_cases = df['patientId'].nunique()\n",
        "negative_cases = total_cases - positive_cases\n",
        "\n",
        "print(f\"--- Dataset Overview ---\")\n",
        "print(f\"Total unique patients: {total_cases}\")\n",
        "print(f\"Patients with pneumonia (positive cases): {positive_cases} ({positive_cases/total_cases:.1%})\")\n",
        "print(f\"Patients without pneumonia (negative cases): {negative_cases} ({negative_cases/total_cases:.1%})\")\n",
        "\n",
        "# Check how many pneumonia cases have multiple bounding boxes\n",
        "bbox_counts = df[df['Target'] == 1].groupby('patientId').size()\n",
        "print(f\"Pneumonia cases with >1 bounding box: {(bbox_counts > 1).sum()}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iX43gE1L9gND"
      },
      "source": [
        "> ### Deep Dive: The DICOM Format\n",
        ">\n",
        "> **DICOM** (Digital Imaging and Communications in Medicine) is the international standard for medical images and related information. It's more than just an image format like JPEG or PNG. A DICOM file is a complex container that holds not only the pixel data of the image but also a rich set of **metadata**.\n",
        ">\n",
        "> This metadata includes:\n",
        "> -   **Patient Information**: Name, ID, age, sex (often anonymized in public datasets).\n",
        "> -   **Study Information**: What kind of study was performed (e.g., Chest X-ray), the date, and referring physician.\n",
        "> -   **Image Acquisition Details**: The type of machine used (e.g., scanner model), exposure settings, pixel spacing (the physical size of a pixel), and image orientation.\n",
        ">\n",
        "> For our task, the most important part is the **pixel array**, which contains the actual image data. We use the `pydicom` library to easily read these files and extract this pixel data for our model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jF1alEFtRene"
      },
      "source": [
        "#### Sanity Check: Visualizing a Single Case\n",
        "\n",
        "Now for a crucial sanity check. We will load a single DICOM image and use the coordinates from our labels file to draw its corresponding bounding box.\n",
        "\n",
        "This simple step is incredibly important for debugging. It visually confirms that our entire data pipeline is working as expected:\n",
        "-   Are we reading the DICOM files correctly?\n",
        "-   Are we parsing the CSV file properly?\n",
        "-   Do the `(x, y, width, height)` coordinates from the file actually correspond to the pneumonia opacity visible in the image?\n",
        "\n",
        "Catching a mistake here can save hours of confusion later."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i9C3c6_B9gNE"
      },
      "outputs": [],
      "source": [
        "# Visualize a Sample\n",
        "# Select a sample row for a patient with pneumonia\n",
        "sample_patient = df[df.Target == 1].iloc[13]\n",
        "image_path = os.path.join(IMAGE_DIR, f\"{sample_patient.patientId}.dcm\")\n",
        "\n",
        "# Read the DICOM file\n",
        "dicom_data = pydicom.dcmread(image_path)\n",
        "image_array = dicom_data.pixel_array\n",
        "\n",
        "# Get the bounding box coordinates from the DataFrame\n",
        "box = sample_patient[['x', 'y', 'width', 'height']].values\n",
        "\n",
        "# Plotting\n",
        "fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "ax.imshow(image_array, cmap='gray')\n",
        "ax.set_title(f\"Patient ID: {sample_patient.patientId}\")\n",
        "\n",
        "# Create a Rectangle patch and add it to the plot\n",
        "rect = patches.Rectangle(\n",
        "    (box[0], box[1]), box[2], box[3],\n",
        "    linewidth=2, edgecolor='r', facecolor='none'\n",
        ")\n",
        "ax.add_patch(rect)\n",
        "ax.axis('off')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gW-iScwvRenf"
      },
      "source": [
        "#### Analyzing Bounding Box Characteristics\n",
        "Now that we've seen a single case, let's analyze the properties of *all* the bounding boxes in the dataset. Understanding the distribution of their sizes and shapes can reveal important characteristics of our data and potential challenges for our model.\n",
        "\n",
        "-   **Area Distribution:** Are most pneumonia findings large and obvious, or small and subtle? If the dataset is dominated by very small boxes, our model might struggle to detect them.\n",
        "-   **Aspect Ratio (Width / Height):** Are the findings generally square-shaped, or are they often very tall and thin, or short and wide? Extreme aspect ratios can sometimes be challenging for standard object detectors.\n",
        "\n",
        "Let's plot these distributions to find out."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bltSHgZqRenf"
      },
      "source": [
        "### Q1: What can we learn from the bounding box distributions?\n",
        "**Your Task**: Look at the two histograms below.\n",
        "1. The first shows the distribution of bounding box **areas**.\n",
        "2. The second shows the distribution of **aspect ratios** (width / height).\n",
        "\n",
        "What do these plots tell you about the pneumonia findings in this dataset? How might this information influence your model design or training strategy?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pIu5ur6wReng"
      },
      "outputs": [],
      "source": [
        "# Analyze Bounding Box Sizes and Aspect Ratios\n",
        "positive_df = df[df['Target'] == 1].copy()\n",
        "positive_df['area'] = positive_df['width'] * positive_df['height']\n",
        "positive_df['aspect_ratio'] = positive_df['width'] / positive_df['height']\n",
        "\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Plot Area Distribution\n",
        "axes[0].hist(positive_df['area'], bins=50, color='skyblue', edgecolor='black')\n",
        "axes[0].set_title('Distribution of Bounding Box Areas')\n",
        "axes[0].set_xlabel('Area (pixels^2)')\n",
        "axes[0].set_ylabel('Frequency')\n",
        "\n",
        "# Plot Aspect Ratio Distribution\n",
        "axes[1].hist(positive_df['aspect_ratio'], bins=50, color='salmon', edgecolor='black')\n",
        "axes[1].set_title('Distribution of Bounding Box Aspect Ratios (W/H)')\n",
        "axes[1].set_xlabel('Aspect Ratio')\n",
        "axes[1].set_ylabel('Frequency')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIKKjYnXSHfl"
      },
      "source": [
        "<details>\n",
        "<summary>Click for Solution & Discussion</summary>\n",
        "\n",
        "**Area Distribution**: The area histogram is heavily skewed to the left, with a long tail. This means that while there is a wide range of sizes, the vast majority of pneumonia patches are relatively **small**. This confirms our earlier suspicion that detecting small objects will be a key challenge for our model.\n",
        "\n",
        "**Aspect Ratio Distribution**: This histogram is centered around 1.0, indicating that most bounding boxes are roughly square-shaped. However, there is a noticeable tail to the right, showing that some findings are significantly wider than they are tall. There are very few tall and thin boxes.\n",
        "\n",
        "**Influence on Strategy**: This information is very useful. The prevalence of small objects reinforces the need for a model with a Feature Pyramid Network (like Faster R-CNN) that is good at multi-scale detection. The aspect ratio distribution could guide the design of anchor boxes in some models, though modern detectors are less sensitive to this. It also suggests that data augmentations that create more varied aspect ratios might be beneficial.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WMkMceHANhIO"
      },
      "source": [
        "\n",
        "\n",
        "> 10 minutes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2h4bXy5V9gNF"
      },
      "source": [
        "### 2.2. The `DetectionDataset` Class\n",
        "\n",
        "To feed our data into a PyTorch model, we need a custom `Dataset` class. This class acts as a blueprint, telling PyTorch how to access and process each individual sample."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e_z24mBf9gNG"
      },
      "source": [
        "> ### Deep Dive: The Target Dictionary for Object Detection\n",
        ">\n",
        "> Unlike classification (which needs a single label) or segmentation (which needs a mask), object detection models in PyTorch require a specific dictionary format for each image's ground truth. This `target` dictionary must contain several key pieces of information:\n",
        ">\n",
        "> -   `boxes`: A tensor of shape `[N, 4]`, where `N` is the number of objects in the image. Each of the 4 values represents a bounding box in **[x_min, y_min, x_max, y_max]** format (also known as PASCAL_VOC format).\n",
        "> -   `labels`: A tensor of shape `[N]` containing the integer class label for each box. In our case, `1` will represent \"pneumonia\".\n",
        "> -   `image_id`: A unique identifier for the image.\n",
        "> -   `area`: The area of each bounding box.\n",
        "> -   `iscrowd`: A boolean tensor indicating if any boxes represent a \"crowd\" of objects (we'll set this to `0` for all boxes).\n",
        ">\n",
        "> Our `__getitem__` method is carefully constructed to load an image and build this precise target dictionary for it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7xjdnGmRenh"
      },
      "source": [
        "### Q2 proposal: Complete the Dataset's `__getitem__` Method\n",
        "\n",
        "Why only positive cases?\n",
        "\n",
        "To feed our data into a PyTorch model, we need a custom `Dataset` class. This class acts as a blueprint, telling PyTorch how to access and process each individual sample. For this detection task, we will focus only on the images that contain pneumonia (`Target == 1`), as they are the ones with bounding boxes to learn from.\n",
        "\n",
        "> **TODO**: The `__getitem__` method is the core of our data pipeline. It's responsible for loading a single image and formatting its corresponding labels correctly.\n",
        ">\n",
        "> Fill in the `...` placeholders in the code below to complete the following steps:\n",
        "> 1.  **Get Bounding Boxes:** Extract the `x`, `y`, `width`, and `height` values for the current `image_id` from the dataframe.\n",
        "> 2.  **Convert Box Format:** The raw data is in `[x, y, width, height]` format. Convert it to the `[x_min, y_min, x_max, y_max]` format that PyTorch models expect.\n",
        "> 3.  **Construct Target Dictionary:** Create the `target` dictionary with all the required keys (`boxes`, `labels`, `image_id`, etc.), making sure they are the correct data types (PyTorch tensors)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "W5n4OMv_2PjH"
      },
      "outputs": [],
      "source": [
        "class PneumoniaDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transforms=None):\n",
        "        super().__init__()\n",
        "        self.df = dataframe[dataframe['Target'] == 1].copy()\n",
        "        self.image_ids = self.df['patientId'].unique()\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        # Image Loading and Preprocessing\n",
        "        image_id = self.image_ids[index]\n",
        "        image_path = os.path.join(self.image_dir, f\"{image_id}.dcm\")\n",
        "\n",
        "        dicom_data = pydicom.dcmread(image_path)\n",
        "        image = dicom_data.pixel_array\n",
        "\n",
        "        # Convert to a 3-channel image for compatibility with torchvision models\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # Q1: FILL IN THE ...\n",
        "\n",
        "        # Get all annotation records for the current image_id\n",
        "        records = ...\n",
        "        boxes = ...\n",
        "\n",
        "        # Convert boxes from [x, y, w, h] to [x_min, y_min, x_max, y_max]\n",
        "        boxes[:, 2] = ... # x_max\n",
        "        boxes[:, 3] = ... # y_max\n",
        "\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        # Create the target dictionary\n",
        "        target = {\n",
        "            'boxes': ..., # Keep as numpy array for albumentations initially\n",
        "            'labels': ..., # Label 1 for pneumonia\n",
        "            'image_id': ...,\n",
        "            'area': ...,\n",
        "            'iscrowd': ...,\n",
        "        }\n",
        "\n",
        "        # Data Augmentation\n",
        "        if self.transforms:\n",
        "            # Convert labels tensor to numpy array before passing to albumentations\n",
        "            labels_np = target['labels'].numpy()\n",
        "\n",
        "            # Pass numpy array for boxes and labels to albumentations\n",
        "            sample = self.transforms(image=image, bboxes=target['boxes'], labels=labels_np)\n",
        "            image = sample['image']\n",
        "\n",
        "            # Handle the case where augmentations remove all boxes\n",
        "            if len(sample['bboxes']) > 0:\n",
        "                target['boxes'] = torch.as_tensor(sample['bboxes'], dtype=torch.float32)\n",
        "                # Convert labels back to tensor\n",
        "                target['labels'] = torch.as_tensor(sample['labels'], dtype=torch.int64)\n",
        "                target['area'] = (target['boxes'][:, 3] - target['boxes'][:, 1]) * (target['boxes'][:, 2] - target['boxes'][:, 0])\n",
        "                target['iscrowd'] = torch.zeros(len(target['boxes']), dtype=torch.int64)\n",
        "            else:\n",
        "                # If all boxes are removed, return empty tensors\n",
        "                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
        "                target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
        "                target['area'] = torch.zeros((0,), dtype=torch.float32)\n",
        "                target['iscrowd'] = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "\n",
        "            # Ensure canvas_size is set for the BoundingBoxes tensor\n",
        "            if len(target['boxes']) > 0:\n",
        "                 target['boxes'] = tv_tensors.BoundingBoxes(target['boxes'], format=\"XYXY\", canvas_size=image.shape[-2:])\n",
        "            else:\n",
        "                 # Still need a BoundingBoxes tensor even if empty, with correct canvas_size\n",
        "                 target['boxes'] = tv_tensors.BoundingBoxes(torch.zeros((0, 4), dtype=torch.float32), format=\"XYXY\", canvas_size=image.shape[-2:])\n",
        "\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux2LyBvIReni"
      },
      "source": [
        "### 2.3. Data Augmentation\n",
        "\n",
        "Data augmentation is a critical technique for improving model performance, especially in medical imaging where datasets can be limited. By applying random transformations to our training images, we create new, plausible training examples. This helps the model become more robust and prevents it from overfitting.\n",
        "\n",
        "We will use the `albumentations` library, which is highly efficient and designed for tasks like object detection where the bounding boxes must be transformed along with the image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QGz_xK9N9gNI"
      },
      "outputs": [],
      "source": [
        "# Define the augmentation pipeline\n",
        "def get_transforms(is_train=True, target_size=256):\n",
        "    if is_train:\n",
        "        return A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            # Resize must be after geometric transforms that change coordinates\n",
        "            A.Resize(height=target_size, width=target_size, p=1.0),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(p=1.0)\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.1))\n",
        "    else: # For validation and testing, we only resize and normalize\n",
        "        return A.Compose([\n",
        "            A.Resize(height=target_size, width=target_size, p=1.0),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(p=1.0)\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "print(\"Augmentation pipelines defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbXwXpL19gNJ"
      },
      "source": [
        "### 2.4. Data Splitting and `DataLoader` Creation\n",
        "\n",
        "We'll now split our list of patient IDs into training, validation, and test sets. It is crucial to split by patient to avoid data leakage. We'll then create our `Dataset` and `DataLoader` instances for each split."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XoT_3NwjRenj"
      },
      "source": [
        "> ### Deep Dive: The `collate_fn`\n",
        ">\n",
        "> When creating a batch of data, the `DataLoader` usually stacks the individual tensors together. However, this doesn't work for object detection targets, because each image can have a *different number* of bounding boxes. You can't stack a tensor of 2 boxes with a tensor of 3 boxes.\n",
        ">\n",
        "> The `collate_fn` (collation function) is a special function we provide to the `DataLoader` to tell it how to handle this. Instead of trying to stack the target dictionaries, `utils.collate_fn` from torchvision simply gathers them into a list. The result of one batch is a list of image tensors and a list of target dictionaries."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYbXU86PRenj"
      },
      "source": [
        "### Q3: Why is a `collate_fn` necessary for this task?\n",
        "**Your Task**: In the `DataLoader` definitions below, we pass `collate_fn=utils.collate_fn`. Based on the Deep Dive above and your understanding of the data, explain in one or two sentences why this is necessary. What would happen if we removed it?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3BPKYYANnRM"
      },
      "source": [
        "\n",
        "\n",
        "> 25 minutes\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LU3zMm83DW3V"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TARGET_SIZE = 256  #@param {type: \"number\"}\n",
        "BATCH_SIZE = 2  #@param {type: \"number\"}\n",
        "\n",
        "# Patient-wise Data Splitting\n",
        "# We focus on the positive pneumonia cases for this detection task\n",
        "positive_patient_ids = df[df['Target'] == 1]['patientId'].unique()\n",
        "\n",
        "# Split positive cases: 70% train, 15% validation, 15% test\n",
        "train_ids, test_ids = train_test_split(positive_patient_ids, test_size=0.3, random_state=RANDOM_SEED)\n",
        "val_ids, test_ids = train_test_split(test_ids, test_size=0.5, random_state=RANDOM_SEED)\n",
        "\n",
        "# Create DataFrames for each split\n",
        "train_df = df[df['patientId'].isin(train_ids)]\n",
        "val_df = df[df['patientId'].isin(val_ids)]\n",
        "test_df = df[df['patientId'].isin(test_ids)]\n",
        "\n",
        "print(f\"--- Data Splits (Positive Cases Only) ---\")\n",
        "print(f\"Training samples: {len(train_ids)}\")\n",
        "print(f\"Validation samples: {len(val_ids)}\")\n",
        "print(f\"Test samples: {len(test_ids)}\")\n",
        "\n",
        "# Create Datasets\n",
        "train_dataset = PneumoniaDataset(train_df, IMAGE_DIR, transforms=get_transforms(is_train=True, target_size=TARGET_SIZE))\n",
        "val_dataset = PneumoniaDataset(val_df, IMAGE_DIR, transforms=get_transforms(is_train=False, target_size=TARGET_SIZE))\n",
        "test_dataset = PneumoniaDataset(test_df, IMAGE_DIR, transforms=get_transforms(is_train=False, target_size=TARGET_SIZE))\n",
        "\n",
        "# Create DataLoaders\n",
        "train_loader = DataLoader(\n",
        "    train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=...\n",
        ")\n",
        "val_loader = DataLoader(\n",
        "    val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=...\n",
        ")\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=...\n",
        ")\n",
        "\n",
        "print(\"\\nDatasets and DataLoaders created successfully.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOZv49GsSt7y"
      },
      "source": [
        "<details>\n",
        "<summary>Discussion</summary>\n",
        "\n",
        "The `collate_fn` is necessary because different images in our dataset have a different number of pneumonia bounding boxes. PyTorch's default collate function would fail because it wouldn't know how to stack target dictionaries with varying numbers of boxes into a single tensor.\n",
        "\n",
        "If we removed it, the `DataLoader` would raise an error as soon as it tried to create a batch containing images with different numbers of annotations.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QQMBl-15Renj"
      },
      "source": [
        "Let's visualize a batch from our `train_loader` to confirm that the augmentations are being applied correctly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F5f1fI609gNL"
      },
      "outputs": [],
      "source": [
        "# Visualization Helper\n",
        "def show_detection_batch(dataloader, n=4):\n",
        "    images, targets = next(iter(dataloader))\n",
        "    k = min(n, len(images))\n",
        "\n",
        "    for i in range(k):\n",
        "        # Un-normalize image for display\n",
        "        img = images[i].permute(1, 2, 0).numpy()\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img = std * img + mean\n",
        "        img = np.clip(img, 0, 1)\n",
        "\n",
        "        # Plotting\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(6, 6))\n",
        "        ax.imshow(img)\n",
        "        ax.set_title(f\"Sample {i+1}, Boxes: {len(targets[i]['boxes'])}\")\n",
        "\n",
        "        for box in targets[i]['boxes']:\n",
        "            x1, y1, x2, y2 = box.numpy()\n",
        "            rect = patches.Rectangle((x1, y1), x2 - x1, y2 - y1, linewidth=2, edgecolor='g', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "        ax.axis('off')\n",
        "        plt.show()\n",
        "\n",
        "print(\"--- Sample Augmented Training Batch ---\")\n",
        "show_detection_batch(train_loader, n=4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-GG9etGNxGd"
      },
      "source": [
        "\n",
        "\n",
        "> 25 minutes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eJkQf_L99gNM"
      },
      "source": [
        "## 3. The Model: Faster R-CNN\n",
        "\n",
        "For our detection task, we will use **Faster R-CNN (Region-based Convolutional Neural Network)**, a powerful and widely-used two-stage object detection model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oX94uY7B9gNN"
      },
      "source": [
        "> ### Deep Dive: How Faster R-CNN Works\n",
        ">\n",
        "> Faster R-CNN breaks down the complex task of object detection into two manageable stages:\n",
        ">\n",
        "> 1.  **Region Proposal Network (RPN)**: Instead of scanning every possible location in an image, the RPN efficiently proposes a set of rectangular regions that are likely to contain an object. It acts as a quick \"attention\" mechanism, telling the model where to look more closely.\n",
        ">\n",
        "> 2.  **Detection Network (Fast R-CNN Head)**: For each proposed region, this network performs two tasks:\n",
        ">     -   **Classification**: It determines the class of the object within the region (e.g., \"pneumonia\" or \"background\").\n",
        ">     -   **Regression**: It refines the coordinates of the bounding box to make it fit the object more tightly.\n",
        ">\n",
        "> We use a model from `torchvision` that has been **pre-trained** on the [COCO dataset](https://cocodataset.org/#home). This means the model's backbone (a ResNet-50) has already learned rich visual features. We only need to replace the final classification layer (the \"head\") with a new one tailored to our specific classes (background and pneumonia). This technique, called **transfer learning**, dramatically speeds up training and improves performance."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6I-5gVp-A9ei"
      },
      "source": [
        "### Q4 proposal: Build the Transfer Learning Model\n",
        "**TODO**: We need to adapt the pre-trained Faster R-CNN model for our specific task. This involves replacing the final classification layer (the \"box predictor\") with a new one that matches our number of classes (2: pneumonia + background).\n",
        "Fill in the `...` placeholders below to:\n",
        " 1. Get the number of input features for the model's box predictor.\n",
        " 2. Create a new `FastRCNNPredictor` with the correct number of `in_features` and `num_classes`.\n",
        " 3. Replace the model's original `box_predictor` with your new one."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xvl6F00SBToh"
      },
      "outputs": [],
      "source": [
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
        "\n",
        "def get_detection_model(num_classes=2):\n",
        "    \"\"\"\n",
        "    Creates a Faster R-CNN model with a pre-trained ResNet-50 backbone.\n",
        "    \"\"\"\n",
        "    # Load a model pre-trained on COCO\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
        "\n",
        "    # --- Q4: FILL IN THE ... ---\n",
        "\n",
        "    # 1. Get the number of input features for the classifier from the model's RoI heads\n",
        "    in_features = ...\n",
        "\n",
        "    # 2. & 3. Create a new FastRCNNPredictor and replace the model's box_predictor\n",
        "    model.roi_heads.box_predictor = ...\n",
        "\n",
        "    return model\n",
        "\n",
        "# Initialize the model and move it to the correct device\n",
        "model = get_detection_model(num_classes=2)\n",
        "model.to(device)\n",
        "\n",
        "print(\"Faster R-CNN model created and moved to device.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u4W7r0Ue9gNP"
      },
      "source": [
        "## 4. Training the Detector\n",
        "\n",
        "We are now ready to train our model. This section defines the training loop, sets up the optimizer and learning rate scheduler, and executes the training process."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR6LzG3M9gNQ"
      },
      "source": [
        "### 4.1. Training Setup\n",
        "\n",
        "With our model defined, we now need to configure the components that will drive the learning process. Think of this like preparing a car for a race: you need an engine, an accelerator, and a strategy for managing your speed.\n",
        "\n",
        "-   **The Optimizer (The Engine):** This is the core algorithm that updates the model's internal parameters (weights) to minimize the loss function. We will use **Stochastic Gradient Descent (SGD) with momentum**, a classic and powerful choice. It not only looks at the current error to decide which direction to go but also considers the direction it was recently moving in (momentum), which helps it navigate the complex loss landscape more smoothly.\n",
        "\n",
        "-   **The Learning Rate (The Accelerator Pedal):** This is a critical hyperparameter that controls how *large* of a step the optimizer takes when updating the model's weights. A learning rate that is too high can cause the model to overshoot the optimal solution, while one that is too low can make training incredibly slow or get stuck.\n",
        "\n",
        "-   **The Learning Rate Scheduler (The Gearbox/Cruise Control):** We rarely use a fixed learning rate throughout training. A **learning rate scheduler** dynamically adjusts the learning rate as training progresses. We will use a `StepLR`, which decreases the learning rate by a set factor after a certain number of epochs. This strategy allows the model to make large progress early on and then take smaller, more careful steps to fine-tune its performance as it gets closer to a good solution."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z6R7Kb-vRenk"
      },
      "source": [
        "### Q5: Configure the Optimizer and Scheduler\n",
        "\n",
        "**Your Task**: A crucial step in training is setting up the optimizer and learning rate scheduler. Fill in the `...` placeholders in the code below to:\n",
        "1.  Define the `params` to be optimized (hint: these are the model parameters that require gradients).\n",
        "2.  Create an `SGD` optimizer with the specified learning rate, momentum, and weight decay.\n",
        "3.  Create a `StepLR` scheduler that will decrease the learning rate by a factor of `gamma` every `step_size` epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ly3LErE8Renk"
      },
      "outputs": [],
      "source": [
        "# Training Hyperparameters\n",
        "NUM_EPOCHS = 5      # For a real run, 10-20 epochs would be better\n",
        "LEARNING_RATE = 0.005\n",
        "MOMENTUM = 0.9\n",
        "WEIGHT_DECAY = 0.0005\n",
        "\n",
        "# Q4: FILL IN THE ...\n",
        "# Gather the parameters that need to be trained\n",
        "params = ...\n",
        "optimizer = torch.optim.SGD(...)\n",
        "\n",
        "# A learning rate scheduler that decreases the LR by 10x every 3 epochs\n",
        "lr_scheduler = torch.optim.lr_scheduler.StepLR(...)\n",
        "\n",
        "print(\"Optimizer and Learning Rate Scheduler configured.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gARKbRPZRenl"
      },
      "source": [
        "### 4.2. The Training Loop\n",
        "\n",
        "The training loop iterates through our dataset for a specified number of epochs. In each epoch, it processes the entire training set, updates the model's weights, and then evaluates the model's performance on the validation set.\n",
        "\n",
        "For brevity and to use a standardized and optimized implementation, we will use the `train_one_epoch` and `evaluate` functions from the official `torchvision` reference scripts, which we downloaded earlier."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l_177lYg9gNQ"
      },
      "outputs": [],
      "source": [
        "from engine import train_one_epoch\n",
        "# Main Training Loop\n",
        "# This will take a significant amount of time to run.\n",
        "# On a standard Colab GPU, expect ~15 minutes per epoch.\n",
        "start_time = time.time()\n",
        "best_map = 0.0\n",
        "history = []\n",
        "\n",
        "print(\"--- Starting Training ---\")\n",
        "for epoch in range(NUM_EPOCHS):\n",
        "    print(f\"\\n--- Epoch {epoch+1}/{NUM_EPOCHS} ---\")\n",
        "\n",
        "    # Train for one epoch, printing every 100 iterations\n",
        "    metric_logger = train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=100)\n",
        "\n",
        "    # Update the learning rate\n",
        "    lr_scheduler.step()\n",
        "\n",
        "    torch.save(model.state_dict(), './Detection/detection_model.pth')\n",
        "\n",
        "end_time = time.time()\n",
        "total_training_time = (end_time - start_time) / 60\n",
        "print(f\"\\n--- Training Finished in {total_training_time:.2f} minutes ---\")\n",
        "print(f\"Best Validation mAP: {best_map:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qh42Q50wEZEW"
      },
      "source": [
        "\n",
        "\n",
        "```\n",
        "1 hour training in T4\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qf5YL_p8Renm"
      },
      "source": [
        "## 5. Evaluation\n",
        "\n",
        "We have trained our model and used the validation set to monitor its progress and save the best version. Now comes the moment of truth: the **final exam**.\n",
        "\n",
        "Up to this point, the **test set** has been kept completely locked away. The model has never seen a single image from it. This is a critical principle of good machine learning practice. The performance on this held-out set provides the most honest and unbiased measure of how our model will perform on new, unseen patients in a real-world scenario. It is the final verdict on our model's capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RwwQVIvRenm"
      },
      "source": [
        "### (Optional) Download Pre-Trained Model\n",
        "\n",
        "Training can take a long time. If you are short on time or running into issues, you can skip the training step and download a pre-trained version of the model weights by running the cell below. You can then proceed directly to the evaluation sections."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J7M77hvpRenm"
      },
      "outputs": [],
      "source": [
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Detection/detection_model.pth\" --local-dir ./\n",
        "print(\"Pre-trained model downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1V56bN_9gNR"
      },
      "source": [
        "### 5.1. Analyzing Performance Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vM0X3f-p9gNS"
      },
      "source": [
        "> ### Deep Dive: Object Detection Evaluation Metrics\n",
        ">\n",
        "> Evaluating an object detector is more complex than simple classification. The primary metrics are based on the concept of **Intersection over Union (IoU)**.\n",
        ">\n",
        "> -   **Intersection over Union (IoU)**: This measures the overlap between a predicted bounding box and a ground truth bounding box. It's calculated as `(Area of Overlap) / (Area of Union)`. An IoU of 1 means a perfect match, while an IoU of 0 means no overlap.\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/iou.png\" width=\"60%\">\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oXdDNl18TugP"
      },
      "outputs": [],
      "source": [
        "def compute_iou(box1, box2):\n",
        "    xA = max(box1[0], box2[0])\n",
        "    yA = max(box1[1], box2[1])\n",
        "    xB = min(box1[2], box2[2])\n",
        "    yB = min(box1[3], box2[3])\n",
        "\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    box1Area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2Area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    iou = interArea / float(box1Area + box2Area - interArea)\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nw-uMeqbTv5T"
      },
      "source": [
        ">\n",
        "> We use IoU to classify each prediction as a:\n",
        "> -   **True Positive (TP)**: The model correctly detects an object (IoU > threshold, e.g., 0.5).\n",
        "> -   **False Positive (FP)**: The model predicts an object where there is none, or the IoU is below the threshold.\n",
        "> -   **False Negative (FN)**: The model fails to detect a ground truth object.\n",
        ">\n",
        "> From these, we calculate:\n",
        "> -   **Precision**: `TP / (TP + FP)` - Of all the predictions made, how many were correct?\n",
        "> -   **Recall**: `TP / (TP + FN)` - Of all the actual objects, how many did the model find?\n",
        ">\n",
        "> -   **Average Precision (AP)**: The ultimate single-number metric. It is the area under the Precision-Recall curve, calculated across different confidence score thresholds. **mAP (mean Average Precision)** is simply the AP averaged over all classes (in our case, we only have one class, so AP and mAP are the same). The `evaluate` function we used automatically calculates mAP at different IoU thresholds, which is the standard for object detection challenges.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/avg_precision.png\" width=\"60%\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VDORYe8gReno"
      },
      "source": [
        "### Q6: Run Evaluation and Visualize Predictions\n",
        "**Your Task**: Now, load the best model you saved during training (or the pre-trained one you downloaded) and run the `evaluate` function on the `test_loader`.\n",
        "\n",
        "After the evaluation, run the visualization code. For each image, the **green boxes are the ground truth** and the **red boxes are the model's predictions**. Analyze the output:\n",
        "-   What is the final mAP on the test set?\n",
        "-   Can you find examples of **True Positives**, **False Negatives**, and **False Positives**?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u6FKdO34rVF-"
      },
      "outputs": [],
      "source": [
        "def evaluate(model, dataloader, device, iou_threshold=0.4, conf_threshold=0.7):\n",
        "    model.eval()\n",
        "    all_precisions = []\n",
        "    all_recalls = []\n",
        "    all_ious = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            for pred, target in zip(outputs, targets):\n",
        "                gt_boxes = target['boxes'].cpu().numpy()\n",
        "                pred_boxes = pred['boxes'].cpu().numpy()\n",
        "                pred_scores = pred['scores'].cpu().numpy()\n",
        "\n",
        "                # Filter by confidence\n",
        "                pred_boxes = pred_boxes[pred_scores > conf_threshold]\n",
        "\n",
        "                # Compute IoUs\n",
        "                matched = set()\n",
        "                ious = []\n",
        "                for gt_box in gt_boxes:\n",
        "                    best_iou = 0\n",
        "                    for i, pred_box in enumerate(pred_boxes):\n",
        "                        if i in matched:\n",
        "                            continue\n",
        "                        iou = compute_iou(gt_box, pred_box)\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_idx = i\n",
        "                    if best_iou >= iou_threshold:\n",
        "                        matched.add(best_idx)\n",
        "                        ious.append(best_iou)\n",
        "\n",
        "                TP = len(matched)\n",
        "                FP = len(pred_boxes) - TP\n",
        "                FN = len(gt_boxes) - TP\n",
        "\n",
        "                precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "                recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "                all_precisions.append(precision)\n",
        "                all_recalls.append(recall)\n",
        "                all_ious.extend(ious)\n",
        "\n",
        "    return {\n",
        "        \"precision\": np.mean(all_precisions),\n",
        "        \"recall\": np.mean(all_recalls),\n",
        "        \"mean_iou\": np.mean(all_ious) if all_ious else 0\n",
        "    }\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    xA = max(box1[0], box2[0])\n",
        "    yA = max(box1[1], box2[1])\n",
        "    xB = min(box1[2], box2[2])\n",
        "    yB = min(box1[3], box2[3])\n",
        "\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    box1Area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2Area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    iou = interArea / float(box1Area + box2Area - interArea)\n",
        "    return iou"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXDLpvsfoBUv"
      },
      "outputs": [],
      "source": [
        "# Load the best model\n",
        "# Create a new model instance and load the saved weights\n",
        "# best_model= model\n",
        "best_model = get_detection_model(num_classes=2)\n",
        "best_model.load_state_dict(torch.load('./Detection/detection_model.pth'))\n",
        "best_model.to(device)\n",
        "\n",
        "print(\"--- Evaluating on Test Set ---\")\n",
        "# Run the evaluation\n",
        "test_evaluator = evaluate(best_model, test_loader, device=device)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1s09QuLUBkt"
      },
      "source": [
        "### Q7: Improved visualization\n",
        "\n",
        "**Your Task**: Now we will add a text tag to our ground truth and predictions `via axes[i].text()`. Fill the color, font size and alpha values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hjYGW6ZRnm3f"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "\n",
        "def visualize_predictions_with_labels(model, dataloader, num_samples=8, score_threshold=0.5):\n",
        "    model.eval()\n",
        "\n",
        "    images, targets = next(iter(dataloader))\n",
        "    images = [img.to(device) for img in images]\n",
        "\n",
        "    with torch.no_grad():\n",
        "        predictions = model(images)\n",
        "\n",
        "    n = min(num_samples, len(images))\n",
        "    print(f\"\\n--- Visualizing {n} Test Predictions with Labels ---\")\n",
        "    print(\"Green = Ground Truth | Red = Prediction (score > 0.5)\")\n",
        "\n",
        "    # auto grid: ceil(sqrt(n)) × ceil(n / sqrt(n))\n",
        "    rows = math.ceil(math.sqrt(n))\n",
        "    cols = math.ceil(n / rows)\n",
        "\n",
        "    fig, axes = plt.subplots(rows, cols, figsize=(5*cols, 5*rows))\n",
        "    axes = np.array(axes).reshape(-1)  # flatten\n",
        "\n",
        "    for i in range(n):\n",
        "        img_cpu = images[i].cpu().permute(1, 2, 0).numpy()\n",
        "        mean = np.array([0.485, 0.456, 0.406])\n",
        "        std = np.array([0.229, 0.224, 0.225])\n",
        "        img_cpu = std * img_cpu + mean\n",
        "        img_cpu = np.clip(img_cpu, 0, 1)\n",
        "\n",
        "        axes[i].imshow(img_cpu)\n",
        "        axes[i].axis('off')\n",
        "\n",
        "        # GT boxes (green)\n",
        "        for box in targets[i]['boxes']:\n",
        "            x1, y1, x2, y2 = box.cpu().numpy()\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='g', facecolor='none')\n",
        "            axes[i].add_patch(rect)\n",
        "            # Add text label for ground truth\n",
        "            axes[i].text(x1, y1 - 5, \"Ground Truth\", color='white', fontsize=12, bbox=dict(facecolor='green', alpha=0.8, edgecolor='none'))\n",
        "\n",
        "        # Predictions (red)\n",
        "        pred_boxes = predictions[i]['boxes'][predictions[i]['scores'] > score_threshold]\n",
        "        pred_scores = predictions[i]['scores'][predictions[i]['scores'] > score_threshold]\n",
        "        pred_labels = predictions[i]['labels'][predictions[i]['scores'] > score_threshold]\n",
        "\n",
        "        for box, score, label in zip(pred_boxes, pred_scores, pred_labels):\n",
        "            x1, y1, x2, y2 = box.cpu().numpy()\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            axes[i].add_patch(rect)\n",
        "\n",
        "            # Q7 FILL OUT\n",
        "            # Add text label for prediction with score\n",
        "            label_text = ... # Assuming label 1 is Pneumonia\n",
        "            axes[i].text(x1, y1 - 10, f\"{label_text}: {score:.2f}\", color=..., fontsize=..., bbox=dict(facecolor=..., alpha=..., edgecolor='none'))\n",
        "\n",
        "\n",
        "    # hide unused axes\n",
        "    for j in range(n, len(axes)):\n",
        "        axes[j].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "visualize_predictions_with_labels(best_model, test_loader, num_samples=5, score_threshold=0.5)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "13eYlvc3ONDN"
      },
      "source": [
        "\n",
        "\n",
        "> 55 minutes\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a27zgu4KvJnD"
      },
      "source": [
        "## Part II: Quality Control"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D-3TBhsMmlSH"
      },
      "source": [
        "Reimport libraries (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v6lpKpdShCji",
        "outputId": "58023daf-cd0d-47a4-af8f-e112285f6e17"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#@title re-import libraries\n",
        "# Library Installations\n",
        "!pip install pydicom albumentations -q\n",
        "\n",
        "# Core Library Import\n",
        "import os\n",
        "# os.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
        "import sys, math, random, time, warnings\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "\n",
        "# Data Handling and Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "\n",
        "# Deep Learning with PyTorch & Torchvision\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torchvision\n",
        "from torchvision import tv_tensors\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.utils import draw_bounding_boxes\n",
        "\n",
        "# Data Augmentation & Visualization\n",
        "import albumentations as A\n",
        "from albumentations.pytorch import ToTensorV2\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# Utilities\n",
        "from tqdm.notebook import tqdm\n",
        "# Download official torchvision helper scripts for training and evaluation\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/engine.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_utils.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/coco_eval.py\")\n",
        "os.system(\"wget https://raw.githubusercontent.com/pytorch/vision/main/references/detection/transforms.py\")\n",
        "import utils\n",
        "\n",
        "# Environment Configuration\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Ensure reproducibility\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "seed_everything(RANDOM_SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Setup complete. Using device: {device}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Zq0U-o5hJrj",
        "outputId": "7a086700-24de-45c0-d2a7-32b09d486408"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Dataset Overview ---\n",
            "Total unique patients: 26684\n",
            "Patients with pneumonia (positive cases): 6012 (22.5%)\n",
            "Patients without pneumonia (negative cases): 20672 (77.5%)\n",
            "Pneumonia cases with >1 bounding box: 3398\n",
            "Fetching 1 files: 100% 1/1 [00:00<00:00, 14027.77it/s]\n",
            "/content\n",
            "Pre-trained model downloaded.\n",
            "Augmentation pipelines defined.\n"
          ]
        }
      ],
      "source": [
        "#@title re-load models\n",
        "# Define paths to our data files\n",
        "DATA_PATH = './rsna-pneumonia-detection-challenge'\n",
        "LABELS_PATH = os.path.join(DATA_PATH, 'labels.csv')\n",
        "IMAGE_DIR = os.path.join(DATA_PATH, 'images')\n",
        "\n",
        "# Load the labels into a pandas DataFrame\n",
        "df = pd.read_csv(LABELS_PATH)\n",
        "\n",
        "# Initial Analysis\n",
        "# Calculate the number of unique patients with and without pneumonia\n",
        "positive_cases = df[df['Target'] == 1]['patientId'].nunique()\n",
        "total_cases = df['patientId'].nunique()\n",
        "negative_cases = total_cases - positive_cases\n",
        "\n",
        "print(f\"--- Dataset Overview ---\")\n",
        "print(f\"Total unique patients: {total_cases}\")\n",
        "print(f\"Patients with pneumonia (positive cases): {positive_cases} ({positive_cases/total_cases:.1%})\")\n",
        "print(f\"Patients without pneumonia (negative cases): {negative_cases} ({negative_cases/total_cases:.1%})\")\n",
        "\n",
        "# Check how many pneumonia cases have multiple bounding boxes\n",
        "bbox_counts = df[df['Target'] == 1].groupby('patientId').size()\n",
        "print(f\"Pneumonia cases with >1 bounding box: {(bbox_counts > 1).sum()}\")\n",
        "\n",
        "class PneumoniaDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transforms=None):\n",
        "        super().__init__()\n",
        "        # Q1 Focus: We only work with positive cases for this detection task\n",
        "        self.df = dataframe[dataframe['Target'] == 1].copy()\n",
        "        self.image_ids = self.df['patientId'].unique()\n",
        "        self.image_dir = image_dir\n",
        "        self.transforms = transforms\n",
        "\n",
        "    def __getitem__(self, index: int):\n",
        "        # Image Loading and Preprocessing\n",
        "        image_id = self.image_ids[index]\n",
        "        image_path = os.path.join(self.image_dir, f\"{image_id}.dcm\")\n",
        "\n",
        "        dicom_data = pydicom.dcmread(image_path)\n",
        "        image = dicom_data.pixel_array\n",
        "\n",
        "        # Convert to a 3-channel image for compatibility with torchvision models\n",
        "        image = cv2.cvtColor(image, cv2.COLOR_GRAY2RGB)\n",
        "\n",
        "        # Target Preparation\n",
        "        records = self.df[self.df['patientId'] == image_id]\n",
        "        boxes = records[['x', 'y', 'width', 'height']].values\n",
        "\n",
        "        # Convert boxes from [x, y, w, h] to [x_min, y_min, x_max, y_max]\n",
        "        boxes[:, 2] = boxes[:, 0] + boxes[:, 2] # x_max\n",
        "        boxes[:, 3] = boxes[:, 1] + boxes[:, 3] # y_max\n",
        "\n",
        "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
        "\n",
        "        # Create the target dictionary\n",
        "        target = {\n",
        "            'boxes': boxes, # Keep as numpy array for albumentations initially\n",
        "            'labels': torch.ones(len(boxes), dtype=torch.int64), # Label 1 for pneumonia\n",
        "            'image_id': torch.tensor([index]),\n",
        "            'area': torch.as_tensor(area, dtype=torch.float32),\n",
        "            'iscrowd': torch.zeros(len(boxes), dtype=torch.int64)\n",
        "        }\n",
        "\n",
        "        # Data Augmentation\n",
        "        if self.transforms:\n",
        "            # Convert labels tensor to numpy array before passing to albumentations\n",
        "            labels_np = target['labels'].numpy()\n",
        "\n",
        "            # Pass numpy array for boxes and labels to albumentations\n",
        "            sample = self.transforms(image=image, bboxes=target['boxes'], labels=labels_np)\n",
        "            image = sample['image']\n",
        "\n",
        "            # Handle the case where augmentations remove all boxes\n",
        "            if len(sample['bboxes']) > 0:\n",
        "                target['boxes'] = torch.as_tensor(sample['bboxes'], dtype=torch.float32)\n",
        "                # Convert labels back to tensor\n",
        "                target['labels'] = torch.as_tensor(sample['labels'], dtype=torch.int64)\n",
        "                target['area'] = (target['boxes'][:, 3] - target['boxes'][:, 1]) * (target['boxes'][:, 2] - target['boxes'][:, 0])\n",
        "                target['iscrowd'] = torch.zeros(len(target['boxes']), dtype=torch.int64)\n",
        "            else:\n",
        "                # If all boxes are removed, return empty tensors\n",
        "                target['boxes'] = torch.zeros((0, 4), dtype=torch.float32)\n",
        "                target['labels'] = torch.zeros((0,), dtype=torch.int64)\n",
        "                target['area'] = torch.zeros((0,), dtype=torch.float32)\n",
        "                target['iscrowd'] = torch.zeros((0,), dtype=torch.int64)\n",
        "\n",
        "\n",
        "            # Ensure canvas_size is set for the BoundingBoxes tensor\n",
        "            if len(target['boxes']) > 0:\n",
        "                 target['boxes'] = tv_tensors.BoundingBoxes(target['boxes'], format=\"XYXY\", canvas_size=image.shape[-2:])\n",
        "            else:\n",
        "                 # Still need a BoundingBoxes tensor even if empty, with correct canvas_size\n",
        "                 target['boxes'] = tv_tensors.BoundingBoxes(torch.zeros((0, 4), dtype=torch.float32), format=\"XYXY\", canvas_size=image.shape[-2:])\n",
        "\n",
        "\n",
        "        return image, target\n",
        "\n",
        "    def __len__(self) -> int:\n",
        "        return len(self.image_ids)\n",
        "\n",
        "def compute_iou(box1, box2):\n",
        "    xA = max(box1[0], box2[0])\n",
        "    yA = max(box1[1], box2[1])\n",
        "    xB = min(box1[2], box2[2])\n",
        "    yB = min(box1[3], box2[3])\n",
        "\n",
        "    interArea = max(0, xB - xA) * max(0, yB - yA)\n",
        "    box1Area = (box1[2] - box1[0]) * (box1[3] - box1[1])\n",
        "    box2Area = (box2[2] - box2[0]) * (box2[3] - box2[1])\n",
        "\n",
        "    iou = interArea / float(box1Area + box2Area - interArea)\n",
        "    return iou\n",
        "\n",
        "def evaluate(model, dataloader, device, iou_threshold=0.4, conf_threshold=0.7):\n",
        "    model.eval()\n",
        "    all_precisions = []\n",
        "    all_recalls = []\n",
        "    all_ious = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, targets in dataloader:\n",
        "            images = [img.to(device) for img in images]\n",
        "            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
        "\n",
        "            outputs = model(images)\n",
        "\n",
        "            for pred, target in zip(outputs, targets):\n",
        "                gt_boxes = target['boxes'].cpu().numpy()\n",
        "                pred_boxes = pred['boxes'].cpu().numpy()\n",
        "                pred_scores = pred['scores'].cpu().numpy()\n",
        "\n",
        "                # Filter by confidence\n",
        "                pred_boxes = pred_boxes[pred_scores > conf_threshold]\n",
        "\n",
        "                # Compute IoUs\n",
        "                matched = set()\n",
        "                ious = []\n",
        "                for gt_box in gt_boxes:\n",
        "                    best_iou = 0\n",
        "                    for i, pred_box in enumerate(pred_boxes):\n",
        "                        if i in matched:\n",
        "                            continue\n",
        "                        iou = compute_iou(gt_box, pred_box)\n",
        "                        if iou > best_iou:\n",
        "                            best_iou = iou\n",
        "                            best_idx = i\n",
        "                    if best_iou >= iou_threshold:\n",
        "                        matched.add(best_idx)\n",
        "                        ious.append(best_iou)\n",
        "\n",
        "                TP = len(matched)\n",
        "                FP = len(pred_boxes) - TP\n",
        "                FN = len(gt_boxes) - TP\n",
        "\n",
        "                precision = TP / (TP + FP) if (TP + FP) > 0 else 0\n",
        "                recall = TP / (TP + FN) if (TP + FN) > 0 else 0\n",
        "\n",
        "                all_precisions.append(precision)\n",
        "                all_recalls.append(recall)\n",
        "                all_ious.extend(ious)\n",
        "\n",
        "    return {\n",
        "        \"precision\": np.mean(all_precisions),\n",
        "        \"recall\": np.mean(all_recalls),\n",
        "        \"mean_iou\": np.mean(all_ious) if all_ious else 0\n",
        "    }\n",
        "\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Detection/detection_model.pth\" --local-dir ./\n",
        "print(\"Pre-trained model downloaded.\")\n",
        "\n",
        "from torchvision.models.detection import fasterrcnn_resnet50_fpn_v2\n",
        "\n",
        "def get_detection_model(num_classes=2):\n",
        "    \"\"\"\n",
        "    Creates a Faster R-CNN model with a pre-trained ResNet-50 backbone.\n",
        "    Args:\n",
        "        num_classes (int): The number of classes, including the background.\n",
        "                           For pneumonia vs. not-pneumonia, this is 2.\n",
        "    \"\"\"\n",
        "    # Load a model pre-trained on COCO\n",
        "    model = fasterrcnn_resnet50_fpn_v2(weights='DEFAULT')\n",
        "\n",
        "    # Get the number of input features for the classifier\n",
        "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
        "\n",
        "    # Replace the pre-trained head with a new one\n",
        "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Load the best model\n",
        "# Create a new model instance and load the saved weights\n",
        "# best_model= model\n",
        "best_model = get_detection_model(num_classes=2)\n",
        "best_model.load_state_dict(torch.load('./Detection/detection_model.pth'))\n",
        "best_model.to(device)\n",
        "\n",
        "\n",
        "def get_transforms(is_train=True, target_size=256):\n",
        "    if is_train:\n",
        "        return A.Compose([\n",
        "            A.HorizontalFlip(p=0.5),\n",
        "            A.ShiftScaleRotate(shift_limit=0.05, scale_limit=0.1, rotate_limit=10, p=0.5),\n",
        "            A.RandomBrightnessContrast(p=0.5),\n",
        "            # Resize must be after geometric transforms that change coordinates\n",
        "            A.Resize(height=target_size, width=target_size, p=1.0),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(p=1.0)\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels'], min_area=1, min_visibility=0.1))\n",
        "    else: # For validation and testing, we only resize and normalize\n",
        "        return A.Compose([\n",
        "            A.Resize(height=target_size, width=target_size, p=1.0),\n",
        "            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "            ToTensorV2(p=1.0)\n",
        "        ], bbox_params=A.BboxParams(format='pascal_voc', label_fields=['labels']))\n",
        "\n",
        "print(\"Augmentation pipelines defined.\")\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "TARGET_SIZE = 256  #@param {type: \"number\"}\n",
        "BATCH_SIZE = 2  #@param {type: \"number\"}\n",
        "\n",
        "# Patient-wise Data Splitting\n",
        "# We focus on the positive pneumonia cases for this detection task\n",
        "positive_patient_ids = df[df['Target'] == 1]['patientId'].unique()\n",
        "\n",
        "# Split positive cases: 70% train, 15% validation, 15% test\n",
        "train_ids, test_ids = train_test_split(positive_patient_ids, test_size=0.3, random_state=RANDOM_SEED)\n",
        "val_ids, test_ids = train_test_split(test_ids, test_size=0.5, random_state=RANDOM_SEED)\n",
        "\n",
        "# Create DataFrames for each split\n",
        "train_df = df[df['patientId'].isin(train_ids)]\n",
        "val_df = df[df['patientId'].isin(val_ids)]\n",
        "test_df = df[df['patientId'].isin(test_ids)]\n",
        "test_dataset = PneumoniaDataset(test_df, IMAGE_DIR, transforms=get_transforms(is_train=False, target_size=TARGET_SIZE))\n",
        "test_loader = DataLoader(\n",
        "    test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=utils.collate_fn\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yKmItM-HRenv"
      },
      "source": [
        "## 6. Advanced Quality Control\n",
        "\n",
        "A single mAP score is a useful summary, but to truly trust and improve our model, we need to dig deeper. This section introduces advanced quality control techniques to systematically analyze the model's behavior and identify specific failure modes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z_v5SmX8Renv"
      },
      "source": [
        "### 6.1. Performance vs. Bounding Box Size\n",
        "\n",
        "A common challenge for object detectors is accurately identifying objects across a wide range of sizes. In our case, pneumonia opacities can vary from small, subtle patches to large, consolidated areas. A robust model should perform well on all of them.\n",
        "\n",
        "In this analysis, we will investigate whether our model has a **\"size bias.\"** We'll categorize the ground truth bounding boxes in our test set into \"small,\" \"medium,\" and \"large\" groups. Then, for each group, we'll calculate the model's **Recall**.\n",
        "\n",
        "**Recall** answers the critical question: **\"Of all the actual pneumonia cases in this size category, what fraction did our model successfully find?\"** A low recall for small boxes, for example, would indicate a clinically significant weakness."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vuaqgsESRenv"
      },
      "source": [
        "### Q8: Does the model have a size bias?\n",
        "**Your Task**: Run the code below, which calculates and plots the model's **recall** for small, medium, and large pneumonia patches. Based on the bar chart, does our model perform equally well on all sizes, or does it have a bias? What are the clinical implications of this?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iRZ894NORenw"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import box_iou\n",
        "import collections\n",
        "\n",
        "# Helper function to get all predictions and targets\n",
        "@torch.no_grad()\n",
        "def get_all_preds(model, dataloader, device):\n",
        "    model.eval()\n",
        "    all_preds, all_targets = [], []\n",
        "    for images, targets in tqdm(dataloader, desc=\"Gathering Test Predictions\"):\n",
        "        images = [img.to(device) for img in images]\n",
        "        preds = model(images)\n",
        "        all_preds.extend([{k: v.cpu() for k, v in p.items()} for p in preds])\n",
        "        all_targets.extend(targets)\n",
        "    return all_preds, all_targets\n",
        "\n",
        "all_predictions, all_targets = get_all_preds(best_model, test_loader, device)\n",
        "\n",
        "# Categorize boxes by size\n",
        "size_bins = {'small': [], 'medium': [], 'large': []}\n",
        "IOU_THRESHOLD = 0.5\n",
        "\n",
        "for preds, targets in zip(all_predictions, all_targets):\n",
        "    gt_boxes = targets['boxes']\n",
        "    pred_boxes = preds['boxes'][preds['scores'] > 0.5]\n",
        "\n",
        "    if len(gt_boxes) == 0: continue\n",
        "\n",
        "    # Calculate IoU between all predictions and ground truths for this image\n",
        "    ious = box_iou(gt_boxes, pred_boxes) if len(pred_boxes) > 0 else torch.zeros((len(gt_boxes), 0))\n",
        "\n",
        "    for i, gt_box in enumerate(gt_boxes):\n",
        "        area = (gt_box[2] - gt_box[0]) * (gt_box[3] - gt_box[1])\n",
        "\n",
        "        # Categorize size (thresholds are relative to 256x256 image)\n",
        "        if area < 64**2: category = 'small'\n",
        "        elif area < 96**2: category = 'medium'\n",
        "        else: category = 'large'\n",
        "\n",
        "        # Check if the box was detected (max IoU > threshold)\n",
        "        detected = (ious[i].max() > IOU_THRESHOLD).item() if len(pred_boxes) > 0 else False\n",
        "        size_bins[category].append(detected)\n",
        "\n",
        "# Calculate and plot recall for each size category\n",
        "recall_by_size = {k: np.mean(v) if v else 0 for k, v in size_bins.items()}\n",
        "counts_by_size = {k: len(v) for k, v in size_bins.items()}\n",
        "\n",
        "plt.figure(figsize=(8, 5))\n",
        "plt.bar(recall_by_size.keys(), recall_by_size.values(), color=['#FF9999', '#66B3FF', '#99FF99'])\n",
        "plt.title('Model Recall vs. Bounding Box Size')\n",
        "plt.ylabel('Recall')\n",
        "plt.ylim(0, 1)\n",
        "for i, (cat, recall) in enumerate(recall_by_size.items()):\n",
        "    plt.text(i, recall + 0.02, f'{recall:.2f}\\n(n={counts_by_size[cat]})', ha='center')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b8FDX0QSVBq4"
      },
      "source": [
        "<details>\n",
        "<summary>Click for Discussion</summary>\n",
        "\n",
        "You will likely observe that the recall for **small** boxes is significantly lower than for medium and large boxes. This is a very common finding.\n",
        "\n",
        "**Interpretation**: The model is much better at finding large, clear areas of pneumonia but struggles with smaller, potentially early-stage findings.\n",
        "\n",
        "**Clinical Implications**: This is a critical limitation. A model that consistently misses small findings could fail to detect pneumonia in its early stages, delaying treatment. This analysis highlights the need for strategies specifically aimed at improving small object detection, such as using higher-resolution images, employing specialized network architectures (like Feature Pyramid Networks, which our model already uses), or using data augmentation techniques that create more small object examples.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EqEGVCDlRenw"
      },
      "source": [
        "### 6.2. Confidence Score Analysis & Calibration\n",
        "\n",
        "A good model should not only be accurate, but its confidence scores should be meaningful. A prediction with 95% confidence should be correct much more often than a prediction with 60% confidence. This property is called **calibration**. Analyzing it is essential for deciding what confidence threshold to use in a real-world application.\n",
        "\n",
        "We will first visually inspect the distribution of confidence scores for the model's correct predictions (**True Positives**) versus its incorrect predictions (**False Positives**). This will give us an initial intuition about whether the model is generally more confident when it is correct. We will then quantify this relationship more formally with a **Reliability Diagram**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eU5q_mUgRenw"
      },
      "source": [
        "> ### Deep Dive: Model Calibration\n",
        ">\n",
        "> **What is calibration?** A model is considered well-calibrated if its predicted confidence scores accurately reflect the true probability of an event. For example, if we look at all the detections the model made with a confidence of 80-90%, a well-calibrated model would be correct on about 85% of them.\n",
        ">\n",
        "> **Why does it matter?** In high-stakes applications like medicine, calibration is crucial for building trust. A clinician needs to know if they can trust a model's confidence. An **over-confident** model (one that predicts high scores but is often wrong) is dangerous because it can lead to false assurances. An **under-confident** model is less useful because it may not be trusted even when it's correct.\n",
        ">\n",
        "> **How do we measure it?** We use a **Reliability Diagram**. This plot bins all predictions by their confidence score (e.g., 0-10%, 10-20%, etc.) on the x-axis. For each bin, it then calculates the actual accuracy (the fraction of true positives) and plots that on the y-axis. For a perfectly calibrated model, all points would lie on the diagonal line `y=x`. Deviations from this line show miscalibration."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QviUp8URenw"
      },
      "source": [
        "First, let's visually inspect the distribution of confidence scores for the model's correct predictions (True Positives) versus its incorrect predictions (False Positives)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lz6fYs0_Renw"
      },
      "source": [
        "### Q9: Analyze Confidence and Calibration\n",
        "**Your Task**: Analyze the two plots below.\n",
        "1.  **Score Distribution Plot**: This plot shows two histograms: one for the confidence scores of True Positives (correct detections) and one for False Positives (incorrect detections).\n",
        "    -   What do you observe? Ideally, where would you want these two distributions to be?\n",
        "    -   Based on this plot, is the model generally more confident when it is correct than when it is wrong?\n",
        "2.  **Reliability Diagram**: This plot shows model accuracy as a function of its confidence.\n",
        "    -   Does your model appear to be well-calibrated, over-confident, or under-confident? Why?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S8KbrD8JReny"
      },
      "outputs": [],
      "source": [
        "# Separate True Positives and False Positives\n",
        "tp_scores, fp_scores = [], []\n",
        "\n",
        "for preds, targets in zip(all_predictions, all_targets):\n",
        "    gt_boxes = targets['boxes']\n",
        "    pred_boxes = preds['boxes']\n",
        "    pred_scores = preds['scores']\n",
        "\n",
        "    if len(pred_boxes) == 0: continue\n",
        "\n",
        "    # Match predictions to ground truths\n",
        "    ious = box_iou(gt_boxes, pred_boxes) if len(gt_boxes) > 0 else torch.zeros((0, len(pred_boxes)))\n",
        "\n",
        "    # Track which GTs have been matched to prevent double counting\n",
        "    gt_matched = [False] * len(gt_boxes)\n",
        "\n",
        "    for i, pred_score in enumerate(pred_scores):\n",
        "        # Find the best GT match for this prediction\n",
        "        if ious.shape[0] > 0:\n",
        "            max_iou, max_idx = ious[:, i].max(dim=0)\n",
        "            if max_iou > IOU_THRESHOLD and not gt_matched[max_idx]:\n",
        "                tp_scores.append(pred_score.item())\n",
        "                gt_matched[max_idx] = True # Mark this GT as used\n",
        "            else:\n",
        "                fp_scores.append(pred_score.item())\n",
        "        else: # No GT boxes, all preds are FPs\n",
        "            fp_scores.append(pred_score.item())\n",
        "\n",
        "# Plot the Score Distributions\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.hist(tp_scores, bins=50, range=(0,1), density=True, color='green', alpha=0.7, label=f'True Positives (n={len(tp_scores)})')\n",
        "plt.hist(fp_scores, bins=50, range=(0,1), density=True, color='red', alpha=0.7, label=f'False Positives (n={len(fp_scores)})')\n",
        "plt.title('Confidence Score Distribution for Predictions')\n",
        "plt.xlabel('Confidence Score')\n",
        "plt.ylabel('Density')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "# Plot Reliability Diagram\n",
        "from sklearn.calibration import calibration_curve\n",
        "all_scores = tp_scores + fp_scores\n",
        "true_labels = [1] * len(tp_scores) + [0] * len(fp_scores)\n",
        "prob_true, prob_pred = calibration_curve(true_labels, all_scores, n_bins=10)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "plt.plot(prob_pred, prob_true, \"o-\", label=\"Model Calibration\")\n",
        "plt.plot([0, 1], [0, 1], \"k--\", label=\"Perfect Calibration\")\n",
        "plt.xlabel(\"Mean Predicted Confidence (in bin)\")\n",
        "plt.ylabel(\"Fraction of True Positives (in bin)\")\n",
        "plt.title(\"Reliability Diagram for Pneumonia Detector\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yOvC_G_WVIUw"
      },
      "source": [
        "\n",
        "<details>\n",
        "<summary>Click for Discussion</summary>\n",
        "\n",
        "1.  **Score Distribution Plot**:\n",
        "    -   **Observation**: You will likely see that the distribution for True Positives is shifted to the right (higher scores) compared to the distribution for False Positives. However, there will be a significant overlap.\n",
        "    -   **Ideal Scenario**: In a perfectly calibrated model, the True Positive distribution would be a sharp peak near a confidence of 1.0, and the False Positive distribution would be a sharp peak near 0.0, with very little overlap.\n",
        "    -   **Interpretation**: The overlap indicates that the model sometimes makes mistakes with high confidence and sometimes makes correct predictions with low confidence. While it is generally more confident when it is correct, its scores are not perfectly reliable indicators of correctness.\n",
        "\n",
        "2.  **Reliability Diagram**:\n",
        "    -   **Observation**: You will likely see that the blue line (model calibration) lies consistently **below** the dashed line (perfect calibration), especially for higher confidence bins.\n",
        "    -   **Interpretation**: This indicates the model is **over-confident**. For example, for the bin of predictions where the model's average confidence is ~0.9 (90%), the actual accuracy (fraction of true positives) might only be ~0.75 (75%). It systematically overestimates its own correctness, which is a common issue in modern neural networks.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e352NrqRReny"
      },
      "source": [
        "### 6.3. Visualizing the Hardest Cases\n",
        "\n",
        "While aggregated statistics and distributions are powerful, nothing builds understanding like looking at individual images. Instead of choosing random samples, it is far more insightful to programmatically find and visualize the model's most significant errors. This helps us build an intuition for *why* the model fails.\n",
        "\n",
        "We will focus on two critical error types:\n",
        "\n",
        "1.  **Top False Positives:** These are the detections the model made with the *highest confidence* but were actually incorrect. These are the model's most \"confident mistakes\" and reveal what kinds of image features are most likely to confuse it.\n",
        "2.  **Top False Negatives:** These are the ground truth pneumonia cases that the model *failed to detect* with a reasonable confidence score. From a clinical perspective, these \"misses\" are often the most serious type of error."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bWutlGAhReny"
      },
      "outputs": [],
      "source": [
        "from torchvision.ops import box_iou\n",
        "from matplotlib import patches\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "# Define a label map for our single class\n",
        "LABEL_MAP = {1: \"Pneumonia\"}\n",
        "\n",
        "# Helper to visualize single predictions\n",
        "def visualize_single_case(image, gt_boxes=None, pred_boxes=None, title=\"\"):\n",
        "    # Un-normalize for display\n",
        "    image = image.numpy().transpose(1, 2, 0)\n",
        "    mean = np.array([0.485, 0.456, 0.406])\n",
        "    std = np.array([0.229, 0.224, 0.225])\n",
        "    image = std * image + mean\n",
        "    image = np.clip(image, 0, 1)\n",
        "\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(8, 8))\n",
        "    ax.imshow(image)\n",
        "    ax.set_title(title, fontsize=16)\n",
        "\n",
        "    # Draw Ground Truth Boxes (Green)\n",
        "    if gt_boxes is not None:\n",
        "        for box in gt_boxes:\n",
        "            x1, y1, x2, y2 = box\n",
        "            # Draw the box\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='g', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            # Add the \"Ground Truth\" label above the box with the new style\n",
        "            ax.text(x1, y1 -1, 'Ground Truth',\n",
        "                    color='white',\n",
        "                    fontsize=14,\n",
        "                    bbox=dict(facecolor='green', edgecolor='none', pad=1.5))\n",
        "\n",
        "    # Draw Prediction Boxes (Red)\n",
        "    if pred_boxes is not None:\n",
        "        for box_data in pred_boxes:\n",
        "            # Assumes pred_boxes is a list of tuples: (box, score, label)\n",
        "            box, score, label = box_data\n",
        "            x1, y1, x2, y2 = box\n",
        "            # Draw the box\n",
        "            rect = patches.Rectangle((x1, y1), x2-x1, y2-y1, linewidth=2, edgecolor='r', facecolor='none')\n",
        "            ax.add_patch(rect)\n",
        "            # Add the label with score above the box using the requested style\n",
        "            label_text = LABEL_MAP.get(int(label), f'Class {int(label)}')\n",
        "            ax.text(x1, y1 - 1, f\"{label_text}:{score:.2f}\",\n",
        "                    color=\"white\",\n",
        "                    fontsize=14,\n",
        "                    bbox=dict(facecolor=\"red\", edgecolor=\"none\", pad=1.5))\n",
        "\n",
        "    ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Identify and sort False Positives by score\n",
        "# This part assumes 'all_predictions' and 'all_targets' are available from the previous QC steps\n",
        "fp_info = []\n",
        "IOU_THRESHOLD = 0.5 # A standard threshold\n",
        "\n",
        "for preds, targets in zip(all_predictions, all_targets):\n",
        "    gt_boxes = targets['boxes']\n",
        "    ious = box_iou(preds['boxes'], gt_boxes) if len(gt_boxes) > 0 else torch.zeros((len(preds['boxes']), 0))\n",
        "\n",
        "    for j, pred_box in enumerate(preds['boxes']):\n",
        "        max_iou = ious[j].max().item() if len(gt_boxes) > 0 else 0\n",
        "        if max_iou < IOU_THRESHOLD:\n",
        "            fp_info.append({\n",
        "                'image_idx': targets['image_id'].item(),\n",
        "                'box': pred_box.numpy(),\n",
        "                'score': preds['scores'][j].item(),\n",
        "                'label': preds['labels'][j].item()\n",
        "            })\n",
        "\n",
        "fp_info.sort(key=lambda x: x['score'], reverse=True)\n",
        "\n",
        "print(\"--- Top 3 Most Confident False Positives ---\")\n",
        "for fp in fp_info[:3]:\n",
        "    img, _ = test_dataset[fp['image_idx']]\n",
        "    # Pack prediction info into a list of tuples for the visualization function\n",
        "    pred_data_for_vis = [(fp['box'], fp['score'], fp['label'])]\n",
        "    visualize_single_case(img, pred_boxes=pred_data_for_vis, title=f\"False Positive (Score: {fp['score']:.3f})\")\n",
        "\n",
        "# Identify False Negatives\n",
        "fn_info = []\n",
        "for i, (preds, targets) in enumerate(zip(all_predictions, all_targets)):\n",
        "    gt_boxes = targets['boxes']\n",
        "    pred_boxes = preds['boxes'][preds['scores'] > 0.5] # Only consider confident predictions\n",
        "\n",
        "    if len(gt_boxes) == 0: continue\n",
        "\n",
        "    ious = box_iou(gt_boxes, pred_boxes) if len(pred_boxes) > 0 else torch.zeros((len(gt_boxes), 0))\n",
        "\n",
        "    for j, gt_box in enumerate(gt_boxes):\n",
        "        max_iou = ious[j].max().item() if len(pred_boxes) > 0 else 0\n",
        "        if max_iou < IOU_THRESHOLD:\n",
        "            fn_info.append({\n",
        "                'image_idx': targets['image_id'].item(),\n",
        "                'box': gt_box.numpy()\n",
        "            })\n",
        "\n",
        "print(\"\\n--- 3 Examples of False Negatives (Missed Detections) ---\")\n",
        "for fn in fn_info[:3]:\n",
        "    img, _ = test_dataset[fn['image_idx']]\n",
        "    visualize_single_case(img, gt_boxes=[fn['box']], title=\"False Negative (Missed)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fOv1JoUvOYIY"
      },
      "source": [
        "\n",
        "> 65 minutes\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1PkBCRPKy1M5"
      },
      "source": [
        "## 7. Advanced Quality Control II: Model Explainability with Grad-CAM\n",
        "\n",
        "We have now evaluated our model's performance with metrics (the **what**) and analyzed its specific failure modes (the **where**). But to build true trust in a model, especially in a clinical setting, we must be able to answer the most important question: **why?**\n",
        "\n",
        "-   *Why* did the model make a particular prediction?\n",
        "-   *Which specific features* in the image led to its decision?\n",
        "-   Is the model \"looking\" at the clinically relevant pathology, or is it exploiting a spurious correlation or artifact in the image?\n",
        "\n",
        "This is the domain of **Explainable AI (XAI)**. XAI techniques aim to peek inside the \"black box\" of a neural network to make its decision-making process more transparent and interpretable.\n",
        "\n",
        "For this task, we will use **Grad-CAM (Gradient-weighted Class Activation Mapping)**. Grad-CAM is a powerful and popular XAI technique that produces a visual **heatmap**, highlighting the most important regions in an input image for a particular prediction.\n",
        "\n",
        "-   **Hotter areas (red/yellow)** on the heatmap indicate pixels that strongly influenced the model to make its prediction.\n",
        "-   **Cooler areas (blue/green)** indicate pixels that were less influential.\n",
        "\n",
        "By overlaying this heatmap on our original X-ray, we can visually verify if our model is focusing on the actual pneumonia opacities or if it's being \"distracted\" by irrelevant features. This is a critical final step in validating whether our model is not just accurate, but also right for the right reasons."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LJ-eoTEwy1M7"
      },
      "source": [
        "> ### Deep Dive: How Grad-CAM Works\n",
        ">\n",
        "> Grad-CAM creates its heatmap by combining two key pieces of information from the model:\n",
        ">\n",
        "> 1.  **Feature Maps from a Convolutional Layer**: Deep inside the network, convolutional layers produce feature maps that highlight abstract patterns like textures, edges, and shapes. The final convolutional layers capture the most high-level, class-specific information.\n",
        ">\n",
        "> 2.  **Gradients**: It calculates the gradient (the importance signal) of the model's final prediction score with respect to each feature map. A high gradient for a particular feature map means that map was very influential in the final decision.\n",
        ">\n",
        "> Grad-CAM then computes a weighted average of all the feature maps, where the weight for each map is its gradient. The result is a single heatmap that shows a spatially-localized summary of where the model found the most important evidence for its prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enFD-mfZy1M7"
      },
      "outputs": [],
      "source": [
        "# First, we need to install the pytorch-grad-cam library\n",
        "!pip install grad-cam -q\n",
        "\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "print(\"Grad-CAM library and helpers imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yh7UiNP56qcx"
      },
      "source": [
        "#### Helper function\n",
        "`faster_rcnn_target_function` This function takes the model's raw output and returns a single scalar value (the sum of pneumonia scores) for Grad-CAM to explain. This is more robust than the library's built-in helpers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dewKEfe92eQA"
      },
      "outputs": [],
      "source": [
        "def faster_rcnn_target_function(output):\n",
        "    # For a single image, the output is a dictionary.\n",
        "    scores = output['scores']\n",
        "    labels = output['labels']\n",
        "\n",
        "    # Find the indices of all boxes predicted as \"pneumonia\" (label 1)\n",
        "    pneumonia_indices = (labels == 1).nonzero(as_tuple=True)[0]\n",
        "\n",
        "    if len(pneumonia_indices) == 0:\n",
        "        # If no pneumonia is detected, there is no score to explain.\n",
        "        # Return a zero tensor.\n",
        "        return torch.tensor(0.0, device=scores.device)\n",
        "\n",
        "    # Sum the scores of all detected pneumonia boxes\n",
        "    pneumonia_scores = scores[pneumonia_indices]\n",
        "    return pneumonia_scores.sum()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEP2LtAHy1M8"
      },
      "source": [
        "### Q10: Select a target layer for explanation\n",
        "**Your Task**: Grad-CAM needs to hook into a specific convolutional layer to generate its heatmap. The best layers are usually the final, deep feature-extracting layers of the model's backbone, as they contain the richest semantic information.\n",
        "\n",
        "Let's inspect the model's backbone architecture. Based on the output, identify a suitable final layer. A good choice is often the last block of the main feature extractor. Fill in the `...` with the correct layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCmaOWH1y1M8"
      },
      "outputs": [],
      "source": [
        "# Let's print the model's backbone to see the layers\n",
        "print(best_model.backbone.body)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nNSmHZDMF2Eq"
      },
      "outputs": [],
      "source": [
        "# Q10: Define the target layer for Grad-CAM\n",
        "target_layer = [best_model.backbone.body.layer4]\n",
        "\n",
        "# Instantiate the GradCAM object\n",
        "cam = GradCAM(model=best_model, target_layers=target_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WOfOJRIBFvCF"
      },
      "outputs": [],
      "source": [
        "# Q10: Define the target layer for Grad-CAM\n",
        "# target_layer = [...] # Fill this in based on the architecture above\n",
        "\n",
        "# Instantiate the GradCAM object\n",
        "cam = GradCAM(model=best_model, target_layers=target_layer)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ePDtsZw5y1M8"
      },
      "source": [
        "### Q11: Generate and Interpret the Grad-CAM Visualizations\n",
        "**Your Task**: Run the code below to generate Grad-CAM heatmaps for a few test images. The code will display three images for each sample:\n",
        "1.  The original image with ground truth (green) and prediction (red) boxes.\n",
        "2.  The raw heatmap generated by Grad-CAM.\n",
        "3.  The heatmap overlaid on the original image.\n",
        "\n",
        "**Analyze the results:**\n",
        "-   Do the high-attention areas (red/yellow) in the heatmap correspond to the actual pneumonia opacities?\n",
        "-   In cases where the model made a **False Positive** (a red box with no green box), where is the heatmap located? Does this give you a clue as to what might have confused the model?\n",
        "-   In cases where the model made a **False Negative** (a green box with no red box), is there any activation in the heatmap at all, or did the model completely ignore the area?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4f2VkGlzy1M8"
      },
      "outputs": [],
      "source": [
        "# Define a label map for our single class\n",
        "LABEL_MAP = {1: \"Pneumonia\"}\n",
        "\n",
        "def visualize_grad_cam(model, dataloader, cam_instance, num_samples=4, score_threshold=0.5):\n",
        "    model.eval()\n",
        "    images, targets = next(iter(dataloader))\n",
        "    images = list(img.to(device) for img in images)\n",
        "\n",
        "    print(\"--- Grad-CAM Explanations ---\")\n",
        "    print(\"Green = Ground Truth | Red = Prediction\")\n",
        "\n",
        "    for i in range(min(num_samples, len(images))):\n",
        "        input_tensor = images[i].unsqueeze(0)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            pred = model(input_tensor)[0]\n",
        "\n",
        "        # Skip CAM generation if no boxes are detected\n",
        "        if len(pred['boxes']) == 0:\n",
        "            print(f\"Sample {i+1}: No objects detected by the model. Skipping CAM generation.\")\n",
        "            # (Code to show image and GT for context remains the same)\n",
        "            continue\n",
        "\n",
        "        # Use our custom target function for Grad-CAM\n",
        "        cam_targets = [faster_rcnn_target_function]\n",
        "        grayscale_cam = cam_instance(input_tensor=input_tensor, targets=cam_targets)[0, :]\n",
        "\n",
        "        # Visualization\n",
        "        img_np = input_tensor[0].cpu().permute(1, 2, 0).numpy()\n",
        "        mean, std = np.array([0.485, 0.456, 0.406]), np.array([0.229, 0.224, 0.225])\n",
        "        img_unnormalized = std * img_np + mean\n",
        "        img_unnormalized = np.clip(img_unnormalized, 0, 1)\n",
        "\n",
        "        cam_overlay = show_cam_on_image(img_unnormalized, grayscale_cam, use_rgb=True)\n",
        "\n",
        "        fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "        # Plot 1: Original with Boxes and Labels\n",
        "        axes[0].imshow(img_unnormalized)\n",
        "        axes[0].set_title('Original + Predictions')\n",
        "        axes[0].axis('off')\n",
        "\n",
        "        # Plot 2: Raw Heatmap\n",
        "        axes[1].imshow(grayscale_cam)\n",
        "        axes[1].set_title('Grad-CAM Heatmap')\n",
        "        axes[1].axis('off')\n",
        "\n",
        "        # Plot 3: Overlay with Boxes and Labels\n",
        "        axes[2].imshow(cam_overlay)\n",
        "        axes[2].set_title('Overlay')\n",
        "        axes[2].axis('off')\n",
        "\n",
        "        # Draw boxes and labels on both relevant plots (axes[0] and axes[2])\n",
        "        for ax in [axes[0], axes[2]]:\n",
        "            # Ground truth boxes (Green)\n",
        "            for box in targets[i]['boxes']:\n",
        "                x1, y1, x2, y2 = box.cpu().numpy()\n",
        "                ax.add_patch(patches.Rectangle((x1, y1), x2-x1, y2-y1, lw=2, ec='g', fc='none'))\n",
        "\n",
        "                # # Add styled text label for predictions\n",
        "                ax.text(x1, y1 - 1, \"Ground truth\",\n",
        "                        color=\"white\",\n",
        "                        fontsize=14,\n",
        "                        bbox=dict(facecolor=\"green\", edgecolor=\"none\", pad=1.5))\n",
        "\n",
        "            # Prediction boxes (Red) with labels\n",
        "            for box, score, label in zip(pred['boxes'], pred['scores'], pred['labels']):\n",
        "                if score > score_threshold:\n",
        "                    x1, y1, x2, y2 = box.cpu().numpy()\n",
        "                    ax.add_patch(patches.Rectangle((x1, y1), x2-x1, y2-y1, lw=2, ec='r', fc='none'))\n",
        "\n",
        "                    # Add styled text label for predictions\n",
        "                    label_text = LABEL_MAP.get(int(label), f'Class {int(label)}')\n",
        "                    ax.text(x1, y1 - 1, f\"{label_text}:{score:.2f}\",\n",
        "                            color=\"white\",\n",
        "                            fontsize=14,\n",
        "                            bbox=dict(facecolor=\"red\", edgecolor=\"none\", pad=1.5))\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "du6QNB5KOgqG"
      },
      "source": [
        "\n",
        "> 85 minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YdVfO5vn3n7m"
      },
      "outputs": [],
      "source": [
        "# Instantiate the GradCAM object\n",
        "threshold = 0.6  #@param {type: \"slider\", min: 0, max: 1, step: 0.05}\n",
        "# Define the layer to use for Grad-CAM visualization\n",
        "\n",
        "target_layer = [best_model.backbone.body.layer4]\n",
        "# Or dynamically get the selected layer from the model's backbone body\n",
        "layer_to_use = \"layer3\"  #@param ['layer1', 'layer2', 'layer3', 'layer4']\n",
        "# We need to handle the case where the user might enter a layer name that doesn't exist,\n",
        "# although the dropdown should prevent this. Using getattr is safer.\n",
        "if hasattr(best_model.backbone.body, layer_to_use):\n",
        "    target_layer = [getattr(best_model.backbone.body, layer_to_use)]\n",
        "    print(f\"Target layer for Grad-CAM set to: {layer_to_use}\")\n",
        "else:\n",
        "    print(f\"Error: Layer '{layer_to_use}' not found in the model's backbone body.\")\n",
        "    target_layer = None # Set to None or a default if the layer is not found\n",
        "\n",
        "\n",
        "cam = GradCAM(model=best_model, target_layers=target_layer)\n",
        "\n",
        "# Run the updated visualization on the test loader\n",
        "visualize_grad_cam(best_model, test_loader, cam, num_samples=5, score_threshold= threshold)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vAISk9GORenz"
      },
      "source": [
        "## Conclusion and final thoughts\n",
        "\n",
        "Congratulations on completing this comprehensive, hands-on journey through medical object detection!\n",
        "\n",
        "Over the course of this notebook, you have successfully built and analyzed a deep learning model from start to finish. You have not just trained a model, but have also engaged in the critical practices that separate a simple experiment from a robust, well-understood AI system.\n",
        "\n",
        "**Let's recap what you have achieved:**\n",
        "-   You successfully handled and prepared a real-world medical imaging dataset in the complex **DICOM** format.\n",
        "-   You built a custom **PyTorch `Dataset` and `DataLoader`**, mastering the specific data pipeline required for object detection.\n",
        "-   You fine-tuned a state-of-the-art **Faster R-CNN** model, a powerful architecture used across the industry.\n",
        "-   You learned to interpret crucial **object detection metrics** like mAP, moving beyond simple accuracy.\n",
        "-   Most importantly, you performed an **advanced quality control analysis**, investigating your model's biases, calibration, and explainability with **Grad-CAM**.\n",
        "\n",
        "**Key Takeaways:**\n",
        "-   **Data is paramount:** A deep understanding of your data through EDA and quality control is the foundation of any successful model.\n",
        "-   **Evaluation is multi-faceted:** A single metric is never enough. A thorough analysis of performance across different subsets (like object size), along with visual inspection of failure modes, is essential.\n",
        "-   **Explainability builds trust:** For a model to be useful in a high-stakes field like medicine, we must be able to understand *why* it makes its decisions. Tools like Grad-CAM are a vital step toward building trustworthy AI.\n",
        "\n",
        "### Next steps\n",
        "\n",
        "> This notebook provides a strong foundation. From here, you could explore more advanced models, experiment with different data augmentation strategies, or incorporate the negative (non-pneumonia) samples to build a more comprehensive diagnostic tool.\n",
        "\n",
        "Here are some exciting next steps you could take to build upon what you've learned:\n",
        "\n",
        "-   **Incorporate Negative Samples:** Modify the `Dataset` to include the non-pneumonia cases. This would allow you to train a more complete diagnostic tool that can act as both a detector and a classifier.\n",
        "-   **Experiment with Different Architectures:** Try replacing the Faster R-CNN model with more modern, single-stage detectors like **YOLO** or **RetinaNet**.\n",
        "-   **Hyperparameter Tuning:** Systematically experiment with different learning rates, batch sizes, and data augmentation strategies to further improve your model's mAP score.\n",
        "-   **Explore 3D Medical Imaging:** Apply these concepts to 3D datasets like CT or MRI scans, where the challenges of data handling and model architecture become even more interesting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kBXX2i4ZOeF4"
      },
      "source": [
        "\n",
        "> 90 minutes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}