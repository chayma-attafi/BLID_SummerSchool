{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "T6BeY23EjU0L",
      "metadata": {
        "id": "T6BeY23EjU0L"
      },
      "source": [
        "# Image Classification with Deep Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cgm9viPTP_dN",
      "metadata": {
        "id": "cgm9viPTP_dN"
      },
      "source": [
        "Welcome to the **BILD 2025 Summer School** hands-on session on medical image classification! This notebook will guide you through the complete pipeline of building and critically evaluating a model to identify diseases in chest X-rays.\n",
        "\n",
        "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/albarqounilab/BILD-Summer-School/blob/main/notebooks/day1/classification_exercise.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xY9NKCq8cO48",
      "metadata": {
        "id": "xY9NKCq8cO48"
      },
      "source": [
        "![alt_text](https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/helpers/notebook-banner.png)\n",
        "\n",
        "BILD 2025 is organized under the umbrella of the [Strategic Arab-German Network for Affordable and Democratized AI in Healthcare (SANAD)](https://albarqouni.github.io/funded/sanad/), uniting academic excellence and technological innovation across borders. This year’s edition is organized by the [Albarqouni Lab](https://albarqouni.github.io/) at the [University Hospital Bonn](https://www.ukbonn.de/) and the [University of Bonn](https://www.uni-bonn.de/en). We are proud to partner with leading institutions in the region—Lebanese American University, University of Tunis El Manar, and Duhok Polytechnic University — to deliver a truly international learning experience. Over five intensive days in Tunis, you will explore cutting-edge deep-learning techniques for medical imaging through expert lectures, hands-on labs, and collaborative case studies. Engage with peers and faculty from Germany, Lebanon, Iraq, and Tunisia as you develop practical skills in building and deploying AI models for real-world healthcare challenges. We look forward to an inspiring week of interdisciplinary exchange and the shared commitment to advancing affordable, life-saving AI in medicine.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ea5603fa-4f2d-483d-ba04-4aeed8ddf7c6",
      "metadata": {
        "id": "ea5603fa-4f2d-483d-ba04-4aeed8ddf7c6"
      },
      "source": [
        "## Chest-X-Ray Classification [90 mins]\n",
        "\n",
        "### Today's Goals\n",
        "\n",
        "This session is a practical journey into building and validating a clinical AI tool. By the end of this notebook, you will be able to:\n",
        "\n",
        "-  **Prepare Medical Imaging Data**: Load, preprocess, and structure chest X-ray images and their corresponding metadata.\n",
        "-  **Perform Exploratory Data Analysis (EDA) for Fairness**: Investigate dataset demographics (age, sex) to identify potential sources of bias before training.\n",
        "-  **Train and Benchmark Classifiers**: Fine-tune multiple state-of-the-art CNN architectures (e.g., DenseNet, EfficientNet) for disease classification.\n",
        "-  **Master Classification Metrics**: Implement and interpret evaluation metrics such as Accuracy, Precision, Recall, F1-score, and ROC-AUC.\n",
        "-  **Conduct a Fairness Audit**: Evaluate model performance across different demographic subgroups to detect potential biases in predictions.\n",
        "-  **Apply Explainable AI (XAI)**: Use Grad-CAM to visualize model attention and ensure predictions are based on clinically relevant features.\n",
        "\n",
        "### Objectives\n",
        "\n",
        "You’ll see how AI can be trained to identify pathological findings in chest X-rays, a crucial step in computer-assisted diagnosis. You’ll also apply your classification skills to a challenging real-world problem in medical imaging, while learning how to interpret and validate model predictions beyond raw accuracy scores."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d9ccb1-08d3-4673-b513-39b15963f31c",
      "metadata": {
        "id": "a6d9ccb1-08d3-4673-b513-39b15963f31c"
      },
      "source": [
        "### Dataset\n",
        "\n",
        "The [NIH ChestX-ray-14](https://nihcc.app.box.com/v/ChestXray-NIHCC) dataset is a large collection of chest X-ray images. Each image comes with information about the patient and labels that tell us which diseases (if any) are present. This dataset is widely used in medical AI research because it helps us train and test models to recognize diseases from X-ray images.\n",
        "\n",
        "**What does the dataset contain?**\n",
        "1. Chest X-ray images, each in PNG format. These are pictures of the inside of the chest, showing the lungs and heart.\n",
        "2. A metadata file (`Data_Entry_2017.csv`) that lists information about each image, such as:\n",
        "   - Which diseases are present (if any)\n",
        "   - Patient age and gender\n",
        "   - How the image was taken\n",
        "3. A file with bounding boxes (`BBox_List_2017.csv`) for about 1,000 images. These boxes show exactly where a disease is located in the image.\n",
        "4. Files that split the data into training and test sets. This is important because we want to train our model on some images and test it on others to see how well it works on new data.\n",
        "\n",
        "**Why do we use this dataset?**\n",
        "- It is large and diverse, which helps our model learn better.\n",
        "- It has real medical labels, making our project more realistic.\n",
        "- It allows us to practice both classification (is there a disease?) and detection (where is the disease?).\n",
        "\n",
        "In this notebook, we will use a smaller sample of this dataset and pre-trained models to make the exercises faster and easier to follow."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "QDLtGwZxWZV8",
      "metadata": {
        "id": "QDLtGwZxWZV8"
      },
      "source": [
        "\n",
        "## 1. Environment Setup\n",
        "\n",
        "We install and import required libraries. Run this once per new environment.\n",
        "\n",
        "> **Note:** The cell will install packages (internet required). If you're offline, skip installation and ensure the environment already has the packages.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "O36QhLgXWeBY",
      "metadata": {
        "id": "O36QhLgXWeBY",
        "outputId": "93b90123-e95e-46f3-d19f-eab0ea10ecdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.8 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/7.8 MB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m5.5/7.8 MB\u001b[0m \u001b[31m79.8 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.5 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m70.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.6/52.6 MB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.0/983.0 kB\u001b[0m \u001b[31m67.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for grad-cam (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Setup complete. Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#@title import libraries (2 minutes)\n",
        "# Library Installations\n",
        "!pip install pydicom SimpleITK albumentations torchmetrics grad-cam -q\n",
        "\n",
        "# Core Library Imports\n",
        "import os\n",
        "import sys, math, random, time, warnings\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# Data Handling and Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "\n",
        "# Deep Learning with PyTorch & Torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# Visualization & Metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm.notebook import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, RocCurveDisplay\n",
        "from torchmetrics.classification import BinaryAUROC\n",
        "\n",
        "# Explainability (XAI)\n",
        "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# Environment Configuration\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Ensure reproducibility\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "seed_everything(RANDOM_SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Setup complete. Using device: {device}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LvHCaUJHW5w4",
      "metadata": {
        "id": "LvHCaUJHW5w4"
      },
      "source": [
        "## 2. The Dataset: ChestX-ray14 (NIH) ( 3 minutes)\n",
        "\n",
        "We’ll use the NIH ChestX-ray14 dataset:\n",
        "\n",
        "- Size: ~9.2GB subset\n",
        "\n",
        "- Format: JPG images + accompanying CSV files with labels\n",
        "\n",
        "- Source: NIH Clinical Center\n",
        "\n",
        "**What does the dataset contain?**\n",
        "1. A subset of over 20k chest X-ray images, each in PNG format. These are pictures of the inside of the chest, showing the lungs and heart.\n",
        "2. A metadata file (`Data_Entry_2017.csv`) that lists information about each image, such as:\n",
        "   - Which diseases are present (if any)\n",
        "   - Patient age and gender\n",
        "   - How the image was taken\n",
        "3. A file with bounding boxes (`BBox_List_2017.csv`) for about 1,000 images. These boxes show exactly where a disease is located in the image.\n",
        "4. Files that split the data into training and test sets. This is important because we want to train our model on some images and test it on others to see how well it works on new data.\n",
        "\n",
        "### 2.1 Downloading the Data (3 minutes)\n",
        "Before we can work with the data, we need to download and unzip it. This means we are copying the files from the internet to our computer and making them ready to use.\n",
        "\n",
        "**Why do we do this?**\n",
        "- Machine learning models need data to learn from. Downloading the dataset gives us the images and labels we need for our project.\n",
        "- Unzipping extracts the files from a compressed format so we can access them easily in our code.\n",
        "\n",
        "**Instructions:**\n",
        "- If you have not downloaded the dataset yet, run the following cells to download and unzip the files.\n",
        "- If you already have the data, you can skip these steps by adding a `#` before the `!` in the code (this comments out the line so it won't run).\n",
        "- You can also change the `DATA_PATH` variable if you want to store the data in a different folder.\n",
        "\n",
        "> **Tip:** Downloading large datasets can take a while, depending on your internet speed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qghE049-aGEh",
      "metadata": {
        "id": "qghE049-aGEh",
        "outputId": "02312f44-5974-4332-d40a-41e6345cbf12",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.7/67.7 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "Fetching 1 files:   0% 0/1 [00:00<?, ?it/s]Downloading 'Classification/data_cxr8.zip' to '.cache/huggingface/download/Classification/E1igAzaz1r3gHCgiQOYK0fymcSc=.5cab74c52d5b52d63394a0a51e49a02de4e5f9ed6a4801d9e436e79de7acae9e.incomplete'\n",
            "\n",
            "data_cxr8.zip:   0% 0.00/9.88G [00:00<?, ?B/s]\u001b[A\n",
            "data_cxr8.zip:   0% 10.5M/9.88G [00:01<21:36, 7.61MB/s]\u001b[A\n",
            "data_cxr8.zip:   0% 21.0M/9.88G [00:01<11:56, 13.8MB/s]\u001b[A\n",
            "data_cxr8.zip:   0% 31.5M/9.88G [00:02<08:51, 18.5MB/s]\u001b[A\n",
            "data_cxr8.zip:   0% 41.9M/9.88G [00:02<07:22, 22.2MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 52.4M/9.88G [00:02<06:28, 25.3MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 62.9M/9.88G [00:03<06:00, 27.2MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 73.4M/9.88G [00:03<05:51, 27.9MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 83.9M/9.88G [00:03<05:37, 29.0MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 94.4M/9.88G [00:04<05:37, 29.0MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 105M/9.88G [00:04<05:32, 29.4MB/s] \u001b[A\n",
            "data_cxr8.zip:   1% 115M/9.88G [00:04<05:33, 29.3MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 126M/9.88G [00:05<05:40, 28.6MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 136M/9.88G [00:05<05:44, 28.3MB/s]\u001b[A\n",
            "data_cxr8.zip:   1% 147M/9.88G [00:05<05:50, 27.8MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 157M/9.88G [00:06<05:55, 27.4MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 168M/9.88G [00:06<05:57, 27.1MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 178M/9.88G [00:07<05:58, 27.1MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 189M/9.88G [00:07<05:58, 27.0MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 199M/9.88G [00:07<06:00, 26.9MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 210M/9.88G [00:08<05:56, 27.1MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 220M/9.88G [00:08<05:52, 27.4MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 231M/9.88G [00:08<05:42, 28.2MB/s]\u001b[A\n",
            "data_cxr8.zip:   2% 241M/9.88G [00:09<05:39, 28.4MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 252M/9.88G [00:09<05:36, 28.6MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 262M/9.88G [00:10<05:32, 28.9MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 273M/9.88G [00:10<05:31, 29.0MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 283M/9.88G [00:10<05:33, 28.8MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 294M/9.88G [00:11<05:30, 29.0MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 304M/9.88G [00:11<05:23, 29.6MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 315M/9.88G [00:11<05:18, 30.0MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 325M/9.88G [00:12<05:11, 30.7MB/s]\u001b[A\n",
            "data_cxr8.zip:   3% 336M/9.88G [00:12<05:08, 30.9MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 346M/9.88G [00:13<06:46, 23.5MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 357M/9.88G [00:13<06:25, 24.7MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 367M/9.88G [00:13<06:09, 25.8MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 377M/9.88G [00:14<05:50, 27.1MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 388M/9.88G [00:14<05:42, 27.7MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 398M/9.88G [00:15<05:44, 27.5MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 409M/9.88G [00:15<05:44, 27.5MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 419M/9.88G [00:15<05:48, 27.1MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 430M/9.88G [00:16<05:50, 27.0MB/s]\u001b[A\n",
            "data_cxr8.zip:   4% 440M/9.88G [00:16<05:44, 27.4MB/s]\u001b[A"
          ]
        }
      ],
      "source": [
        "DATA_PATH = \"./Classification\"\n",
        "!pip -q install -U \"huggingface_hub[cli]\" -q\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/data_cxr8.zip\" --local-dir ./\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/csv.zip\" --local-dir ./\n",
        "\n",
        "!unzip -q ./Classification/data_cxr8.zip -d {DATA_PATH}\n",
        "!unzip -q ./Classification/csv.zip -d {DATA_PATH}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UfU_19EebALr",
      "metadata": {
        "id": "UfU_19EebALr"
      },
      "source": [
        "### 2.3. Data Exploration & Fairness Pre-Analysis\n",
        "\n",
        "Before we write a single line of model code, we must understand our data. This process, known as **Exploratory Data Analysis (EDA)**, is our first and most important quality control step. A model trained on poorly understood or biased data will produce untrustworthy results, a risk we cannot afford in a clinical setting.\n",
        "\n",
        "In this section, we will:\n",
        "1.  **Visualize the Data:** Perform a \"sanity check\" to ensure images and labels are loading correctly.\n",
        "2.  **Analyze Label Distribution:** Understand the prevalence of different pathologies to identify class imbalance.\n",
        "3.  **Conduct a Pre-training Fairness Audit:** Investigate the demographic makeup of our dataset (patient age and sex). An imbalance here can lead to a model that performs poorly for underrepresented groups. This proactive check is a cornerstone of building responsible AI.\n",
        "\n",
        "Let's first start by understanding the data structure inside the Classification/ folder:\n",
        "\n",
        "```\n",
        "Classification/\n",
        "  images/                              # Contains chest X-ray PNG images\n",
        "    00000005_003.png\n",
        "    00000005_006.png\n",
        "    00000005_007.png\n",
        "    ...\n",
        "  metadata.csv                         # Full metadata: image IDs, labels, patient info\n",
        "  metadata_filtered.csv                # Processed/filtered metadata\n",
        "  train_df.csv                         # Training set split\n",
        "  val_df.csv                           # Validation set split\n",
        "  test_df.csv                          # Test set split\n",
        "  train_val_list.txt                   # Combined train/val list\n",
        "  test_list.txt                        # Test image list\n",
        "  densenet121-classification.pth       # Pretrained DenseNet121 model weights\n",
        "  efficientnet-classification.pth      # Pretrained EfficientNet model weights\n",
        "  swintransformer-classification.pth   # Pretrained Swin Transformer model weights\n",
        "\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "oPha5-43tr6X",
      "metadata": {
        "id": "oPha5-43tr6X"
      },
      "source": [
        "### Load dataframe metadata\n",
        "\n",
        "A **dataframe** is a table of data, like a spreadsheet, that we can easily work with in Python using the pandas library. Here, we load the metadata for all our images. This metadata tells us important information about each image, such as which diseases are present, the patient ID, and more. Loading this information helps us organize and prepare our data for training and testing our model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Vffq_f-m6X8g",
      "metadata": {
        "id": "Vffq_f-m6X8g"
      },
      "outputs": [],
      "source": [
        "# Load and observe available data\n",
        "DATA_PATH = \"./Classification\"\n",
        "metadata_df = pd.read_csv(f'{DATA_PATH}/metadata.csv')\n",
        "metadata_df#.head() # Print the 5 fist rows of the dataframe"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kxMCwAB0R5Xk",
      "metadata": {
        "id": "kxMCwAB0R5Xk"
      },
      "source": [
        "In our dataset, we have two types of information:\n",
        "\n",
        "- A metadata file (metadata.csv) that lists more than 112,000 entries, one for each expected X-ray.\n",
        "\n",
        "- A folder of actual images, which contains only 24,502 files.\n",
        "\n",
        "Now the question is: do all metadata entries have a corresponding image file? Let’s visualize this relationship.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/metadata_images_relationship.png\" width=\"600\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Fq5QWbf7QRRb",
      "metadata": {
        "id": "Fq5QWbf7QRRb"
      },
      "source": [
        "This view makes it even clearer:\n",
        "\n",
        "- Most of the data exists only in the metadata file, but without images, we can’t use them.\n",
        "\n",
        "- The overlap gives us the real working subset we’ll use for training and evaluation: 24,502 chest X-rays.\n",
        "\n",
        "- The orange area for ‘Only in Images’ is empty, meaning every image in the folder has a metadata entry — which is good for consistency."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DP7dICXbtzei",
      "metadata": {
        "id": "DP7dICXbtzei"
      },
      "source": [
        "Now we need to make sure that the information in our dataframe matches the images we actually downloaded. This step filters out any entries in the metadata that do not have a corresponding image file, so we only work with images that are available on our computer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ITpC_G566lP_",
      "metadata": {
        "id": "ITpC_G566lP_"
      },
      "outputs": [],
      "source": [
        "imgs = glob(f'{DATA_PATH}/images/*')\n",
        "imgs_basename = [os.path.basename(i) for i in imgs]\n",
        "\n",
        "metadata_df = metadata_df.loc[metadata_df['Image Index'].isin(imgs_basename)]\n",
        "metadata_df.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ij9dk1gTfZwC",
      "metadata": {
        "id": "ij9dk1gTfZwC"
      },
      "outputs": [],
      "source": [
        "#@title Plot Patient Images\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Show a few random patient images\n",
        "def plot_random_images(metadata_df, data_path, n=6):\n",
        "    \"\"\"Plots n random images with their labels.\"\"\"\n",
        "    sample_df = metadata_df.sample(n)\n",
        "    plt.figure(figsize=(15, 8))\n",
        "\n",
        "    for i, (_, row) in enumerate(sample_df.iterrows()):\n",
        "        img_path = os.path.join(data_path, \"images\", row[\"Image Index\"])\n",
        "        img = plt.imread(img_path)\n",
        "\n",
        "        plt.subplot(2, n//2, i+1)\n",
        "        plt.imshow(img, cmap=\"gray\")\n",
        "        plt.title(f\"Patient {row['Patient ID']}\\nLabel: {row['Finding Labels']}\", fontsize=9)\n",
        "        plt.axis(\"off\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Call the function\n",
        "plot_random_images(metadata_df, DATA_PATH, n=6)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DkfLApQltnLE",
      "metadata": {
        "id": "DkfLApQltnLE"
      },
      "source": [
        "##3. Data Splitting & Preparation\n",
        "\n",
        "#### Deep Dive: The Golden Rule of Medical AI - Splitting by Patient\n",
        "\n",
        "How we split our data is one of the most critical decisions in the entire project. In medical imaging, there is one golden rule: **always split by patient, never by image.**\n",
        "\n",
        "**Why is this so important?**\n",
        "Slices or images from the same patient are highly correlated. They share the same unique anatomy, come from the same scanner session, and may have similar imaging artifacts.\n",
        "\n",
        "If we were to randomly put some images from Patient A into the training set and others into the test set, our model could \"cheat.\" It might learn to recognize Patient A's specific anatomy instead of the general features of the disease. This is a critical error called **data leakage**. It leads to an artificially high performance score during testing, but the model would fail dramatically when deployed on a new, unseen patient.\n",
        "\n",
        "By splitting our data at the patient level, we guarantee that all images from a single patient belong exclusively to either the training, validation, or test set. This ensures our final evaluation is an honest measure of the model's ability to generalize to new individuals.\n",
        "\n",
        "### 3.1 Load patient splits\n",
        "\n",
        "To train and evaluate our model properly, we need to split our data into different groups:\n",
        "- **Training set:** Used to teach the model.\n",
        "- **Validation set:** Used to check how well the model is learning during training.\n",
        "- **Test set:** Used to see how well the model works on completely new data.\n",
        "\n",
        "In this step, we load lists of which images belong to each group. This helps us make sure that the model is tested on images it has never seen before, which is important for getting a fair measure of its performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "eipGr43oiUPw",
      "metadata": {
        "id": "eipGr43oiUPw"
      },
      "outputs": [],
      "source": [
        "train_val_patients = pd.read_csv(f'{DATA_PATH}/train_val_list.txt', header=None, names=['patientId'])\n",
        "test_patients = pd.read_csv(f'{DATA_PATH}/test_list.txt', header=None, names=['patientId'])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E5vsy6EFi4pj",
      "metadata": {
        "id": "E5vsy6EFi4pj"
      },
      "source": [
        "The `.txt` files contain lists of image names that belong to the training/validation or test sets. To use these splits, we need to match the image names in these files with the information in our main database (`metadata.csv`). This way, we know which images and labels go into each group for training and testing."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bJ6kj9ArLWPg",
      "metadata": {
        "id": "bJ6kj9ArLWPg"
      },
      "source": [
        "### 3.2 Handle targets\n",
        "\n",
        "In machine learning, a **target** is what we want the model to predict. For this project, the target is the disease label for each image. In this step, we prepare the target labels so that our model can learn to predict them. This may involve simplifying the labels or grouping them in a way that makes the problem easier to solve."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tS7yltl-MNbv",
      "metadata": {
        "id": "tS7yltl-MNbv"
      },
      "source": [
        "In the next step, we look at how many times each disease label appears in our data. Some diseases are very rare, which can make it hard for the model to learn about them. To keep things simple and make sure our model has enough examples to learn from, we will remove labels that appear less than 1,500 times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oxdVDqsIMcbC",
      "metadata": {
        "id": "oxdVDqsIMcbC"
      },
      "outputs": [],
      "source": [
        "# pd.set_option('display.max_rows', None)\n",
        "label_counts = metadata_df['Finding Labels'].value_counts()\n",
        "print(\"Label distributions:\")\n",
        "print(label_counts)\n",
        "# pd.reset_option('display.max_rows')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "tqIz4i7rt-LV",
      "metadata": {
        "id": "tqIz4i7rt-LV"
      },
      "source": [
        "We remove rare labels (diseases that appear in fewer than 1,500 images) so that our model has enough examples to learn from. This helps the model focus on the most common diseases and improves its ability to make accurate predictions."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YK7xNjgKb85a",
      "metadata": {
        "id": "YK7xNjgKb85a"
      },
      "source": [
        "After filtering out rare labels, we are left with the most common disease categories. The table below shows how many images belong to each label. This helps us understand the balance of our dataset and which diseases our model will learn to recognize."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eBW_rxY2uOhB",
      "metadata": {
        "id": "eBW_rxY2uOhB"
      },
      "source": [
        "First, we look at how many images there are for each disease label. This helps us see if some diseases are much more common than others, which can affect how well our model learns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22DM5KLAlaXb",
      "metadata": {
        "id": "22DM5KLAlaXb"
      },
      "outputs": [],
      "source": [
        "label_counts = metadata_df['Finding Labels'].value_counts()\n",
        "rare_labels = label_counts[label_counts < 1500].index\n",
        "rare_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fo_nf5u0uaUy",
      "metadata": {
        "id": "fo_nf5u0uaUy"
      },
      "source": [
        "Now we update our data table (DataFrame) to remove any images with rare disease labels. This makes sure our model only sees images with the most common labels, which helps it learn better."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NWjI8zgiubiV",
      "metadata": {
        "id": "NWjI8zgiubiV"
      },
      "outputs": [],
      "source": [
        "metadata_df_filtered = metadata_df[~metadata_df['Finding Labels'].isin(rare_labels)].copy()\n",
        "\n",
        "print(f\"Original shape: {metadata_df.shape}\")\n",
        "print(f\"Filtered shape: {metadata_df_filtered.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hw2tGIC7loOi",
      "metadata": {
        "id": "hw2tGIC7loOi"
      },
      "outputs": [],
      "source": [
        "metadata_df_filtered['Finding Labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "jz9itQUZj4C8",
      "metadata": {
        "id": "jz9itQUZj4C8"
      },
      "source": [
        "To make our task easier, we will turn the problem into a **binary classification** problem. This means the model will learn to answer a simple question: Is this X-ray healthy or does it show signs of disease?\n",
        "\n",
        "- **Class 0 (Negative):** Images labeled as 'No Finding' (healthy)\n",
        "- **Class 1 (Positive):** Images with any disease label (pathology present)\n",
        "\n",
        "This approach is common in deep learning when starting out, because it is easier for the model to learn to distinguish between just two categories. The category we want the model to predict is called the **target class**. Here, you can also try focusing on a specific disease (like 'Effusion') or experiment with more classes to see how the model behaves."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zNFARnXT0TOs",
      "metadata": {
        "id": "zNFARnXT0TOs"
      },
      "outputs": [],
      "source": [
        "pathology_ = \"Effusion\"  #@param ['Infiltration', 'Atelectasis', 'Effusion', 'Nodule', 'Pneumothorax', 'Mass']\n",
        "keep = {\n",
        "    'No Finding', pathology_,\n",
        "}\n",
        "\n",
        "# split each cell into a list, then keep rows where at least one element is in `keep`\n",
        "df_filtered = metadata_df_filtered[\n",
        "    metadata_df_filtered['Finding Labels']\n",
        "      .str.split('|')                         # or .str.split(',') if comma‑separated\n",
        "      .apply(lambda labels: any(lbl in keep for lbl in labels))\n",
        "].copy()\n",
        "df_filtered['Finding Labels'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MSbPWf5hvEgO",
      "metadata": {
        "id": "MSbPWf5hvEgO"
      },
      "source": [
        "We can further clean our dataset by selecting only one **view acquisition** type for our classifier. 'View acquisition' refers to the way the X-ray image was taken (for example, from the front or the side). Using only one type (like 'PA' for posteroanterior) helps the model learn more consistently, because all images will look similar in terms of orientation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28BaE1wQ0l3I",
      "metadata": {
        "id": "28BaE1wQ0l3I"
      },
      "outputs": [],
      "source": [
        "df_filtered = df_filtered[df_filtered[\"View Position\"] == 'PA']\n",
        "df_filtered['View Position'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rFWbq74kvBvB",
      "metadata": {
        "id": "rFWbq74kvBvB"
      },
      "source": [
        "Now we create a new column called `Binary Label` in our data. This column will have a value of 0 for healthy images and 1 for images with any disease. This process is called **label encoding** and is very common in deep learning, because models work best with numbers instead of text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9Bi2PeFM03hr",
      "metadata": {
        "id": "9Bi2PeFM03hr"
      },
      "outputs": [],
      "source": [
        "df_filtered['Binary Label'] = (df_filtered['Finding Labels'] != 'No Finding').astype(int)\n",
        "df_filtered['Binary Label'].value_counts()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YFZEF6EWn9rj",
      "metadata": {
        "id": "YFZEF6EWn9rj"
      },
      "source": [
        "### 3.3 Balance Classes & Prepare Subset\n",
        "\n",
        "#### Deep Dive: The Problem of Class Imbalance\n",
        "\n",
        "In real-world medical datasets, some conditions are far more common than others. When we look at the `value_counts()`, we see a long tail of rare findings. This presents a challenge:\n",
        "\n",
        "1.  **Learning Difficulty:** A model can't learn to recognize a disease from only a handful of examples. It will likely overfit to those specific images rather than learning generalizable features.\n",
        "2.  **Evaluation unreliability:** Metrics can be misleading. A model could achieve 99% accuracy by simply always predicting the majority class (e.g., \"No Finding\").\n",
        "\n",
        "To build a focused and effective model for this session, we will simplify the problem. We will first filter out very rare labels and then convert this into a **binary classification** task: predicting the presence or absence of a specific, common pathology versus a healthy state ('No Finding'). Now we balance the dataset by limiting the number of examples per class. This ensures that the model sees a similar number of positive (disease) and negative (healthy) examples during training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YSEjqoO7ieaq",
      "metadata": {
        "id": "YSEjqoO7ieaq"
      },
      "source": [
        "We use a widely used Sampling method: **Undersampling** majority class in order to speed up training, and balance our pathology class at the same time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "gzIR-sLHirmx",
      "metadata": {
        "id": "gzIR-sLHirmx"
      },
      "outputs": [],
      "source": [
        "# Separate majority and minority classes\n",
        "df_majority = df_filtered[df_filtered['Binary Label'] == 0]\n",
        "df_minority = df_filtered[df_filtered['Binary Label'] == 1]\n",
        "\n",
        "# Undersample majority class\n",
        "df_majority_undersampled = df_majority.sample(n=len(df_minority), random_state=RANDOM_SEED)\n",
        "\n",
        "# Concatenate minority class with undersampled majority class\n",
        "df_balanced = pd.concat([df_majority_undersampled, df_minority])\n",
        "\n",
        "# Shuffle the balanced dataframe\n",
        "df_filtered = df_balanced.sample(frac=1, random_state=RANDOM_SEED).reset_index(drop=True)\n",
        "\n",
        "print(f\"Balanced dataset size: {df_filtered.shape[0]}\")\n",
        "print(\"Final class distribution after undersampling:\")\n",
        "print(df_filtered['Binary Label'].value_counts())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5Bssc5DHnWkm",
      "metadata": {
        "id": "5Bssc5DHnWkm"
      },
      "source": [
        "### 3.4 Train / Test Split\n",
        "Now we use the lists of patient IDs to split our data into a **training set** (used to teach the model) and a **test set** (used to check how well the model works on new, unseen data). This is called a **train-test split** and is a key step in building reliable machine learning models.\n",
        "\n",
        "But first we extract the train_val and test set `train_val_list.txt` and `test_list.txt` from `train_val_patients` and `test_patients`, by comparing the `Image Index` from our filtere dataframe with the `patientID` list so we can use the bounding box labels that exist in the test set as quality control:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NmJ0FIShlU19",
      "metadata": {
        "id": "NmJ0FIShlU19"
      },
      "outputs": [],
      "source": [
        "# Split df_filtered based on patient IDs from the loaded lists\n",
        "train_val_df = df_filtered[df_filtered['Image Index'].isin(train_val_patients['patientId'])].copy()\n",
        "test_df = df_filtered[df_filtered['Image Index'].isin(test_patients['patientId'])].copy()\n",
        "\n",
        "print(f\"Train val shape: {train_val_df.shape}\")\n",
        "print(f\"Test set shape: {test_df.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "DR3hoNNJvmYM",
      "metadata": {
        "id": "DR3hoNNJvmYM"
      },
      "source": [
        "### 3.5 Final Train/Validation Split\n",
        "\n",
        "Now we split the balanced subset into separate training and validation sets. The training set is used to teach the model, while the validation set is used to monitor the model’s learning during training and to tune hyperparameters. We use the stratify parameter to maintain class balance in both splits, ensuring that both sets contain similar proportions of positive and negative examples. This train-validation split is essential for building a reliable model and avoiding overfitting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xWVzC4w3vUnm",
      "metadata": {
        "id": "xWVzC4w3vUnm"
      },
      "outputs": [],
      "source": [
        "train_df, val_df = train_test_split(\n",
        "    train_val_df,\n",
        "    test_size=0.2,\n",
        "    stratify=train_val_df['Binary Label'],\n",
        "    random_state=42\n",
        ")\n",
        "print(\"Train:\", train_df.shape, \"Validation:\", val_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "F9Vt4KBlxGOW",
      "metadata": {
        "id": "F9Vt4KBlxGOW"
      },
      "source": [
        "### 3.6 Deep Learning Data Terminology\n",
        "\n",
        "Before we start building datasets and data loaders in PyTorch, it is important to understand these key concepts :\n",
        "\n",
        "- **Batch:** A batch is a small group of samples processed together by the model before updating its parameters. Using batches makes training faster and more stable.\n",
        "- **Epoch:** One epoch means the model has seen all the training data once. Training usually takes many epochs.\n",
        "- **DataLoader:** In PyTorch, a DataLoader helps us load data in batches, shuffle it, and use multiple CPU cores to speed up the process. This is essential for efficient deep learning training."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "BCJnQBP1wE03",
      "metadata": {
        "id": "BCJnQBP1wE03"
      },
      "source": [
        "## 4. Pretrained Model & Prepare Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6ius3AZvwsR",
      "metadata": {
        "id": "f6ius3AZvwsR"
      },
      "source": [
        "#### Downloading Our Previously Trained Model Weights\n",
        "\n",
        "If you have time constraints/low resources, please download the already fine-tunned models from Hugging Face (DenseNet121, EfficientNet, Swin Transformer).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9bt5pdF_q4-D",
      "metadata": {
        "id": "9bt5pdF_q4-D"
      },
      "outputs": [],
      "source": [
        "# Download pretrained model weights from Hugging Face\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/densenet121-classification.pth\" --local-dir ./\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/efficientnet-classification.pth\" --local-dir ./\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/swintransformer-classification.pth\" --local-dir ./\n",
        "print(\"Model weights downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fUgU5u8OwvAF",
      "metadata": {
        "id": "fUgU5u8OwvAF"
      },
      "source": [
        "### 4.1 Define Custom Dataset in PyTorch\n",
        "Next we define our custom `ChestXrayDataset` using torch `Dataset` from `torch.utils.data`\n",
        "\n",
        "\n",
        "In PyTorch, datasets are represented as classes inheriting from torch.utils.data.Dataset. Here, we define a ChestXrayDataset class to handle image loading and preprocessing.\n",
        "\n",
        "\n",
        "This dataset class does three main things:\n",
        "\n",
        "-  Loads the X-ray images from a directory.\n",
        "\n",
        "-  Applies any preprocessing or transformations (resizing, normalization, augmentation) specified by transform.\n",
        "\n",
        "-  Returns the image and its corresponding label as a PyTorch tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jujnAUaMxBkF",
      "metadata": {
        "id": "jujnAUaMxBkF"
      },
      "outputs": [],
      "source": [
        "class ChestXrayDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['Image Index'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = torch.tensor(row['Binary Label'], dtype=torch.float32)\n",
        "        return image, label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OKhmhPbJw4L0",
      "metadata": {
        "id": "OKhmhPbJw4L0"
      },
      "source": [
        "### 4.2 Image Transforms\n",
        "\n",
        "**Transforms** are changes we make to images as we load them. This can include resizing, flipping, rotating, or normalizing the images. When we do these changes randomly during training, it is called **data augmentation**. Data augmentation helps the model learn to recognize patterns in different situations, making it more robust and less likely to memorize the training data (a problem called overfitting)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wG5zmY0Ow-8T",
      "metadata": {
        "id": "wG5zmY0Ow-8T"
      },
      "outputs": [],
      "source": [
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "image_size_= 224\n",
        "\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.Resize((image_size_,image_size_)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomRotation(10),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xM4XoBfDzQub",
      "metadata": {
        "id": "xM4XoBfDzQub"
      },
      "source": [
        "Now the `val_transforms`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "n0lqTxmPzUmL",
      "metadata": {
        "id": "n0lqTxmPzUmL"
      },
      "outputs": [],
      "source": [
        "val_transforms   = transforms.Compose([\n",
        "    transforms.Resize((image_size_,image_size_)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4xyIR_3MyqDg",
      "metadata": {
        "id": "4xyIR_3MyqDg"
      },
      "source": [
        "### 4.3 Dataloaders\n",
        "\n",
        "A **DataLoader** is a tool in PyTorch that helps us load data in small groups called **mini-batches**. Instead of giving the model one image at a time, we give it a batch of images. This makes training faster and helps the model learn more stable patterns. Dataloaders also make it easy to shuffle the data and use multiple CPU cores for loading."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e_dne34tyrOs",
      "metadata": {
        "id": "e_dne34tyrOs"
      },
      "outputs": [],
      "source": [
        "img_dir  = f'{DATA_PATH}/images'\n",
        "\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eaZ_U7XzzkGP",
      "metadata": {
        "id": "eaZ_U7XzzkGP"
      },
      "source": [
        "We pass the `Dataset` as an argument to `DataLoader`. This wraps an iterable over our dataset, and supports automatic batching, sampling, shuffling and multiprocess data loading. Here we define a batch size of 32, *i.e.* each element in the dataloader iterable will return a batch of 32 features and labels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nAy1Y-rlzmEp",
      "metadata": {
        "id": "nAy1Y-rlzmEp"
      },
      "outputs": [],
      "source": [
        "for X, y in train_loader:\n",
        "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
        "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "adTYoq-gz4Yi",
      "metadata": {
        "id": "adTYoq-gz4Yi"
      },
      "outputs": [],
      "source": [
        "IMAGENET_MEAN = np.array([0.485, 0.456, 0.406], dtype=np.float32)\n",
        "IMAGENET_STD  = np.array([0.229, 0.224, 0.225], dtype=np.float32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZR_-CnJYy-Kv",
      "metadata": {
        "id": "ZR_-CnJYy-Kv"
      },
      "outputs": [],
      "source": [
        "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WO8Sp67b0h_n",
      "metadata": {
        "id": "WO8Sp67b0h_n"
      },
      "source": [
        "## 4.4 Loading Pretrained Models & Transfer Learning\n",
        "\n",
        "A **pretrained model** is a model that has already been trained on a large dataset (like ImageNet) and has learned useful features. The structure of the model is called its **architecture** (for example, DenseNet, ResNet, EfficientNet). Using a pretrained model and adapting it to our own data is called **transfer learning**. This is very helpful because it allows us to get good results even with smaller datasets and less training time.\n",
        "In 'torchvision.models' we can find many popular pretrained models and architectures."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vEEolVerMV5E",
      "metadata": {
        "id": "vEEolVerMV5E"
      },
      "outputs": [],
      "source": [
        "import torchvision\n",
        "print(torchvision.__version__)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SPkgRRks0j5p",
      "metadata": {
        "id": "SPkgRRks0j5p"
      },
      "outputs": [],
      "source": [
        "torchvision.models.list_models()[::10]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a0cc7bbb",
      "metadata": {
        "id": "a0cc7bbb"
      },
      "source": [
        "### 4.5  Understanding Model Layers\n",
        "\n",
        "When looking at a deep learning model, you will see several types of layers. Here is what to look for in each:\n",
        "\n",
        "- **Convolutional layers:** These are the building blocks of most image models. They scan the input image with small filters (sliding windows) to detect patterns like edges, shapes, or textures. The first convolutional layer takes the raw image (with 1 channel for grayscale or 3 for RGB) and produces feature maps.\n",
        "- **Normalization layers (BatchNorm):** These layers help the model train faster and more reliably by keeping the outputs of previous layers at a similar scale. Batch Normalization (BatchNorm) is the most common type. It makes training more stable and helps the model generalize better.\n",
        "- **Pooling layers:** Pooling reduces the size of the feature maps, making the model faster and helping it focus on the most important features. The most common is Max Pooling, which keeps only the largest value in each region.\n",
        "- **Activation functions:** After each convolution, the model uses an activation function (like ReLU) to introduce non-linearity. This helps the model learn complex patterns, not just straight lines.\n",
        "\n",
        "- **First layer:** This is usually a convolutional layer that takes the input image. Check its input dimension (number of channels, usually 1 for grayscale or 3 for RGB images).\n",
        "- **Second layer:** Often another convolutional, normalization, activation, or pooling layer, building on the features from the first.\n",
        "- **Second to last layer:** This is typically a feature layer just before the classifier. Its output dimension shows the number of features passed to the final classifier.\n",
        "- **Last layer:** This is the classifier or output layer. Its output dimension should match the number of classes (1 for binary classification).\n",
        "\n",
        "By examining these layers, you can understand how the model processes the input and what features are used for the final prediction."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E-Pz2iQT7FXr",
      "metadata": {
        "id": "E-Pz2iQT7FXr"
      },
      "source": [
        "### 4.6. Transfer Learning\n",
        "\n",
        "Training a deep neural network from scratch requires an enormous amount of data and computational power. Instead, we will use a powerful technique called **transfer learning**.\n",
        "\n",
        "The concept is simple: we take a model that has already been trained on a massive dataset (like ImageNet, which contains over a million everyday photographs) and adapt it for our specific task. While chest X-rays are different from photos of cats and cars, the pre-trained model has already learned to recognize fundamental visual patterns like edges, textures, shapes, and gradients.\n",
        "\n",
        "By using its pre-trained \"visual knowledge\" as a starting point, we only need to fine-tune the final layers for our specific task of X-ray classification. This approach offers several key advantages:\n",
        "- **Faster Training:** The model converges much more quickly.\n",
        "- **Better Performance:** It often leads to higher accuracy, especially with smaller medical datasets.\n",
        "- **Reduced Data Needs:** We can achieve strong results without needing millions of labeled images.\n",
        "\n",
        "We will start from pretrained models for the following architectures: DenseNet121, EfficientNet, Swin Transformer.\n",
        "\n",
        "These weights are downloaded from Hugging Face and will be used for transfer learning.\n",
        "\n",
        "**Why do we use pretrained models?**\n",
        "\n",
        "  - Training a deep network from scratch would require lots of GPUs/time before we can achieve a high performance.\n",
        "\n",
        "  - Instead, we use models that have already been pretrained on ImageNet (1M+ images). We then fine-tune them on our medical dataset (transfer learning). This also makes training faster and often improves performance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6QMilt66Efv_",
      "metadata": {
        "id": "6QMilt66Efv_"
      },
      "source": [
        "### DenseNet121\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/densenet.png\" width=\"600\">\n",
        "\n",
        "Huang, Gao, et al. \"Densely connected convolutional networks.\" Proceedings of the IEEE conference on computer vision and pattern recognition. 2017.\n",
        "\n",
        "- Dense connections between layers → improves gradient flow.\n",
        "\n",
        "- Popular for medical imaging tasks (used in the NIH ChestX-ray paper itself).\n",
        "\n",
        "- We load DenseNet121 pretrained on ImageNet.\n",
        "\n",
        "- Replace the classifier with a single output unit for binary classification.\n",
        "\n",
        "- Printing first, second, second-to-last, and last layers gives insight into the model structure."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UzO-t4Fx0nXV",
      "metadata": {
        "id": "UzO-t4Fx0nXV"
      },
      "outputs": [],
      "source": [
        "model = models.densenet121(pretrained=True)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
        "model = model.to(device)\n",
        "\n",
        "# Print only the first and last two layer blocks\n",
        "layers = list(model.children())\n",
        "print('First layer block:')\n",
        "print(layers[0])\n",
        "print('\\n---')\n",
        "print('Second layer block:')\n",
        "print(layers[1])\n",
        "print('\\n...')\n",
        "print('Second to last layer block:')\n",
        "print(layers[-2])\n",
        "print('\\n---')\n",
        "print('Last layer block:')\n",
        "print(layers[-1])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lTlY4zP295rs",
      "metadata": {
        "id": "lTlY4zP295rs"
      },
      "source": [
        "## 5. Train the Model (10 minutes)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "qaEdGcOx0y9Q",
      "metadata": {
        "id": "qaEdGcOx0y9Q"
      },
      "source": [
        "### 5.1 Hyperparameters\n",
        "\n",
        "**Hyperparameters** are settings that you choose before training your model. They control how the learning process works. Common hyperparameters include:\n",
        "- **Number of epochs:** How many times the model sees the whole training set.\n",
        "- **Batch size:** How many samples are in each batch.\n",
        "- **Learning rate:** How big the steps are when updating the model's weights.\n",
        "\n",
        "Tuning hyperparameters is important because it can make a big difference in how well your model learns."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "CP3Y2XJR0znF",
      "metadata": {
        "id": "CP3Y2XJR0znF"
      },
      "outputs": [],
      "source": [
        "learning_rate = 1e-4\n",
        "batch_size = 64\n",
        "epochs = 20"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UTXB_k_Y08_f",
      "metadata": {
        "id": "UTXB_k_Y08_f"
      },
      "source": [
        "### 5.2 Optimization loop\n",
        "\n",
        "Training a deep learning model involves an **optimization loop**. Each time the model sees the whole training set, it completes one **epoch**. The process has two main parts:\n",
        "- **Train loop:** The model learns from the training data and updates its parameters.\n",
        "- **Validation loop:** The model is tested on validation data to see how well it is learning.\n",
        "\n",
        "A **loss function** measures how far the model's predictions are from the true answers. The goal of training is to minimize this loss. The optimization loop repeats for many epochs until the model performs well."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "AFH4boS51SCH",
      "metadata": {
        "id": "AFH4boS51SCH"
      },
      "source": [
        "Inside the training loop, the model learns by adjusting its parameters using **gradients**. Gradients show how much each parameter should change to reduce the loss. The process of calculating gradients and updating parameters is called **backpropagation**.\n",
        "\n",
        "- **optimizer.zero_grad():** Resets the gradients to zero before each batch.\n",
        "- **loss.backward():** Calculates the gradients using backpropagation.\n",
        "- **optimizer.step():** Updates the model's parameters using the gradients.\n",
        "- **Learning rate scheduler (like OneCycleLR):** Adjusts the learning rate during training to help the model learn better and faster."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UEz2Amsl09dM",
      "metadata": {
        "id": "UEz2Amsl09dM"
      },
      "outputs": [],
      "source": [
        "# Re-initialize the model after cleanup\n",
        "model = models.densenet121(pretrained=True)\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 1)\n",
        "model = model.to(device)\n",
        "\n",
        "\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = OneCycleLR(optimizer, max_lr=learning_rate, epochs=epochs, steps_per_epoch=len(train_loader), total_steps=epochs * len(train_loader))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0PlztayZ1JN7",
      "metadata": {
        "id": "0PlztayZ1JN7"
      },
      "outputs": [],
      "source": [
        "pos_frac = train_df['Binary Label'].mean()\n",
        "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
        "\n",
        "criterion  = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Rlu-3UTk4j87",
      "metadata": {
        "id": "Rlu-3UTk4j87"
      },
      "source": [
        "### 5.3 Define Training & Validation Functions\n",
        "\n",
        "Next, we define our **training function** and **validation function**. The training function teaches the model using the training data, while the validation function checks how well the model is doing on data it hasn't seen before. Keeping these functions separate helps us monitor the model's progress and avoid overfitting (when the model memorizes the training data but doesn't generalize well to new data)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6KZltOsh5EqE",
      "metadata": {
        "id": "6KZltOsh5EqE"
      },
      "outputs": [],
      "source": [
        "def train_loop(model, loader, criterion, optimizer, scheduler, device):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for imgs, labels in tqdm(loader, desc=\"  Training\", leave=False):\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        logits = model(imgs).squeeze(1)\n",
        "        loss = criterion(logits, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        try:\n",
        "            scheduler.step()\n",
        "        except ValueError:\n",
        "            pass\n",
        "\n",
        "        running_loss += loss.item() * imgs.size(0)\n",
        "\n",
        "    avg_loss = running_loss / len(loader.dataset)\n",
        "    return avg_loss\n",
        "\n",
        "\n",
        "def val_loop(model, loader, criterion, auroc, device):\n",
        "    model.eval()\n",
        "    auroc.reset()\n",
        "    running_preds = []\n",
        "    running_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader, desc=\"  Validation\", leave=False):\n",
        "            imgs = imgs.to(device)\n",
        "            logits = model(imgs).squeeze(1)\n",
        "            probs = torch.sigmoid(logits)\n",
        "\n",
        "            preds = (probs > 0.5).int().cpu().numpy()\n",
        "            running_preds.extend(preds.tolist())\n",
        "            running_labels.extend(labels.int().tolist())\n",
        "\n",
        "            auroc.update(probs, labels.int().to(device))\n",
        "\n",
        "    acc = accuracy_score(running_labels, running_preds)\n",
        "    val_auroc = auroc.compute().item()\n",
        "    return acc, val_auroc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Gkc2GXQ0Av_0",
      "metadata": {
        "id": "Gkc2GXQ0Av_0"
      },
      "source": [
        "### 5.4 Load Dataset\n",
        "\n",
        "Define the train and validation datasets and dataloaders."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J38g7NXJ4kQq",
      "metadata": {
        "id": "J38g7NXJ4kQq"
      },
      "outputs": [],
      "source": [
        "img_dir  = f'{DATA_PATH}/images'\n",
        "\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=90, pin_memory=True)\n",
        "\n",
        "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
        "val_loader   = DataLoader(val_ds,   batch_size=32, num_workers=32, pin_memory=True)\n",
        "\n",
        "auroc = BinaryAUROC().to(device)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P7eHpm6yA7tL",
      "metadata": {
        "id": "P7eHpm6yA7tL"
      },
      "source": [
        "### 5.5 Train the DenseNet121 Model\n",
        "\n",
        "We now start the actual training loop for DenseNet121."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SPGrRs1PA3CN",
      "metadata": {
        "id": "SPGrRs1PA3CN"
      },
      "outputs": [],
      "source": [
        "best_val_auroc = 0.0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")\n",
        "\n",
        "    # Save model on best validation AUROC\n",
        "    if val_auroc > best_val_auroc:\n",
        "        best_val_auroc = val_auroc\n",
        "        torch.save(model.state_dict(), f'{DATA_PATH}/densenet121-classification.pth')\n",
        "        print(f\"  Saved model with best AUROC: {best_val_auroc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1qjY1rqFpcgQ",
      "metadata": {
        "cellView": "form",
        "id": "1qjY1rqFpcgQ"
      },
      "outputs": [],
      "source": [
        "#@title Free up RAM and GPU memory\n",
        "\n",
        "del model   # delete model object\n",
        "del optimizer\n",
        "del scheduler\n",
        "torch.cuda.empty_cache()  # clears unused cached memory\n",
        "gc.collect()  # run garbage collector"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "G2gzcB-Z5_Xi",
      "metadata": {
        "id": "G2gzcB-Z5_Xi"
      },
      "source": [
        "## 6. Benchmarking model architectures (20 minutes)\n",
        "\n",
        "A **CNN architecture** is the specific design or structure of a convolutional neural network. Different architectures (like ResNet, DenseNet, EfficientNet, Swin Transformer) use different building blocks:\n",
        "- **Skip connections:** Allow information to skip layers, helping very deep networks learn better (used in ResNet).\n",
        "- **Dense connections:** Connect each layer to every other layer in a block, improving information flow (used in DenseNet).\n",
        "- **Normalization layers:** Help stabilize and speed up training by keeping the data flowing through the network at a similar scale.\n",
        "\n",
        "Trying different architectures is important because some may work better for your specific problem. In this section, you will train and compare several architectures to see which performs best on your data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "LPuX8sCi6FvW",
      "metadata": {
        "id": "LPuX8sCi6FvW"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q1.</b> In deep learning, different model architectures can have a big impact on performance. Complete the following cells to train and compare these models:\n",
        "    - EfficientNet\n",
        "    - Swin Transformer\n",
        "</div>\n",
        "\n",
        "Comparing different models helps you understand which design works best for your specific task and data."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "E_0Pwmcd97n8",
      "metadata": {
        "id": "E_0Pwmcd97n8"
      },
      "source": [
        "### EfficientNet\n",
        "\n",
        "- Balances model depth, width, and resolution efficiently.\n",
        "\n",
        "- Often achieves better accuracy with fewer parameters.\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/efficientnet.png\" width=\"600\">\n",
        "\n",
        "Tan, M., & Le, Q. (2019, May). Efficientnet: Rethinking model scaling for convolutional neural networks. In International conference on machine learning (pp. 6105-6114). PMLR."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yOEjyXbo-Dty",
      "metadata": {
        "id": "yOEjyXbo-Dty"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load pretrained EfficientNet B0 with default weights\n",
        "weights = EfficientNet_B0_Weights.DEFAULT\n",
        "model_enb0 = efficientnet_b0(weights=weights)\n",
        "\n",
        "# Modify the classifier to output a single value for binary classification\n",
        "model_enb0.classifier[1] = nn.Linear(in_features=1280, out_features=1)\n",
        "\n",
        "# Move model to device (GPU or CPU)\n",
        "model = model_enb0.to(device)\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = 1e-3\n",
        "batch_size = 64\n",
        "epochs = 20\n",
        "\n",
        "# Compute positive class weight for imbalanced dataset\n",
        "pos_frac = train_df['Binary Label'].mean()\n",
        "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
        "\n",
        "# Loss function\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Optimizer and learning rate scheduler\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=learning_rate,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=len(train_loader),\n",
        "    total_steps=epochs * len(train_loader)\n",
        ")\n",
        "\n",
        "# Dataset and DataLoader\n",
        "img_dir  = f'{DATA_PATH}/images'\n",
        "\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True, num_workers=8, pin_memory=True)\n",
        "\n",
        "val_ds = ChestXrayDataset(val_df, img_dir, transform=val_transforms)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, num_workers=4, pin_memory=True)\n",
        "\n",
        "# AUROC metric\n",
        "auroc = BinaryAUROC().to(device)\n",
        "\n",
        "best_val_auroc = 0.0\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")\n",
        "\n",
        "    # Save model on best validation AUROC\n",
        "    if val_auroc > best_val_auroc:\n",
        "        best_val_auroc = val_auroc\n",
        "        torch.save(model.state_dict(), f'{DATA_PATH}/efficientnet-classification.pth')\n",
        "        print(f\"  Saved model with best AUROC: {best_val_auroc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "P2PotfWt9_ef",
      "metadata": {
        "id": "P2PotfWt9_ef"
      },
      "source": [
        "### SwinTransformer\n",
        "\n",
        "- A Vision Transformer (ViT) variant.\n",
        "\n",
        "- Uses shifted windows for efficient self-attention.\n",
        "\n",
        "- Very strong performance on classification & detection.\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/swintransformer.png\" width=\"500\">\n",
        "\n",
        "Liu, Z., Lin, Y., Cao, Y., Hu, H., Wei, Y., Zhang, Z., ... & Guo, B. (2021). Swin transformer: Hierarchical vision transformer using shifted windows. In Proceedings of the IEEE/CVF international conference on computer vision (pp. 10012-10022)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UD-eEjS8_PWr",
      "metadata": {
        "id": "UD-eEjS8_PWr"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import swin_t, Swin_T_Weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Load pretrained Swin Transformer\n",
        "model_swin = ... # Complete\n",
        "model_swin.head = nn.Linear(... # Complete)\n",
        "\n",
        "# Send model to device\n",
        "model = ... # Complete\n",
        "\n",
        "# Hyperparameters\n",
        "learning_rate = ... # Complete\n",
        "batch_size = ... # Complete\n",
        "epochs = ... # Complete\n",
        "\n",
        "# Loss function with class imbalance adjustment\n",
        "pos_frac = train_df['Binary Label'].mean()\n",
        "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = ... # Complete\n",
        "scheduler = OneCycleLR(\n",
        "    ... # Complete\n",
        ")\n",
        "\n",
        "# Dataset & DataLoader\n",
        "img_dir = f\"{DATA_PATH}/images\"\n",
        "\n",
        "train_ds = ChestXrayDataset(... # Complete)\n",
        "train_loader = DataLoader(... # Complete)\n",
        "\n",
        "val_ds = ChestXrayDataset(... # Complete)\n",
        "val_loader = DataLoader(... # Complete)\n",
        "\n",
        "# Metric\n",
        "auroc = BinaryAUROC().to(device)\n",
        "\n",
        "best_val_auroc = 0.0\n",
        "# Training Loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")\n",
        "\n",
        "    # Save model on best validation AUROC\n",
        "    if val_auroc > best_val_auroc:\n",
        "        best_val_auroc = val_auroc\n",
        "        torch.save(model.state_dict(), f'{DATA_PATH}/swintransformer-classification.pth')\n",
        "        print(f\"  Saved model with best AUROC: {best_val_auroc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MStBLzpEBI01",
      "metadata": {
        "id": "MStBLzpEBI01"
      },
      "source": [
        "The Swin Transformer initially learned useful features (Val Acc ~0.79, AUROC ~0.87) but quickly collapsed. From epoch 3 onward, AUROC dropped to ~0.5 while accuracy reflected the majority class, indicating the model was no longer distinguishing classes. This behavior is caused by a high learning rate destabilizing pretrained weights and class imbalance in the dataset. AUROC is the better metric here, showing the model is effectively guessing randomly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XFsgDrLs4BVr",
      "metadata": {
        "id": "XFsgDrLs4BVr"
      },
      "outputs": [],
      "source": [
        "print(train_df['Binary Label'].value_counts(normalize=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2CMWsg3LCHq9",
      "metadata": {
        "id": "2CMWsg3LCHq9"
      },
      "source": [
        "This shows a class imbalance in the dataset, with the majority class (0) being about twice the size of the minority class (1).\n",
        "\n",
        "We fix this by:\n",
        "\n",
        "- Using a WeightedRandomSampler to create balanced batches during training.\n",
        "\n",
        "- Applying a class-weighted loss (BCEWithLogitsLoss with pos_weight) to give more importance to the minority class.\n",
        "\n",
        "- Adjusting learning rate and batch size for more stable training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3EOlmREr4rAr",
      "metadata": {
        "id": "3EOlmREr4rAr"
      },
      "outputs": [],
      "source": [
        "from torchvision.models import swin_t, Swin_T_Weights\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "from torch.utils.data import DataLoader, WeightedRandomSampler\n",
        "import numpy as np\n",
        "\n",
        "# Load pretrained Swin Transformer\n",
        "model_swin = swin_t(... # Complete)\n",
        "model_swin.head = nn.Linear(... # Complete)\n",
        "\n",
        "# Send model to device\n",
        "model = ... # Complete\n",
        "\n",
        "# Hyperparameters (tuned for stability)\n",
        "learning_rate = ... # Complete  # smaller LR for Swin\n",
        "batch_size = ... # Complete        # smaller batch for better gradient updates\n",
        "epochs = ... # Complete\n",
        "\n",
        "# Class imbalance adjustment\n",
        "pos_frac = train_df['Binary Label'].mean()\n",
        "pos_weight = torch.tensor([(1 - pos_frac) / pos_frac]).to(device)\n",
        "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
        "\n",
        "# Optimizer & Scheduler\n",
        "optimizer = ... # Complete\n",
        "\n",
        "# Dataset\n",
        "img_dir = f\"{DATA_PATH}/images\"\n",
        "train_ds = ChestXrayDataset(train_df, img_dir, transform=train_transforms)\n",
        "val_ds   = ChestXrayDataset(val_df,   img_dir, transform=val_transforms)\n",
        "\n",
        "# WeightedRandomSampler for balanced batches\n",
        "class_counts = train_df['Binary Label'].value_counts().to_dict()\n",
        "weights = [1.0 / class_counts[label] for label in train_df['Binary Label']]\n",
        "sampler = WeightedRandomSampler(... # Complete)\n",
        "\n",
        "# DataLoaders\n",
        "train_loader = DataLoader(... # Complete)\n",
        "val_loader   = DataLoader(... # Complete)\n",
        "\n",
        "# Scheduler (now safe to define since train_loader exists)\n",
        "scheduler = OneCycleLR(\n",
        "    optimizer,\n",
        "    max_lr=learning_rate,\n",
        "    epochs=epochs,\n",
        "    steps_per_epoch=len(train_loader)\n",
        ")\n",
        "\n",
        "# Metric\n",
        "auroc = BinaryAUROC().to(device)\n",
        "best_val_auroc = 0.0\n",
        "\n",
        "# Training Loop\n",
        "for epoch in range(1, epochs + 1):\n",
        "    print(f\"Epoch {epoch}/{epochs}\")\n",
        "\n",
        "    train_loss = train_loop(model, train_loader, criterion, optimizer, scheduler, device)\n",
        "    val_acc, val_auroc = val_loop(model, val_loader, criterion, auroc, device)\n",
        "\n",
        "    print(f\"  Train Loss: {train_loss:.4f}  |  Val Acc: {val_acc:.4f}  |  Val AUROC: {val_auroc:.4f}\")\n",
        "\n",
        "    # Save model on best validation AUROC\n",
        "    if val_auroc > best_val_auroc:\n",
        "        best_val_auroc = val_auroc\n",
        "        torch.save(model.state_dict(), f'{DATA_PATH}/swintransformer-classification.pth')\n",
        "        print(f\"  Saved model with best AUROC: {best_val_auroc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "VxdaYvqLCTIN",
      "metadata": {
        "id": "VxdaYvqLCTIN"
      },
      "source": [
        "After these changes, the model achieves a high Val AUROC (~0.91), showing it can discriminate between classes effectively despite the imbalance."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cSPhS2LtDKv_",
      "metadata": {
        "id": "cSPhS2LtDKv_"
      },
      "source": [
        "### Model Benchmarking Summary\n",
        "\n",
        "We trained three different architectures on our chest X-ray dataset: **DenseNet121**, **EfficientNet-B0**, and **Swin Transformer**. Key results:\n",
        "\n",
        "| Model            | Train Loss (final) | Val Acc (final) | Val AUROC (final) |\n",
        "|-----------------|-----------------|----------------|------------------|\n",
        "| DenseNet121      | 0.338           | 0.835          | 0.906           |\n",
        "| EfficientNet-B0  | 0.327           | 0.838          | 0.906           |\n",
        "| Swin Transformer | 0.318           | 0.842          | 0.907           |\n",
        "\n",
        "**Observations:**\n",
        "- DenseNet121 has highest validation accuracy.\n",
        "- EfficientNet has the same Val AUCRoc than DensNet121.\n",
        "- Swin Transformer captures global features but requires careful tuning; its training loss still has a lot of room for improvement.\n",
        "\n",
        "After completing the experiments, summarize the results:\n",
        "\n",
        "- Which loss function achieved the highest validation performance (e.g., best Accuracy, AUROC) on the triage classification task?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2iEXO2Z46XIu",
      "metadata": {
        "id": "2iEXO2Z46XIu"
      },
      "source": [
        "## 7. Final Evaluation\n",
        "\n",
        "We have trained our models (DenseNet121, EfficientNet-B0, Swin Transformer) and used the validation set to guide hyperparameter tuning. Now, we evaluate them on the held-out test set, which the models have never seen before. This gives the most honest estimate of performance on new, unseen patients — the final verdict on model capability."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-dMPkiNwCNCW",
      "metadata": {
        "id": "-dMPkiNwCNCW"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "# Define models\n",
        "# DenseNet121\n",
        "model_densenet = models.densenet121(pretrained=False)\n",
        "num_ftrs = model_densenet.classifier.in_features\n",
        "model_densenet.classifier = nn.Linear(num_ftrs, 1)\n",
        "model_densenet = model_densenet.to(device)\n",
        "\n",
        "# EfficientNet-B0\n",
        "model_enb0 = models.efficientnet_b0(pretrained=False)\n",
        "num_ftrs = model_enb0.classifier[1].in_features\n",
        "model_enb0.classifier[1] = nn.Linear(num_ftrs, 1)\n",
        "model_enb0 = model_enb0.to(device)\n",
        "\n",
        "# Swin Transformer\n",
        "model_swin = models.swin_t(pretrained=False)\n",
        "num_ftrs = model_swin.head.in_features\n",
        "model_swin.head = nn.Linear(num_ftrs, 1)\n",
        "model_swin = model_swin.to(device)\n",
        "\n",
        "# Load checkpoints safely\n",
        "def load_weights_safely(model, checkpoint_path):\n",
        "    # Map location to CPU to avoid CUDA device issues\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    state_dict = checkpoint if isinstance(checkpoint, dict) else checkpoint.state_dict()\n",
        "\n",
        "    # Filter out classifier weights if they don’t match\n",
        "    model_dict = model.state_dict()\n",
        "    filtered_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
        "\n",
        "    # Update and load\n",
        "    model_dict.update(filtered_dict)\n",
        "    model.load_state_dict(model_dict, strict=False)\n",
        "    print(f\"Loaded weights from {checkpoint_path} (ignored mismatched layers)\")\n",
        "\n",
        "# Apply loading\n",
        "load_weights_safely(model_densenet, f'{DATA_PATH}/densenet121-classification.pth')\n",
        "load_weights_safely(model_enb0, f'{DATA_PATH}/efficientnet-classification.pth')\n",
        "load_weights_safely(model_swin, f'{DATA_PATH}/swintransformer-classification.pth')\n",
        "\n",
        "print(\"All models loaded and ready!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "gpn4ZousMcIZ",
      "metadata": {
        "id": "gpn4ZousMcIZ"
      },
      "source": [
        "### Metrics\n",
        "\n",
        "After training, we need to measure how well our models perform. In deep learning, we use different **metrics** to evaluate models:\n",
        "\n",
        "- **Accuracy:** The percentage of correct predictions. Gives a quick overview but can be misleading with imbalanced classes.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/accuracy.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dIXZ88zgOk7i",
      "metadata": {
        "id": "dIXZ88zgOk7i"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "true_labels = [0, 1, 1, 0, 1, 0]\n",
        "predicted_labels = [0, 1, 1, 0, 1, 1]\n",
        "\n",
        "accuracy = accuracy_score(true_labels, predicted_labels)\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CotC1N41MhEj",
      "metadata": {
        "id": "CotC1N41MhEj"
      },
      "source": [
        "- **AUROC curve:** Measures the model’s ability to discriminate positive vs. negative classes; robust to class imbalance. The ROC Curve is a figure that shows how well the model separates healthy from diseased images at different thresholds.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/rocauc.png\" width=\"600\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nYDX_5RcOtg9",
      "metadata": {
        "id": "nYDX_5RcOtg9"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import roc_auc_score, roc_curve\n",
        "\n",
        "true_labels = np.array([0, 0, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1])\n",
        "predicted_probabilities = np.array([0.4, 0.3, 0.7, 0.3, 0.4, 0.4, 0.2, 0.45, 0.35, 0.15, 0.55, 0.85, 0.25, 0.85, 0.45, 0.7, 0.95, 0.95, 0.8, 0.8])\n",
        "\n",
        "# Calculate ROC AUC\n",
        "roc_auc = roc_auc_score(true_labels, predicted_probabilities)\n",
        "print(f\"ROC AUC: {roc_auc:.4f}\")\n",
        "\n",
        "# Plot ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(true_labels, predicted_probabilities)\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(fpr, tpr, label=f'ROC curve (area = {roc_auc:.2f})')\n",
        "plt.plot([0, 1], [0, 1], 'k--')\n",
        "plt.xlim([0.0, 1.0])\n",
        "plt.ylim([0.0, 1.05])\n",
        "plt.xlabel('False Positive Rate')\n",
        "plt.ylabel('True Positive Rate')\n",
        "plt.title('Receiver Operating Characteristic')\n",
        "plt.legend(loc=\"lower right\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "87uXm0AJMeCd",
      "metadata": {
        "id": "87uXm0AJMeCd"
      },
      "source": [
        "- **Precision:** Answers the question: \"Of all the pixels the model labeled as tumor, what fraction were actually tumor?\" High precision means the model makes few false positive errors. Clinically, this translates to not raising false alarms or suggesting unnecessary biopsies.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/precision.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Pl1yrodnO3e4",
      "metadata": {
        "id": "Pl1yrodnO3e4"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import precision_score\n",
        "\n",
        "true_labels = [0, 1, 1, 0, 1, 0]\n",
        "predicted_labels = [0, 1, 1, 0, 1, 1]\n",
        "\n",
        "precision = precision_score(true_labels, predicted_labels)\n",
        "print(f\"Precision: {precision:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "pSPS0NpYMfLr",
      "metadata": {
        "id": "pSPS0NpYMfLr"
      },
      "source": [
        "\n",
        "- **Recall:** Answers the question: \"Of all the pixels that were actually tumor, what fraction did the model correctly identify?\" High recall means the model makes few false negative errors. This is often critically important in medicine, as it relates to not missing a diagnosis.\n",
        "\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/recall.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PiilUtSCPQt-",
      "metadata": {
        "id": "PiilUtSCPQt-"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import recall_score\n",
        "\n",
        "true_labels = [0, 1, 1, 0, 1, 0]\n",
        "predicted_labels = [0, 1, 1, 0, 1, 1]\n",
        "\n",
        "recall = recall_score(true_labels, predicted_labels)\n",
        "print(f\"Sensitivity/recall: {recall:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-nw6QmtrMgNu",
      "metadata": {
        "id": "-nw6QmtrMgNu"
      },
      "source": [
        "- **F1-score:** Answers the question: “How well does the model balance precision and recall?” It is the harmonic mean of the two, ensuring that a model must perform well on both dimensions rather than excelling in only one.\n",
        "<img src=\"https://raw.githubusercontent.com/albarqounilab/BILD-Summer-School/refs/heads/main/images/f1score.png\" width=\"600\">"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1GLS4luvPXxl",
      "metadata": {
        "id": "1GLS4luvPXxl"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import f1_score\n",
        "\n",
        "true_labels = [0, 1, 1, 0, 1, 0]\n",
        "predicted_labels = [0, 1, 1, 0, 1, 1]\n",
        "\n",
        "f1score = f1_score(true_labels, predicted_labels)\n",
        "print(f\"F1-score: {f1score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wNVpLAryCLmx",
      "metadata": {
        "id": "wNVpLAryCLmx"
      },
      "source": [
        "Using multiple metrics gives a more complete picture of model performance, especially when the data is imbalanced."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "k_5aseiPOahl",
      "metadata": {
        "id": "k_5aseiPOahl"
      },
      "source": [
        "\n",
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q2.</b> Compare the performance of the four models using appropriate metrics:</div>\n",
        "\n",
        "- Accuracy score\n",
        "- Precision and Recall\n",
        "- F1-score\n",
        "- Plot the ROC curve\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "KlksQS_sV1iF",
      "metadata": {
        "id": "KlksQS_sV1iF"
      },
      "outputs": [],
      "source": [
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "\n",
        "def eval_models(model_list, model_names, loader, device=\"cuda\", threshold=0.5, plot_roc=True):\n",
        "    def _to_pos_prob(logits):\n",
        "        if logits.ndim == 2 and logits.shape[1] == 2:\n",
        "            return torch.softmax(logits, dim=1)[:, 1]\n",
        "        if logits.ndim == 1 or (logits.ndim == 2 and logits.shape[1] == 1):\n",
        "            return torch.sigmoid(logits.view(-1))\n",
        "        raise ValueError(f\"Only binary/2-class outputs supported, got shape {tuple(logits.shape)}\")\n",
        "\n",
        "    results, all_probs, all_labels = {}, {}, None\n",
        "    if plot_roc: plt.figure(figsize=(8,6))\n",
        "\n",
        "    for model, name in zip(model_list, model_names):\n",
        "        model.eval().to(device)\n",
        "        probs_list, labels_list = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X = X.to(device)\n",
        "                p = _to_pos_prob(model(X))\n",
        "                probs_list.extend(p.detach().cpu().numpy())\n",
        "                labels_list.extend(y.numpy())\n",
        "\n",
        "        y_true = np.array(labels_list)\n",
        "        y_prob = np.array(probs_list)\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "\n",
        "        if all_labels is None: all_labels = y_true\n",
        "\n",
        "        acc  = ... # COMPLETE\n",
        "        prec = ... # COMPLETE\n",
        "        rec  = ... # COMPLETE\n",
        "        f1   = ... # COMPLETE\n",
        "        auc  = ... # COMPLETE\n",
        "\n",
        "        print(f\"{name}: acc={acc:.4f}  prec={prec:.4f}  rec={rec:.4f}  f1={f1:.4f}  auc={auc:.4f}\")\n",
        "\n",
        "        if plot_roc:\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "            plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.2f})\")\n",
        "\n",
        "        results[name] = {\"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1, \"roc_auc\":auc}\n",
        "        all_probs[name] = y_prob\n",
        "\n",
        "    if plot_roc:\n",
        "        plt.plot([0,1],[0,1],\"k--\"); plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(\"ROC Curve Comparison\"); plt.grid(True); plt.legend(); plt.show()\n",
        "\n",
        "    return {\"labels\": all_labels, \"probs\": all_probs, \"metrics\": results}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "vAWE5iIGVapF",
      "metadata": {
        "id": "vAWE5iIGVapF"
      },
      "outputs": [],
      "source": [
        "# Test dataset & loader\n",
        "test_ds = ChestXrayDataset(test_df, img_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model_list  = [model_densenet, model_enb0, model_swin]\n",
        "model_names = ['DenseNet 121', 'EfficientNet B0', 'Swin Transformer']\n",
        "\n",
        "out = eval_models(model_list, model_names, test_loader, device=device, threshold=0.5, plot_roc=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "RxBv6NIEV-gw",
      "metadata": {
        "id": "RxBv6NIEV-gw"
      },
      "outputs": [],
      "source": [
        "out[\"metrics\"] # -> per-model metrics dict\n",
        "# out[\"probs\"][model_name] -> probability vector\n",
        "# out[\"labels\"] -> ground-truth vector (shared)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K8V5BczZHycx",
      "metadata": {
        "id": "K8V5BczZHycx"
      },
      "source": [
        "- Which model gives the best performance and should be selected?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Qo0Z3a0Fwc9i",
      "metadata": {
        "id": "Qo0Z3a0Fwc9i"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "0WXKgwPGtjkY",
      "metadata": {
        "id": "0WXKgwPGtjkY"
      },
      "source": [
        "## 8. Advanced Quality Control I: Fairness Audit\n",
        "\n",
        "A model that is highly accurate on average can still be dangerously unreliable if its performance is not uniform across different patient groups. An AI tool that works well for one demographic but fails for another is not only unfair—it's clinically unsafe. This is a critical concern for regulatory bodies like the FDA and a cornerstone of building responsible AI.\n",
        "\n",
        "In this crucial section, we will conduct a **fairness audit** to investigate whether our best-performing model exhibits any bias related to patient demographics. We will:\n",
        "\n",
        "1.  **Define Protected Attributes**: Identify key demographic variables like `Patient Sex` and `Patient Age` that a fair model should not discriminate based upon.\n",
        "2.  **Get All Model Predictions**: Efficiently run our best model on the entire test set to get a single results table.\n",
        "3.  **Analyze Performance Across Subgroups**: Calculate key metrics separately for each group (e.g., Male vs. Female, `<50` vs. `50+`) to spot disparities.\n",
        "4.  **Quantify Bias with Fairness Metrics**: Introduce and compute formal fairness metrics like **Equal Opportunity Difference (EOD)** and **Average Odds Difference (AOD)** to put a number on any observed performance gaps.\n",
        "\n",
        "This analysis moves us beyond a single performance number and toward a deeper understanding of our model's real-world behavior.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mmyJ_yUA-GQ4",
      "metadata": {
        "id": "mmyJ_yUA-GQ4"
      },
      "source": [
        "Reimport libraries (if needed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DvTpgFA7-F6o",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DvTpgFA7-F6o",
        "outputId": "5afea25b-2a48-47ee-ec34-3478453da671"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Setup complete. Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "#@title re-import libraries\n",
        "# Library Installations\n",
        "!pip install pydicom SimpleITK albumentations torchmetrics grad-cam -q\n",
        "\n",
        "# Core Library Imports\n",
        "import os\n",
        "import sys, math, random, time, warnings\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "import gc\n",
        "\n",
        "# Data Handling and Processing\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import cv2\n",
        "import pydicom\n",
        "from PIL import Image\n",
        "\n",
        "# Deep Learning with PyTorch & Torchvision\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
        "import torchvision\n",
        "from torchvision import transforms, models\n",
        "from torch.optim.lr_scheduler import OneCycleLR\n",
        "\n",
        "# Visualization & Metrics\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from tqdm import tqdm\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, roc_auc_score, RocCurveDisplay\n",
        "from torchmetrics.classification import BinaryAUROC\n",
        "\n",
        "# Explainability (XAI)\n",
        "from pytorch_grad_cam import GradCAM, ScoreCAM, GradCAMPlusPlus\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "\n",
        "# Environment Configuration\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# Ensure reproducibility\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "seed_everything(RANDOM_SEED)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Setup complete. Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "EzDjppJ1-ljl",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EzDjppJ1-ljl",
        "outputId": "3d2700cc-4872-4633-bbc6-a7d0ac934eef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loaded weights from ./Classification/densenet121-classification.pth (ignored mismatched layers)\n",
            "Loaded weights from ./Classification/efficientnet-classification.pth (ignored mismatched layers)\n",
            "Loaded weights from ./Classification/swintransformer-classification.pth (ignored mismatched layers)\n",
            "All models loaded and ready!\n"
          ]
        }
      ],
      "source": [
        "#@title re-load models\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "\n",
        "DATA_PATH=\"./Classification\"\n",
        "img_dir  = f'{DATA_PATH}/images'\n",
        "\n",
        "class ChestXrayDataset(Dataset):\n",
        "    def __init__(self, df, img_dir, transform=None):\n",
        "        self.df = df\n",
        "        self.img_dir = img_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.img_dir, row['Image Index'])\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        if self.transform:\n",
        "            image = self.transform(image)\n",
        "        label = torch.tensor(row['Binary Label'], dtype=torch.float32)\n",
        "        return image, label\n",
        "\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "\n",
        "image_size_= 224\n",
        "val_transforms   = transforms.Compose([\n",
        "    transforms.Resize((image_size_,image_size_)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# Define models\n",
        "# DenseNet121\n",
        "model_densenet = models.densenet121(pretrained=False)\n",
        "num_ftrs = model_densenet.classifier.in_features\n",
        "model_densenet.classifier = nn.Linear(num_ftrs, 1)\n",
        "model_densenet = model_densenet.to(device)\n",
        "\n",
        "# EfficientNet-B0\n",
        "model_enb0 = models.efficientnet_b0(pretrained=False)\n",
        "num_ftrs = model_enb0.classifier[1].in_features\n",
        "model_enb0.classifier[1] = nn.Linear(num_ftrs, 1)\n",
        "model_enb0 = model_enb0.to(device)\n",
        "\n",
        "# Swin Transformer\n",
        "model_swin = models.swin_t(pretrained=False)\n",
        "num_ftrs = model_swin.head.in_features\n",
        "model_swin.head = nn.Linear(num_ftrs, 1)\n",
        "model_swin = model_swin.to(device)\n",
        "\n",
        "# Load checkpoints safely\n",
        "def load_weights_safely(model, checkpoint_path):\n",
        "    # Map location to CPU to avoid CUDA device issues\n",
        "    checkpoint = torch.load(checkpoint_path, map_location=torch.device('cpu'))\n",
        "    state_dict = checkpoint if isinstance(checkpoint, dict) else checkpoint.state_dict()\n",
        "\n",
        "    # Filter out classifier weights if they don’t match\n",
        "    model_dict = model.state_dict()\n",
        "    filtered_dict = {k: v for k, v in state_dict.items() if k in model_dict and v.size() == model_dict[k].size()}\n",
        "\n",
        "    # Update and load\n",
        "    model_dict.update(filtered_dict)\n",
        "    model.load_state_dict(model_dict, strict=False)\n",
        "    print(f\"Loaded weights from {checkpoint_path} (ignored mismatched layers)\")\n",
        "\n",
        "# Apply loading\n",
        "load_weights_safely(model_densenet, f'{DATA_PATH}/densenet121-classification.pth')\n",
        "load_weights_safely(model_enb0, f'{DATA_PATH}/efficientnet-classification.pth')\n",
        "load_weights_safely(model_swin, f'{DATA_PATH}/swintransformer-classification.pth')\n",
        "\n",
        "print(\"All models loaded and ready!\")\n",
        "\n",
        "import torch, numpy as np, matplotlib.pyplot as plt\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, roc_curve\n",
        "\n",
        "def eval_models(model_list, model_names, loader, device=\"cuda\", threshold=0.5, plot_roc=True):\n",
        "    def _to_pos_prob(logits):\n",
        "        if logits.ndim == 2 and logits.shape[1] == 2:\n",
        "            return torch.softmax(logits, dim=1)[:, 1]\n",
        "        if logits.ndim == 1 or (logits.ndim == 2 and logits.shape[1] == 1):\n",
        "            return torch.sigmoid(logits.view(-1))\n",
        "        raise ValueError(f\"Only binary/2-class outputs supported, got shape {tuple(logits.shape)}\")\n",
        "\n",
        "    results, all_probs, all_labels = {}, {}, None\n",
        "    if plot_roc: plt.figure(figsize=(8,6))\n",
        "\n",
        "    for model, name in zip(model_list, model_names):\n",
        "        model.eval().to(device)\n",
        "        probs_list, labels_list = [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for X, y in loader:\n",
        "                X = X.to(device)\n",
        "                p = _to_pos_prob(model(X))\n",
        "                probs_list.extend(p.detach().cpu().numpy())\n",
        "                labels_list.extend(y.numpy())\n",
        "\n",
        "        y_true = np.array(labels_list)\n",
        "        y_prob = np.array(probs_list)\n",
        "        y_pred = (y_prob >= threshold).astype(int)\n",
        "\n",
        "        if all_labels is None: all_labels = y_true\n",
        "\n",
        "        acc  = accuracy_score(y_true, y_pred)\n",
        "        prec = precision_score(y_true, y_pred, zero_division=0)\n",
        "        rec  = recall_score(y_true, y_pred, zero_division=0)\n",
        "        f1   = f1_score(y_true, y_pred, zero_division=0)\n",
        "        auc  = roc_auc_score(y_true, y_prob)\n",
        "\n",
        "        print(f\"{name}: acc={acc:.4f}  prec={prec:.4f}  rec={rec:.4f}  f1={f1:.4f}  auc={auc:.4f}\")\n",
        "\n",
        "        if plot_roc:\n",
        "            fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
        "            plt.plot(fpr, tpr, label=f\"{name} (AUC={auc:.2f})\")\n",
        "\n",
        "        results[name] = {\"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1, \"roc_auc\":auc}\n",
        "        all_probs[name] = y_prob\n",
        "\n",
        "    if plot_roc:\n",
        "        plt.plot([0,1],[0,1],\"k--\"); plt.xlabel(\"False Positive Rate\"); plt.ylabel(\"True Positive Rate\")\n",
        "        plt.title(\"ROC Curve Comparison\"); plt.grid(True); plt.legend(); plt.show()\n",
        "\n",
        "    return {\"labels\": all_labels, \"probs\": all_probs, \"metrics\": results}"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ATsh57IQT3s6",
      "metadata": {
        "id": "ATsh57IQT3s6"
      },
      "source": [
        "#### Download & Load the .csv datasets if required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "FXqBjdRgT0uE",
      "metadata": {
        "id": "FXqBjdRgT0uE"
      },
      "outputs": [],
      "source": [
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/train_df.csv\" --local-dir ./\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/val_df.csv\" --local-dir ./\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/test_df.csv\" --local-dir ./\n",
        "\n",
        "# Download pretrained model weights from Hugging Face (Optional)\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/densenet121-classification.pth\" --local-dir ./\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/efficientnet-classification.pth\" --local-dir ./\n",
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/swintransformer-classification.pth\" --local-dir ./\n",
        "print(\"Model weights downloaded.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cUUUCB1FUdMT",
      "metadata": {
        "id": "cUUUCB1FUdMT"
      },
      "source": [
        "Load the csv dataframes once again if you need them:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "GapYggMxUFyw",
      "metadata": {
        "id": "GapYggMxUFyw"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"./Classification\"\n",
        "train_df = pd.read_csv(f'{DATA_PATH}/train_df.csv')\n",
        "val_df   = pd.read_csv(f'{DATA_PATH}/val_df.csv')\n",
        "\n",
        "train_val_df = pd.concat([train_df, val_df])\n",
        "\n",
        "test_df  = pd.read_csv(f'{DATA_PATH}/test_df.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "o4pirhJMv0n-",
      "metadata": {
        "id": "o4pirhJMv0n-"
      },
      "source": [
        "#### Deep Dive: Understanding Fairness Metrics\n",
        "\n",
        "To quantify fairness, we need to go beyond overall accuracy. We use specific metrics that compare error rates between groups. Two of the most common are:\n",
        "\n",
        "*   **Equal Opportunity Difference (EOD):** This metric answers the question: **\"For patients who actually have the disease, does the model identify it at the same rate for each group?\"**\n",
        "    *   It measures the absolute difference in the **True Positive Rate (Recall/Sensitivity)** between groups.\n",
        "    *   An EOD close to 0 means the model is equally good at \"catching\" the disease in all groups, avoiding a higher rate of dangerous false negatives for any single group.\n",
        "\n",
        "*   **Average Odds Difference (AOD):** This is a stricter metric that answers: **\"Are both the 'catch rate' (TPR) and the 'false alarm rate' (FPR) similar across groups?\"**\n",
        "    *   It is the average of the absolute differences in the True Positive Rate and the False Positive Rate between groups.\n",
        "    *   A low AOD (close to 0) indicates that the model's errors (both false negatives and false positives) are balanced across the different demographics."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "zNKr9rNuvStS",
      "metadata": {
        "id": "zNKr9rNuvStS"
      },
      "outputs": [],
      "source": [
        "# First, let's select our best model to audit\n",
        "# For this example, we'll choose the DenseNet121 model\n",
        "best_model = model_densenet # Or model_enb0, model_swin\n",
        "best_model.to(device)\n",
        "best_model.eval()\n",
        "\n",
        "# --- Utility to get all predictions once ---\n",
        "def get_all_predictions(model, loader, device):\n",
        "    \"\"\"Runs model on all data in loader and returns a DataFrame with results.\"\"\"\n",
        "    all_probs = []\n",
        "    all_labels = []\n",
        "    with torch.no_grad():\n",
        "        for imgs, labels in tqdm(loader, desc=\"Getting all test predictions\"):\n",
        "            imgs = imgs.to(device)\n",
        "            logits = model(imgs).squeeze(1)\n",
        "            probs = torch.sigmoid(logits)\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "\n",
        "    results_df = pd.DataFrame({\n",
        "        'true_label': np.array(all_labels).astype(int),\n",
        "        'probability': np.array(all_probs)\n",
        "    })\n",
        "    results_df['predicted_label'] = (results_df['probability'] > 0.5).astype(int)\n",
        "    return results_df\n",
        "\n",
        "# Run inference on the full test set\n",
        "test_ds = ChestXrayDataset(test_df, img_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False, num_workers=8, pin_memory=True)\n",
        "predictions_df = get_all_predictions(best_model, test_loader, device)\n",
        "\n",
        "# Merge predictions with original test dataframe to get demographic data\n",
        "# We reset the index to ensure a clean join\n",
        "test_df_with_preds = test_df.reset_index().join(predictions_df)\n",
        "\n",
        "print(\"Created a unified dataframe with predictions and demographics.\")\n",
        "display(test_df_with_preds.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RwTTyNAUv3yI",
      "metadata": {
        "id": "RwTTyNAUv3yI"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q3. Conduct the Fairness Audit and Interpret the Results</b>\n",
        "\n",
        "Now that we have a single table with all the necessary information, you can conduct the fairness audit. Run the following code cells to analyze the model's performance across different subgroups.\n",
        "\n",
        "**Your Task:**\n",
        "1.  Examine the demographic distributions for **Patient Sex** and **Patient Age**. Are the groups balanced?\n",
        "2.  Review the performance metrics (Accuracy, Precision, Recall, F1, AUROC) calculated for each subgroup. Are there noticeable performance gaps?\n",
        "3.  Analyze the **EOD** and **AOD** scores for the sex-based comparison. Based on these values, would you conclude that the model is fair? Justify your answer.\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ka2lKdoeTA2C",
      "metadata": {
        "id": "ka2lKdoeTA2C"
      },
      "outputs": [],
      "source": [
        "test_df['Patient Age'].hist()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "kb9z5JoKF7-B",
      "metadata": {
        "id": "kb9z5JoKF7-B"
      },
      "source": [
        "This function, `compute_AOD_EOD_from_split`, takes the true labels and predictions for two distinct subgroups (e.g., 'Female' and 'Male') and calculates two standard fairness metrics:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4KpBF5g7Abhp",
      "metadata": {
        "id": "4KpBF5g7Abhp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def compute_AOD_EOD_from_split(y_true_F, y_pred_F, y_true_M, y_pred_M):\n",
        "    def group_rates(y_true, y_pred):\n",
        "        tn, fp, fn, tp = confusion_matrix(y_true, y_pred, labels=[0, 1]).ravel()\n",
        "        tpr = tp / (tp + fn + 1e-6)\n",
        "        fpr = fp / (fp + tn + 1e-6)\n",
        "        return tpr, fpr\n",
        "\n",
        "    tpr_F, fpr_F = group_rates(y_true_F, y_pred_F)\n",
        "    tpr_M, fpr_M = group_rates(y_true_M, y_pred_M)\n",
        "\n",
        "    # Equal Opportunity Difference = |TPR_F - TPR_M|\n",
        "    EOD = abs(tpr_F - tpr_M)\n",
        "\n",
        "    # Average Odds Difference = 0.5 * (|FPR_F - FPR_M| + |TPR_F - TPR_M|)\n",
        "    AOD = 0.5 * (abs(fpr_F - fpr_M) + EOD)\n",
        "\n",
        "    return {\n",
        "        'TPR_F': tpr_F,\n",
        "        'TPR_M': tpr_M,\n",
        "        'FPR_F': fpr_F,\n",
        "        'FPR_M': fpr_M,\n",
        "        'EOD': EOD,\n",
        "        'AOD': AOD\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b9zu8iWNvasC",
      "metadata": {
        "id": "b9zu8iWNvasC"
      },
      "outputs": [],
      "source": [
        "# Subgroup Analysis by Patient Sex\n",
        "print(\"Analysis by Patient Sex\")\n",
        "\n",
        "# Visualize the distribution\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=test_df_with_preds, x=...#COMPLETE, hue='true_label')\n",
        "plt.title('Distribution of Outcomes by Patient Sex')\n",
        "plt.show()\n",
        "\n",
        "# Calculate performance metrics for each subgroup\n",
        "sex_metrics = test_df_with_preds.groupby(...#COMPLETE).apply(\n",
        "    lambda g: pd.Series({\n",
        "        'accuracy': accuracy_score(g['true_label'], g['predicted_label']),\n",
        "        'precision': precision_score(g['true_label'], g['predicted_label']),\n",
        "        'recall': recall_score(g['true_label'], g['predicted_label']),\n",
        "        'f1_score': f1_score(g['true_label'], g['predicted_label']),\n",
        "        'roc_auc': roc_auc_score(g['true_label'], g['probability']),\n",
        "        'count': len(g)\n",
        "    })\n",
        ")\n",
        "print(\"\\nPerformance Metrics by Patient Sex:\")\n",
        "display(sex_metrics)\n",
        "\n",
        "# Calculate formal fairness metrics\n",
        "df_F = test_df_with_preds[test_df_with_preds[...#COMPLETE] == ...#COMPLETE]\n",
        "df_M = test_df_with_preds[test_df_with_preds[...#COMPLETE] == ...#COMPLETE]\n",
        "\n",
        "fairness_metrics_sex = compute_AOD_EOD_from_split(\n",
        "    df_F['true_label'], df_F['predicted_label'],\n",
        "    df_M['true_label'], df_M['predicted_label']\n",
        ")\n",
        "print(\"\\nFormal Fairness Metrics (Female vs. Male):\")\n",
        "print(f\"  Equal Opportunity Difference (EOD): {fairness_metrics_sex['EOD']:.4f}\")\n",
        "print(f\"  Average Odds Difference (AOD):    {fairness_metrics_sex['AOD']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ywav_ip2vfDx",
      "metadata": {
        "id": "Ywav_ip2vfDx"
      },
      "outputs": [],
      "source": [
        "# Subgroup Analysis by Patient Age\n",
        "print(\"\\nAnalysis by Patient Age\")\n",
        "\n",
        "# Create age bins and visualize the distribution\n",
        "test_df_with_preds['Age Group'] = pd.cut(test_df_with_preds[...#COMPLETE], bins=[0, 50, 100], labels=[...#COMPLETE], right=False)\n",
        "plt.figure(figsize=(6, 4))\n",
        "sns.countplot(data=test_df_with_preds, x='Age Group', hue='true_label')\n",
        "plt.title('Distribution of Outcomes by Patient Age Group')\n",
        "plt.show()\n",
        "\n",
        "# Calculate performance metrics for each subgroup\n",
        "age_metrics = test_df_with_preds.groupby('Age Group').apply(\n",
        "    lambda g: pd.Series({\n",
        "        'accuracy': accuracy_score(g['true_label'], g['predicted_label']),\n",
        "        'precision': precision_score(g['true_label'], g['predicted_label']),\n",
        "        'recall': recall_score(g['true_label'], g['predicted_label']),\n",
        "        'f1_score': f1_score(g['true_label'], g['predicted_label']),\n",
        "        'roc_auc': roc_auc_score(g['true_label'], g['probability']),\n",
        "        'count': len(g)\n",
        "    })\n",
        ")\n",
        "print(\"\\nPerformance Metrics by Patient Age Group:\")\n",
        "display(age_metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "EADjX9zHvneg",
      "metadata": {
        "id": "EADjX9zHvneg"
      },
      "source": [
        "<details>\n",
        "<summary>Click for & Discussion</summary>\n",
        "\n",
        "**Interpretation of Results:**\n",
        "\n",
        "1.  **Demographic Distributions:** You'll notice that the dataset is not perfectly balanced. There are more male patients than female patients, and the prevalence of the pathology (positive labels) differs across groups. For example, the `50+` age group has a higher rate of disease than the `<50` group. These underlying differences in the data are often where biases can originate.\n",
        "\n",
        "2.  **Performance Gaps:** When looking at the metrics tables, you might see small differences. For instance, the ROC AUC for the `50+` group might be slightly higher than for the `<50` group. Small fluctuations are normal, but large gaps (e.g., >5% difference in ROC AUC or Recall) would be a cause for concern.\n",
        "\n",
        "3.  **Fairness Metrics (EOD & AOD):** The key is to look at the magnitude of the EOD and AOD values.\n",
        "    *   **Low Values (e.g., < 0.05):** If the EOD and AOD are very small, it suggests that the model's performance is relatively fair between the groups at this specific 0.5 decision threshold. The error rates (both false positives and false negatives) are distributed similarly.\n",
        "    *   **High Values (e.g., > 0.10):** A high EOD would be a major red flag. For example, an EOD of 0.15 would mean there is a 15 percentage point difference in the model's ability to detect the disease between males and females. This is a clinically significant bias that would need to be addressed before the model could be used responsibly.\n",
        "\n",
        "**Conclusion:** Based on this audit, you can make a more informed statement about your model's trustworthiness. For example: \"While our DenseNet121 model achieves a high overall ROC AUC, our fairness audit reveals a slight performance drop for patients under 50. The model shows minimal bias with respect to patient sex, with an AOD of less than 0.02, indicating its errors are well-balanced between males and females.\"\n",
        "</details>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7okHIG9mTKwY",
      "metadata": {
        "id": "7okHIG9mTKwY"
      },
      "source": [
        "\n",
        "Our cohort splits `297` patients under 50 and `261` aged 50 or older. This gives us also an imbalanced younger vs. older comparison. Later, we’ll check if our classifier behaves differently across these age bins (e.g. disparate error rates).\n",
        "\n",
        " #### **Potential Sources of Bias in the Evaluation Set**\n",
        "\n",
        "Our dataset contains two protected attributes that are fully populated, Sex and Age. Below we outline where bias might arise and identify which attribute currently appears least affected by sampling imbalance.\n",
        "### **Which Attribute Is Least Biased?**\n",
        "\n",
        "Given the observed counts, the Sex/Gender split (44 % vs 56 %) is closer to parity than the Age split. Hence, sex currently introduces less sampling bias than age, though fairness still needs empirical validation.\n",
        "### **Note on Race / Ethnicity**\n",
        "\n",
        "No race or ethnicity values are present in this dataset. As a result, we cannot quantify nor mitigate bias along that dimension. If such information becomes available in future data collections, it should be incorporated into the fairness analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6llOnmB1WVka",
      "metadata": {
        "id": "6llOnmB1WVka"
      },
      "source": [
        "Run the evaluator on the new dataframe"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ettsM5WyWUzR",
      "metadata": {
        "id": "ettsM5WyWUzR"
      },
      "outputs": [],
      "source": [
        "test_ds = ChestXrayDataset(test_df, img_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model_list  = [model_densenet, model_enb0, model_swin]\n",
        "model_names = ['DenseNet 121', 'EfficientNet B0', 'Swin Transformer']\n",
        "\n",
        "out = eval_models(model_list, model_names, test_loader, device=device, threshold=0.5, plot_roc=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qwisSRPRaWBo",
      "metadata": {
        "id": "qwisSRPRaWBo"
      },
      "outputs": [],
      "source": [
        "test_ds = ChestXrayDataset(test_df_with_preds.loc[test_df_with_preds['Age Group']=='50+'], img_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model_list  = [model_densenet, model_enb0, model_swin]\n",
        "model_names = ['DenseNet 121', 'EfficientNet B0', 'Swin Transformer']\n",
        "\n",
        "out = eval_models(model_list, model_names, test_loader, device=device, threshold=0.5, plot_roc=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bGuw5zbsajSB",
      "metadata": {
        "id": "bGuw5zbsajSB"
      },
      "outputs": [],
      "source": [
        "test_ds = ChestXrayDataset(test_df_with_preds.loc[test_df_with_preds['Age Group']=='<50'], img_dir, transform=val_transforms)\n",
        "test_loader = DataLoader(\n",
        "    test_ds,\n",
        "    batch_size=32,\n",
        "    num_workers=8,\n",
        "    pin_memory=True,\n",
        "    shuffle=False\n",
        ")\n",
        "\n",
        "model_list  = [model_densenet, model_enb0, model_swin]\n",
        "model_names = ['DenseNet 121', 'EfficientNet B0', 'Swin Transformer']\n",
        "\n",
        "out = eval_models(model_list, model_names, test_loader, device=device, threshold=0.5, plot_roc=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rW1ZIzu9RoZD",
      "metadata": {
        "id": "rW1ZIzu9RoZD"
      },
      "source": [
        "## 9. Advanced Quality Control II: Explainable AI (XAI)\n",
        "\n",
        "We've evaluated *what* our model predicts (its accuracy and fairness), but to truly trust it, we must understand *why* it makes a certain prediction. Is it looking at the correct, clinically relevant pathology, or is it focusing on an artifact or a shortcut in the image?\n",
        "\n",
        "This is the domain of **Explainable AI (XAI)**. XAI techniques help us peek inside the \"black box\" of the neural network to make its decision-making process transparent. For this, we will use one of the most popular XAI techniques: **Grad-CAM**."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hZP2aCxjnOp2",
      "metadata": {
        "id": "hZP2aCxjnOp2"
      },
      "source": [
        "### 9.1. Establishing a Visual Ground Truth: Where is the Pathology?\n",
        "\n",
        "Our classification model predicts *if* a disease like 'Effusion' is present in an image. But a crucial question for building trust remains: **is the model looking at the right place?** Before we can use XAI techniques like Grad-CAM to see where our model is focusing, we first need to establish our own visual ground truth.\n",
        "\n",
        "The NIH dataset provides a file, `BBox_List_2017.csv`, containing bounding box annotations created by radiologists for a subset of the images. While our model was not trained to *draw* these boxes (a detection/localization task coming in the `detection_exercise.ipynb`), they serve as an invaluable **\"answer key\"** for us. They show us precisely where the clinically relevant findings are located.\n",
        "\n",
        "In the following step, we will use our `plot_image_with_bbox` utility to visualize several examples of our chosen pathology with these ground truth boxes overlaid. Pay close attention to the location, size, and appearance of the findings. This will build your intuition and provide a crucial visual baseline for when we evaluate our model's explainability heatmaps in the next section. We will be asking: **Do our model's heatmaps align with these ground truth boxes?**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-Dl8RJNukEyG",
      "metadata": {
        "id": "-Dl8RJNukEyG"
      },
      "outputs": [],
      "source": [
        "!hf download albarqouni/bild-dataset --repo-type dataset --include \"Classification/BBox_List_2017.csv\" --local-dir ./"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "izixNIOJbyxb",
      "metadata": {
        "id": "izixNIOJbyxb"
      },
      "outputs": [],
      "source": [
        "DATA_PATH = \"./Classification\"\n",
        "bbox_df = pd.read_csv(f'{DATA_PATH}/BBox_List_2017.csv')\n",
        "bbox_df['Finding Label'].value_counts()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "V2pWcySOX5Bf",
      "metadata": {
        "cellView": "form",
        "id": "V2pWcySOX5Bf"
      },
      "outputs": [],
      "source": [
        "#@title Utility visualization\n",
        "\n",
        "\n",
        "def plot_image_with_bbox(image_path, bbox_data):\n",
        "    # skip cleanly if file not found\n",
        "    if not os.path.exists(image_path):\n",
        "        print(f\"[skip] Not found: {image_path}\")\n",
        "        return\n",
        "\n",
        "    img = cv2.imread(image_path)\n",
        "    if img is None:\n",
        "        print(f\"[skip] Cannot read: {image_path}\")\n",
        "        return\n",
        "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    df = bbox_data.copy()\n",
        "    # normalize bbox columns to x,y,w,h (supports both schemas)\n",
        "    if 'x' not in df.columns:\n",
        "        if {'Bbox [x', 'y', 'w', 'h]'}.issubset(df.columns):\n",
        "            df = df.rename(columns={'Bbox [x':'x','y':'y','w':'w','h]':'h'})\n",
        "        elif {'Bbox [x y w h]', 'Unnamed: 6', 'Unnamed: 7', 'Unnamed: 8'}.issubset(df.columns):\n",
        "            df = df.rename(columns={'Bbox [x y w h]':'x','Unnamed: 6':'y','Unnamed: 7':'w','Unnamed: 8':'h'})\n",
        "        else:\n",
        "            print(\"[skip] Unrecognized bbox columns\"); return\n",
        "\n",
        "    for c in ('x','y','w','h'):\n",
        "        df[c] = pd.to_numeric(df[c], errors='coerce')\n",
        "    df = df.dropna(subset=['x','y','w','h'])\n",
        "\n",
        "    for _, r in df.iterrows():\n",
        "        x, y, w, h = int(r['x']), int(r['y']), int(r['w']), int(r['h'])\n",
        "        cv2.rectangle(img, (x, y), (x + w, y + h), (255, 0, 0), 2)\n",
        "\n",
        "    plt.figure(figsize=(8, 8))\n",
        "    plt.imshow(img); plt.axis('off')\n",
        "    plt.title(os.path.basename(image_path))\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "oGQju9amXokI",
      "metadata": {
        "id": "oGQju9amXokI"
      },
      "outputs": [],
      "source": [
        "pathology_ = \"Effusion\"  #@param ['Infiltrate', 'Atelectasis', 'Effusion', 'Nodule', 'Pneumothorax', 'Mass']\n",
        "# Filter bbox_df for 'Infiltrate' and select a few images\n",
        "imgs = glob(f'{DATA_PATH}/images/*')\n",
        "imgs_basename = [os.path.basename(i) for i in imgs]\n",
        "\n",
        "bbox_df = bbox_df.loc[bbox_df['Image Index'].isin(imgs_basename)]\n",
        "infiltrate_bboxes = bbox_df[bbox_df['Finding Label'] == pathology_]\n",
        "num_images_to_plot = 5\n",
        "images_to_plot = infiltrate_bboxes['Image Index'].unique()[:num_images_to_plot]\n",
        "\n",
        "IMG_DIR = f'{DATA_PATH}/images'\n",
        "\n",
        "# Plot the selected images with their bounding boxes\n",
        "for img_name in images_to_plot:\n",
        "    img_path = os.path.join(IMG_DIR, img_name)\n",
        "    img_bboxes = infiltrate_bboxes[infiltrate_bboxes['Image Index'] == img_name]\n",
        "    plot_image_with_bbox(img_path, img_bboxes)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "svy_iJbijHPn",
      "metadata": {
        "id": "svy_iJbijHPn"
      },
      "source": [
        "#### Preparing for Explainability: Helper Functions\n",
        "\n",
        "To systematically generate and visualize our Grad-CAM heatmaps, we'll use a few helper functions. This keeps our main analysis code clean and readable by abstracting away the complexities of layer selection and plotting.\n",
        "\n",
        "Here’s a breakdown of what each function does:\n",
        "\n",
        "*   **`get_last_conv(model, selector)`**: This is a crucial utility for finding the right layer inside our model to apply Grad-CAM.\n",
        "    *   **Why it's needed:** Grad-CAM works by analyzing the feature maps of a convolutional layer. The best results typically come from the final convolutional layers, as they contain the most abstract, high-level information the model has learned.\n",
        "    *   **How it works:** This function intelligently scans the model and allows us to easily select the very last conv layer (`'last_layer'`), the second-to-last (`'last_layer2'`), or even a specific named layer, giving us the flexibility to experiment.\n",
        "\n",
        "*   **`visualize_k_per_class(...)`**: This function handles all the plotting for our XAI analysis.\n",
        "    *   **Why it's needed:** We want a standardized way to compare the model's \"focus\" with the ground truth.\n",
        "    *   **How it works:** For a given set of images and their corresponding heatmaps, it automatically finds and displays a specified number of examples for each class (e.g., 2 \"No Finding\" and 2 \"Effusion\" cases). It produces a clean, side-by-side comparison showing the original image next to the image with the Grad-CAM heatmap overlaid, complete with ground truth and prediction labels. This makes it easy to visually assess if the model is \"looking\" at the right place.\n",
        "\n",
        "*   **`idx_to_name(i)`**: A simple utility to convert numeric labels (like `0`) back to their human-readable names (like `'No Finding'`) for clear plot titles."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "E0Xburg6jaNB",
      "metadata": {
        "id": "E0Xburg6jaNB"
      },
      "outputs": [],
      "source": [
        "#@title Utility functions\n",
        "def idx_to_name(i): return CLASS_NAMES.get(int(i), str(int(i)))\n",
        "\n",
        "def get_last_conv(model: nn.Module, selector: str = \"last_layer\"):\n",
        "    \"\"\"\n",
        "    selector:\n",
        "      - 'last_layer'  -> last Conv2d\n",
        "      - 'last_layer2' -> 2nd to last Conv2d\n",
        "      - 'last_layer3' -> 3rd to last Conv2d\n",
        "      - 'last_layer4' -> 4th to last Conv2d\n",
        "      - dotted attribute path (e.g., 'backbone.body.layer3')\n",
        "    Returns: (layer_name, module)\n",
        "    \"\"\"\n",
        "    # If selector is a dotted path, try resolve directly\n",
        "    if '.' in selector or (selector and selector not in {\"last_layer\",\"last_layer2\",\"last_layer3\",\"last_layer4\"}):\n",
        "        mod = _resolve_attr_path(model, selector)\n",
        "        if isinstance(mod, nn.Module):\n",
        "            return selector, mod\n",
        "        # fall through to conv list if not found\n",
        "\n",
        "    # Gather all Conv2d in definition order\n",
        "    convs = [(name, m) for name, m in model.named_modules() if isinstance(m, nn.Conv2d)]\n",
        "    if not convs:\n",
        "        raise RuntimeError(\"No nn.Conv2d layers found; CAM needs a conv feature map. \"\n",
        "                           \"Provide a valid attribute path or choose a CNN-backed model.\")\n",
        "\n",
        "    # Map selector to index from the end\n",
        "    suffix_to_k = {\"last_layer\":1, \"last_layer2\":2, \"last_layer3\":3, \"last_layer4\":4}\n",
        "    k = suffix_to_k.get(selector, 1)\n",
        "    if k > len(convs):\n",
        "        print(f\"[warn] {selector} requested but only {len(convs)} conv layers available; using last.\")\n",
        "        k = 1\n",
        "    return convs[-k]  # (name, module)\n",
        "\n",
        "def visualize_k_per_class(orig_chw, heatmaps, true_labels, pred_labels, pred_confs,\n",
        "                          k_per_class=5, method_name=\"CAM\", mean=mean, std=std):\n",
        "    idx_neg = np.where(pred_labels == 0)[0][:k_per_class]\n",
        "    idx_pos = np.where(pred_labels == 1)[0][:k_per_class]\n",
        "\n",
        "    if len(idx_neg) < k_per_class:\n",
        "        print(f\"[warn] Only {len(idx_neg)} negatives available.\")\n",
        "    if len(idx_pos) < k_per_class:\n",
        "        print(f\"[warn] Only {len(idx_pos)} positives available.\")\n",
        "\n",
        "    show_indices = list(idx_neg) + list(idx_pos)\n",
        "\n",
        "    for i in show_indices:\n",
        "        # unnormalize to [0,1], CHW->HWC\n",
        "        img = orig_chw[i].transpose(1, 2, 0)\n",
        "        img = np.clip(std * img + mean, 0, 1)\n",
        "\n",
        "        cam_img = show_cam_on_image(img, heatmaps[i], use_rgb=True)\n",
        "\n",
        "        plt.figure(figsize=(8,4))\n",
        "        plt.subplot(1,2,1)\n",
        "        plt.imshow(img); plt.axis('off')\n",
        "        plt.title(f\"Original — Ground Truth: {idx_to_name(true_labels[i])}\")\n",
        "\n",
        "        plt.subplot(1,2,2)\n",
        "        plt.imshow(cam_img); plt.axis('off')\n",
        "        plt.title(f\"{method_name} — Pred: {idx_to_name(pred_labels[i])} ({pred_confs[i]:.2f})\")\n",
        "        plt.tight_layout(); plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SdCDPQ7qtopQ",
      "metadata": {
        "id": "SdCDPQ7qtopQ"
      },
      "source": [
        "### 9.2 Heatmaps Heatmaps"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xmtTDQFdgeKU",
      "metadata": {
        "id": "xmtTDQFdgeKU"
      },
      "source": [
        "What these CAM helpers do (short)\n",
        "\n",
        "- **`pick_cam_constructor(name_str)`**  \n",
        "  Maps a string to a CAM class and a pretty name: `\"GradCAM\"`, `\"GradCAM++\"`, or `\"ScoreCAM\"`.  \n",
        "  Returns `(cam_class, label)` so you can build the CAM once per model/layer.\n",
        "\n",
        "- **`run_cam_for_correct_samples(model, loader, target_layer, cam_ctor, device)`**  \n",
        "  1) Builds the CAM object for `target_layer`.  \n",
        "  2) Runs the model over the data and handles **multiclass** (softmax, top-1) and **binary** (sigmoid) outputs.  \n",
        "  3) Keeps **only correct predictions** (to avoid explaining mistakes).  \n",
        "  4) For each kept sample, sets the CAM **target = predicted class** (or index `0` for single-logit binary) and computes the heatmap.  \n",
        "  5) Collects and returns:\n",
        "     - `imgs_np` — input images (N, C, H, W), **unnormalized later by you** for display  \n",
        "     - `heatmaps_np` — CAM heatmaps (N, H, W)  \n",
        "     - `y_true_np`, `y_pred_np` — ground truth & predicted labels (N,)  \n",
        "     - `y_conf_np` — confidences (top-1 prob or max(sigmoid, 1−sigmoid))  \n",
        "     \n",
        "Use this output to plot side-by-side: the original image and its CAM overlay, knowing all shown cases are correctly classified. Compare with the bounding boxes above to spot the pathology and assess if its correctly predicted qualitatively.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0btFFmbDkXUC",
      "metadata": {
        "id": "0btFFmbDkXUC"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q6.</b> Implement the Core XAI Logic.\n",
        "\n",
        "Now for the most visually insightful part of our analysis. We will use the `pytorch-grad-cam` library to generate the heatmaps that show us where our model is \"looking.\"\n",
        "\n",
        "Your task is to complete the core logic inside the `run_cam_for_correct_samples` function. This function is designed to iterate through our test data, find the images the model classified **correctly**, and then generate a heatmap explaining *why* the model made that correct decision.\n",
        "\n",
        "**Your Task:**\n",
        "Fill in the `...` placeholders in the code below to complete the following three critical steps:\n",
        "\n",
        "1.  **Instantiate the CAM Object:** Create an instance of the chosen CAM method (e.g., `GradCAM`) using the provided `cam_ctor`, the `model`, and the `target_layer`.\n",
        "2.  **Define the CAM Targets:** For the `pytorch-grad-cam` library, we need to specify *what* we want to explain. For a classification model, the target is the predicted class. Create a list of `ClassifierOutputTarget` objects, one for each of the correctly predicted images.\n",
        "3.  **Generate the Heatmaps:** Call the `cam` object with the correct `input_tensor` and the list of `targets` you just created to generate the batch of heatmaps.\n",
        "\n",
        "</div>\n",
        "\n",
        "<details>\n",
        "<summary>Click for Discussion</summary>\n",
        "\n",
        "**Discussion:**\n",
        "\n",
        "*   **Step 1** is the standard initialization for any CAM object in the library. We tell it which model and which specific layer within that model to analyze.\n",
        "*   **Step 2** is conceptually the most important. We are telling Grad-CAM, \"For this first image, explain the prediction for class X; for the second image, explain the prediction for class Y,\" and so on. Since we have already filtered for correct predictions, `preds_corr` contains the correct class index for each image, which is exactly what `ClassifierOutputTarget` needs. We create one target for each image in our correctly predicted batch (`X_corr`).\n",
        "*   **Step 3** is the main execution call. We pass the batch of input image tensors (`X_corr`) and our corresponding list of targets (`targets_corr`), and the library handles the complex backpropagation and weighting to produce the final heatmaps (`hm_batch`).\n",
        "\n",
        "By only running this analysis on correctly classified images, we ensure our visual inspection is focused on understanding the model's *successful* reasoning patterns, which is key for building trust.\n",
        "</details>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "B2M3VwvMlgga",
      "metadata": {
        "id": "B2M3VwvMlgga"
      },
      "outputs": [],
      "source": [
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "def pick_cam_constructor(name_str: str):\n",
        "    s = name_str.lower().replace(\" \", \"\")\n",
        "    if \"gradcam++\" in s:\n",
        "        return GradCAMPlusPlus, \"Grad-CAM++\"\n",
        "    if \"score\" in s:\n",
        "        return ScoreCAM, \"Score-CAM\"\n",
        "    return GradCAM, \"Grad-CAM\"\n",
        "\n",
        "\n",
        "def run_cam_for_correct_samples(model, loader, target_layer, cam_ctor, device):\n",
        "    \"\"\"\n",
        "    This function runs a CAM method on correctly classified samples from a data loader.\n",
        "    TODO: Fill in the placeholders to complete the function.\n",
        "    \"\"\"\n",
        "    # Instantiate the CAM object\n",
        "    # Hint: Use the cam_ctor (e.g., GradCAM), the model, and the target_layer\n",
        "    cam = ...\n",
        "\n",
        "    heatmaps, imgs, y_true, y_pred, y_conf = [], [], [], [], []\n",
        "\n",
        "    for X, y in tqdm(loader, desc=\"Generating Heatmaps\", leave=False):\n",
        "        X = X.to(device)\n",
        "        y_np = y.numpy()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            out = model(X)\n",
        "            # This part handles both binary and multi-class outputs to get predictions\n",
        "            pos_prob = torch.sigmoid(out).view(-1)\n",
        "            top1_idx = (pos_prob > 0.5).long()\n",
        "            top1_prob = torch.where(top1_idx == 1, pos_prob, 1.0 - pos_prob)\n",
        "\n",
        "        preds_np = top1_idx.detach().cpu().numpy()\n",
        "        confs_np = top1_prob.detach().cpu().numpy()\n",
        "\n",
        "        # Filter to keep only the samples that the model predicted correctly\n",
        "        correct_mask = (preds_np == y_np)\n",
        "        if correct_mask.sum() == 0:\n",
        "            continue\n",
        "\n",
        "        X_corr       = X[correct_mask]\n",
        "        y_corr       = y_np[correct_mask]\n",
        "        preds_corr   = preds_np[correct_mask]\n",
        "        confs_corr   = confs_np[correct_mask]\n",
        "\n",
        "        # Define the CAM targets for the correctly predicted samples\n",
        "        # Hint: The target for each image is its predicted class. Create a list of\n",
        "        # ClassifierOutputTarget objects using the `preds_corr` array.\n",
        "        targets_corr = ...\n",
        "\n",
        "        # Generate the heatmaps for the batch of correct predictions\n",
        "        # Hint: Call the `cam` object with the input images (`X_corr`) and their targets.\n",
        "        hm_batch = ...\n",
        "\n",
        "        # The rest of the function collects the results\n",
        "        heatmaps.extend(hm_batch)\n",
        "        imgs.extend(X_corr.detach().cpu())\n",
        "        y_true.extend(y_corr)\n",
        "        y_pred.extend(preds_corr)\n",
        "        y_conf.extend(confs_corr)\n",
        "\n",
        "    if not imgs:\n",
        "        print(\"No correct predictions were found to visualize.\")\n",
        "        return None\n",
        "\n",
        "    # Package results into NumPy arrays for easy visualization\n",
        "    imgs_np     = torch.stack(imgs).cpu().numpy()\n",
        "    heatmaps_np = np.stack(heatmaps, axis=0)\n",
        "    y_true_np   = np.array(y_true, dtype=int)\n",
        "    y_pred_np   = np.array(y_pred, dtype=int)\n",
        "    y_conf_np   = np.array(y_conf, dtype=float)\n",
        "\n",
        "    return imgs_np, heatmaps_np, y_true_np, y_pred_np, y_conf_np"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dPlRic7IiPu3",
      "metadata": {
        "id": "dPlRic7IiPu3"
      },
      "source": [
        "#### Deep Dive: How Grad-CAM Works\n",
        "\n",
        "**Grad-CAM (Gradient-weighted Class Activation Mapping)** produces a visual heatmap that highlights the regions in an input image that were most important for a specific prediction.\n",
        "\n",
        "-   **Hotter areas (red/yellow)** on the heatmap indicate pixels that strongly influenced the model to make its decision.\n",
        "-   **Cooler areas (blue)** indicate pixels that were less influential.\n",
        "\n",
        "It works by combining two key pieces of information:\n",
        "1.  **Feature Maps:** The final convolutional layer of the network produces feature maps that contain high-level semantic information (e.g., \"this area has a texture consistent with infiltration\").\n",
        "2.  **Gradients:** Grad-CAM calculates the \"importance\" (gradient) of each feature map with respect to the final prediction.\n",
        "\n",
        "By taking a weighted average of the feature maps, Grad-CAM creates a heatmap that shows us exactly where the model \"looked\" to make its decision. By overlaying this on the original X-ray, we can visually confirm if our model is behaving like a radiologist or if it's being \"distracted\" by irrelevant information.\n",
        "\n",
        "#### Deep Dive: How Grad-CAM++ Works\n",
        "\n",
        "Grad-CAM++ builds on Grad-CAM but improves the localization of small or multiple objects:\n",
        "\n",
        "- **Weighted Combination of Feature Maps**: Instead of a simple global average of gradients, Grad-CAM++ uses a weighted sum that accounts for pixel-wise contributions.\n",
        "\n",
        "- **Better Fine-Grained Localization**: This allows the heatmap to more precisely highlight smaller regions that are critical to the prediction, especially when multiple objects or details matter.\n",
        "\n",
        "ScoreCAM on the other had avoids gradient computation and often produces smoother, less noisy heatmaps."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tpmo6OuaQXXq",
      "metadata": {
        "id": "tpmo6OuaQXXq"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from pytorch_grad_cam import GradCAM, GradCAMPlusPlus, ScoreCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
        "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
        "\n",
        "import gc\n",
        "torch.cuda.empty_cache()  # clears unused cached memory\n",
        "gc.collect()  # run garbage collector\n",
        "\n",
        "# Config\n",
        "pathology_= \"Effusion\"\n",
        "CLASS_NAMES = {0: \"No finding\", 1: pathology_}  # set `pathology_` to your label string\n",
        "n_samples = 2  #@param {type: \"number\"}\n",
        "\n",
        "# selector: 'last_layer'  -> last Conv2d | 'last_layer2' -> 2nd to last Conv2d etc.\n",
        "layer_to_use = \"last_layer\"  #@param ['last_layer4', 'last_layer3', 'last_layer2', 'last_layer']\n",
        "heatmap_method = \"GradCAM\"  # @param ['GradCAM', 'ScoreCAM', 'GradCAM++']\n",
        "\n",
        "mean = np.array([0.485, 0.456, 0.406])\n",
        "std  = np.array([0.229, 0.224, 0.225])\n",
        "\n",
        "image_size_= 512\n",
        "test_transforms   = transforms.Compose([\n",
        "    transforms.Resize((image_size_,image_size_)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "test_ds   = ChestXrayDataset(test_df,   img_dir, transform=test_transforms)\n",
        "test_loader   = DataLoader(test_ds,   batch_size=n_samples, num_workers=8, pin_memory=True, shuffle=False)\n",
        "\n",
        "# main loop\n",
        "cam_ctor, cam_name = pick_cam_constructor(heatmap_method)\n",
        "\n",
        "model_list  = [model_densenet, model_enb0, model_swin]\n",
        "model_names = ['DenseNet 121', 'EfficientNet B0', 'Swin Transformer']\n",
        "for model, model_name in zip(model_list, model_names):\n",
        "    model.to(device).eval()\n",
        "    layer_name, target_layer = get_last_conv(model, selector=layer_to_use)\n",
        "    print(f\"\\t\\n{cam_name} for {model_name} | target layer: {layer_name}\")\n",
        "\n",
        "    out = run_cam_for_correct_samples(model, test_loader, target_layer, cam_ctor, device)\n",
        "    if out is None:\n",
        "        print(\"No correct predictions to visualize.\"); continue\n",
        "\n",
        "    imgs_np, heatmaps_np, y_true_np, y_pred_np, y_conf_np = out\n",
        "    visualize_k_per_class(imgs_np, heatmaps_np, y_true_np, y_pred_np, y_conf_np,\n",
        "                          k_per_class=n_samples, method_name=cam_name, mean=mean, std=std)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ji4fjeBut4wh",
      "metadata": {
        "id": "Ji4fjeBut4wh"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "<b>Q6.</b> In the previous cell, we created the <code>GradCAM heatmaps</code>.\n",
        "\n",
        "- Observe the code and the plots, and interpret how well the model is performing\n",
        "- What remarks can you identify with respect to the <code>GradCAM</code>\n",
        "- Can you spot the lesions in the image under the predicted class activation maps? Where do the difficulties arise?"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rXOMzIexIo2L",
      "metadata": {
        "id": "rXOMzIexIo2L"
      },
      "source": [
        "# Conclusion and Final Thoughts\n",
        "\n",
        "Congratulations on completing this comprehensive, hands-on journey through medical image classification!\n",
        "\n",
        "Over the course of this notebook, you have successfully built, trained, and evaluated deep learning models from start to finish. You have gone beyond mere training—you have engaged in critical practices that transform a simple experiment into a robust, interpretable AI system.\n",
        "\n",
        "## Key Achievements\n",
        "\n",
        "- You successfully handled and prepared a real-world medical imaging dataset, including preprocessing and normalization.\n",
        "- You built custom PyTorch `Dataset` and `DataLoader` pipelines tailored for classification tasks.\n",
        "- You trained state-of-the-art models (DenseNet, EfficientNet, Swin Transformer) and evaluated them on unseen test data.\n",
        "- You implemented advanced evaluation metrics beyond simple accuracy, including precision, recall, F1-score, and AUROC and explored potential bias and fairness.\n",
        "- You applied explainable AI techniques such as Grad-CAM, Grad-CAM++, and Score-CAM to interpret model predictions and identify the regions of the X-rays that influenced each decision.\n",
        "\n",
        "## Key Takeaways\n",
        "\n",
        "- **Data is foundational:** A thorough understanding of your dataset, including class balance and preprocessing, is essential for successful model training.  \n",
        "- **Evaluation is multi-dimensional:** Relying on a single metric is insufficient. Metrics like AUROC, F1-score, and visual inspection of correctly and incorrectly classified images give a complete picture of model performance.  \n",
        "- **Interpretability builds trust:** In high-stakes domains like medicine, it’s crucial to understand why models make their decisions and assess their potential bias. Techniques like Grad-CAM help clinicians trust AI-assisted predictions.\n",
        "\n",
        "## Next Steps\n",
        "\n",
        "This notebook provides a strong foundation for further exploration. Here are some directions to extend your work:\n",
        "\n",
        "1. **Include Negative Samples:** Incorporate non-diseased cases to create a more comprehensive diagnostic model capable of both detection and classification.  \n",
        "2. **Experiment with Architectures:** Try alternative architectures or ensemble approaches to improve classification performance.  \n",
        "3. **Hyperparameter Optimization:** Systematically tune learning rates, batch sizes, and data augmentation strategies to improve metrics like AUROC and F1-score.  \n",
        "4. **3D Medical Imaging:** Extend these methods to 3D datasets (CT or MRI scans) to handle more complex structures and richer diagnostic information.  \n",
        "5. **Model Explainability:** Explore LIME or integrated gradients alongside Grad-CAM to provide multi-faceted interpretability for clinical decision support.\n",
        "\n",
        "By following these practices, you have not only trained accurate models but also built a framework for safe, interpretable, and clinically useful AI in medical imaging.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "B5IdJmBHK0M6",
      "metadata": {
        "id": "B5IdJmBHK0M6"
      },
      "source": [
        "> 90 minutes"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "torch-gpu",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}